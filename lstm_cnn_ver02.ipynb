{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/yyliu/code/NLP/data/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u74f96b08eeb68fe4b0ac4c13a6f276ed.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of train set: 784\n",
      "sentence number of dementia subject: 394\n",
      "sentence number of control normal subject: 390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.520 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of train set: 89\n",
      "sentence number of dementia subject: 48\n",
      "sentence number of control normal subject: 41\n"
     ]
    }
   ],
   "source": [
    "import data_preprocess\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# keras module\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, Input, Conv1D, MaxPool1D, concatenate, Flatten\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "\n",
    "# global variables\n",
    "SEQUENCE_LENGTH = 17\n",
    "EMBEDDING_DIM = 100\n",
    "CONTROL_TRAIN = 'control.txt'\n",
    "DEMENTIA_TRAIN = 'dementia.txt'\n",
    "CONTROL_TEST = 'control_test.txt'\n",
    "DEMENTIA_TEST = 'dementia_test.txt'\n",
    "# W2V_MODEL = '100features_20context_20mincount_zht'\n",
    "\n",
    "# data preprocessing, load model, w2v_lookup_table, train_x, train_y, train_x_seg, train_x_onehot\n",
    "\n",
    "# w2v_model = data_preprocess.load_wordvec_model(W2V_MODEL)\n",
    "x_train, y_train = data_preprocess.read_sentence_single_label(DEMENTIA_TRAIN, CONTROL_TRAIN)\n",
    "x_train_seg = data_preprocess.segmentation(x_train)\n",
    "x_test, y_test = data_preprocess.read_sentence_single_label(DEMENTIA_TEST, CONTROL_TEST)\n",
    "x_test_seg = data_preprocess.segmentation(x_test)\n",
    "\n",
    "# x_onehot, vocab_processor = data_preprocess.text_to_onehot(x_seg)\n",
    "\n",
    "# Split data into train and validate part\n",
    "\n",
    "# x_train, x_dev, y_train, y_dev = data_preprocess.cross_validate_data(\n",
    "#     x_onehot, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 2000\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(x_train_seg)\n",
    "x_train_tokens = tokenizer.texts_to_sequences(x_train_seg)\n",
    "x_test_tokens = tokenizer.texts_to_sequences(x_test_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max token number: 17\n"
     ]
    }
   ],
   "source": [
    "num_tokens = [len(tokens) for tokens in x_train_tokens+x_test_tokens]\n",
    "max_tokens = np.max(num_tokens)\n",
    "print('max token number: '+str(max_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = 'post'\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens, padding=pad, truncating=pad)\n",
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens, padding=pad, truncating=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_string(tokens):\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().isoformat()\n",
    "out_dir = os.path.abspath(os.path.join(\n",
    "            os.path.curdir, \"runs_2\", timestamp, \"summaries\"))\n",
    "tb = TensorBoard(log_dir=out_dir, histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 17, 100)      200000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv1D)                 (None, 17, 64)       19264       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv1D)                 (None, 17, 64)       25664       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_3 (Conv1D)                 (None, 17, 64)       32064       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1, 64)        0           conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1, 64)        0           conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 64)        0           conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 192)       0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 1)         193         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1)            0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 277,185\n",
      "Trainable params: 277,185\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /home/yyliu/anaconda3/envs/NLP/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py:1557: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Train on 705 samples, validate on 79 samples\n",
      "Epoch 1/10\n",
      "705/705 [==============================]705/705 [==============================] - 1s 2ms/step - loss: 0.7498 - acc: 0.5589 - val_loss: 0.4627 - val_acc: 0.9620\n",
      "\n",
      "Epoch 2/10\n",
      "705/705 [==============================]705/705 [==============================] - 0s 252us/step - loss: 0.6184 - acc: 0.6525 - val_loss: 0.2979 - val_acc: 1.0000\n",
      "\n",
      "Epoch 3/10\n",
      "705/705 [==============================]705/705 [==============================] - 0s 254us/step - loss: 0.5964 - acc: 0.6355 - val_loss: 0.7551 - val_acc: 0.4937\n",
      "\n",
      "Epoch 4/10\n",
      "705/705 [==============================]705/705 [==============================] - 0s 391us/step - loss: 0.5074 - acc: 0.7674 - val_loss: 0.4404 - val_acc: 0.8101\n",
      "\n",
      "Epoch 5/10\n",
      "705/705 [==============================]705/705 [==============================] - 0s 260us/step - loss: 0.4547 - acc: 0.8071 - val_loss: 0.5528 - val_acc: 0.7215\n",
      "\n",
      "Epoch 6/10\n",
      "705/705 [==============================]705/705 [==============================] - 0s 266us/step - loss: 0.3942 - acc: 0.8511 - val_loss: 0.8124 - val_acc: 0.5190\n",
      "\n",
      "Epoch 7/10\n",
      "705/705 [==============================]705/705 [==============================] - 0s 377us/step - loss: 0.3312 - acc: 0.9050 - val_loss: 0.3035 - val_acc: 0.8861\n",
      "\n",
      "Epoch 8/10\n",
      "705/705 [==============================]705/705 [==============================] - 0s 250us/step - loss: 0.3114 - acc: 0.8965 - val_loss: 0.9262 - val_acc: 0.4557\n",
      "\n",
      "Epoch 9/10\n",
      "705/705 [==============================]705/705 [==============================] - 0s 248us/step - loss: 0.2969 - acc: 0.8879 - val_loss: 0.6390 - val_acc: 0.6582\n",
      "\n",
      "Epoch 10/10\n",
      "705/705 [==============================]705/705 [==============================] - 0s 278us/step - loss: 0.2987 - acc: 0.8738 - val_loss: 0.4347 - val_acc: 0.7595\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x7fb4e2f76898>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = Input(shape=(SEQUENCE_LENGTH,))\n",
    "net = inputs\n",
    "net = Embedding(input_dim=num_words, \n",
    "                   output_dim=EMBEDDING_DIM, \n",
    "                   input_length=max_tokens)(net)\n",
    "pathway1 = Conv1D(kernel_size=3, strides=1, filters=64, padding='same', \n",
    "            activation='relu', name='conv_1')(net)\n",
    "pathway1 = MaxPool1D(pool_size=SEQUENCE_LENGTH)(pathway1)\n",
    "pathway2 = Conv1D(kernel_size=4, strides=1, filters=64, padding='same', \n",
    "            activation='relu', name='conv_2')(net)\n",
    "pathway2 = MaxPool1D(pool_size=SEQUENCE_LENGTH)(pathway2)\n",
    "pathway3 = Conv1D(kernel_size=5, strides=1, filters=64, padding='same', \n",
    "            activation='relu', name='conv_3')(net)\n",
    "pathway3 = MaxPool1D(pool_size=SEQUENCE_LENGTH)(pathway3)\n",
    "net = concatenate([pathway1, pathway2, pathway3], axis=2)\n",
    "net = Dense(1, activation='sigmoid')(net)\n",
    "net = Flatten()(net)\n",
    "outputs = net\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train_pad, y_train,\n",
    "          validation_split=0.1, epochs=10, batch_size=32, \n",
    "          shuffle=True, \n",
    "          callbacks=[tb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================]89/89 [==============================] - 0s 221us/step\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(x_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.78%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x=x_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in y_pred])\n",
    "cls_true = np.array(y_test)\n",
    "\n",
    "incorrect = np.where(cls_pred != cls_true)\n",
    "incorrect = incorrect[0]\n",
    "print(len(incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 49, 51, 57, 60, 63, 69, 71, 72, 73, 76, 78, 80, 81, 83, 84, 86,\n",
       "       87])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_idx = [x_test_seg[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30287164],\n",
       "       [0.8521873 ],\n",
       "       [0.51299137],\n",
       "       [0.69842285],\n",
       "       [0.5369447 ],\n",
       "       [0.611333  ],\n",
       "       [0.6506513 ],\n",
       "       [0.5599379 ],\n",
       "       [0.57345104],\n",
       "       [0.8440187 ],\n",
       "       [0.8743253 ],\n",
       "       [0.5661764 ],\n",
       "       [0.9264749 ],\n",
       "       [0.8389887 ],\n",
       "       [0.50526226],\n",
       "       [0.650428  ],\n",
       "       [0.5260157 ],\n",
       "       [0.52443296]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_true[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['流理台 的 櫃子',\n",
       " '威廉 太太 實在 太高興 了',\n",
       " '完全 就 忘記 把 水龍頭 給關 了 起來',\n",
       " '再 一個 廚房 內',\n",
       " '以至於 沒 辦法 流通',\n",
       " '在 媽媽 背後',\n",
       " '椅子 搖晃 快 掉下來 了',\n",
       " '想 幫 男孩 拿 東西',\n",
       " '媽媽 在 擦乾 盤子',\n",
       " '水龍頭 沒 關',\n",
       " '水沒關 都 滿 出來 了',\n",
       " '媽媽 在 擦 盤子',\n",
       " '水已 滿 出來 了',\n",
       " '流 滿地',\n",
       " '想到 出神',\n",
       " '兄妹 在 玩耍',\n",
       " '打開 了 櫃子 要 拿 東西',\n",
       " '要 妹妹 接著']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
