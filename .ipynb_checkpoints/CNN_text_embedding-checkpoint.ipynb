{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yyliu/anaconda3/envs/NLP/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Building prefix dict from /home/yyliu/code/NLP/data/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u74f96b08eeb68fe4b0ac4c13a6f276ed.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of train set: 873\n",
      "sentence number of dementia subject: 442\n",
      "sentence number of control normal subject: 431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.132 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import data_preprocess\n",
    "CONTROL_TOTAL = 'control_origin.txt'\n",
    "DEMENTIA_TOTAL = 'dementia_origin.txt'\n",
    "x_train, y_train = data_preprocess.read_sentence(DEMENTIA_TOTAL, CONTROL_TOTAL)\n",
    "y_train = data_preprocess.label_to_scalar(y_train)\n",
    "x_train_seg = data_preprocess.segmentation(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v_model = gensim.models.Word2Vec.load('../wordvec_model/500features_20context_20mincount_zht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(10000)\n",
    "tokenizer.fit_on_texts(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tokens = tokenizer.texts_to_sequences(x_train_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9851088201603666\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "num_tokens = [len(tokens) for tokens in x_train_tokens]\n",
    "num_tokens = np.array(num_tokens)\n",
    "pad_len = np.mean(num_tokens) + 3*np.std(num_tokens)\n",
    "pad_len = int(pad_len)\n",
    "print(np.sum(num_tokens<pad_len)/len(num_tokens))\n",
    "print(pad_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=pad_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENTH_LENGTH = pad_len\n",
    "EMBEDDING_DIM = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_model(embedding_matrix, pad_len):\n",
    "    # Architect Model\n",
    "    inputs = Input(shape=(pad_len,))\n",
    "    net = inputs\n",
    "    if True:\n",
    "        net = Embedding(input_dim=num_words,\n",
    "                        output_dim=EMBEDDING_DIM,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=SEQUENCE_LENGTH,\n",
    "                        trainable=False)(net)\n",
    "    else:\n",
    "        net = Embedding(input_dim=num_words,\n",
    "                        output_dim=EMBEDDING_DIM,\n",
    "                        input_length=SEQUENCE_LENGTH)(net)\n",
    "\n",
    "    pathway1 = Conv1D(kernel_size=3, strides=1, filters=64, padding='same',\n",
    "                      activation='relu', name='conv_1')(net)\n",
    "    pathway1 = MaxPool1D(pool_size=SEQUENCE_LENGTH)(pathway1)\n",
    "    pathway2 = Conv1D(kernel_size=4, strides=1, filters=64, padding='same',\n",
    "                      activation='relu', name='conv_2')(net)\n",
    "    pathway2 = MaxPool1D(pool_size=SEQUENCE_LENGTH)(pathway2)\n",
    "    pathway3 = Conv1D(kernel_size=5, strides=1, filters=64, padding='same',\n",
    "                      activation='relu', name='conv_3')(net)\n",
    "    pathway3 = MaxPool1D(pool_size=SEQUENCE_LENGTH)(pathway3)\n",
    "    net = concatenate([pathway1, pathway2, pathway3], axis=2)\n",
    "    if DROPOUT_LAYER:\n",
    "        net = Dropout(rate=dropout_rate)(net)\n",
    "    if LSTM_LAYER:\n",
    "        if DROPOUT_LAYER:\n",
    "            net = LSTM(units=32, return_sequences=True,\n",
    "                       name='LSTM_1', dropout=dropout_rate)(net)\n",
    "            net = LSTM(units=8, name='LSTM_2', dropout=dropout_rate)(net)\n",
    "        else:\n",
    "            net = LSTM(units=32, return_sequences=True, name='LSTM_1')(net)\n",
    "            net = LSTM(units=8, name='LSTM_2')(net)\n",
    "\n",
    "    net = Dense(1, activation='sigmoid')(net)\n",
    "    net = Flatten()(net)\n",
    "    outputs = net\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='Adam', loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_val(x_train_pad, y_train, n_folds):\n",
    "    # K fold cross validation\n",
    "    skf = StratifiedKFold(y_train, n_folds=n_folds, shuffle=True)\n",
    "    best_model = None\n",
    "    last_acc = 0\n",
    "    acc_avg = 0\n",
    "    word_dict = w2v_model.wv.vocab\n",
    "    word_index = tokenizer.word_index\n",
    "    word_embedding = word_dict\n",
    "#     for word, i in word_index.items():\n",
    "#         embedding_vector = word_index.get(word)\n",
    "#         if embedding_vector is not None:\n",
    "#             word_embedding[i] = embedding_vector\n",
    "    for i, (train, val) in enumerate(skf):\n",
    "        print('Running fold: ', str(i+1))\n",
    "        model = get_nn_model(word_embedding, pad_len)\n",
    "        model.fit(x_train_pad[train], y_train[train],\n",
    "                  epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True, verbose=2, callbacks=[tb])\n",
    "        result = model.evaluate(x_train_pad[val], y_train[val])\n",
    "\n",
    "        print('Validation acc: {}'.format(result[1]))\n",
    "        acc_avg += result[1]\n",
    "        if result[1] > last_acc:\n",
    "            best_model = model\n",
    "            y_pred = model.predict(x_train_pad[val])\n",
    "            plot_roc_curve(y_train[val], y_pred, out_dir)\n",
    "        last_acc = result[1]\n",
    "    acc_avg /= n_folds\n",
    "    return best_model, acc_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold:  1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PRE_TRAINED' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-c2038b7dbb06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk_fold_cross_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-65-87ab3defcc55>\u001b[0m in \u001b[0;36mk_fold_cross_val\u001b[0;34m(x_train_pad, y_train, n_folds)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running fold: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_nn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         model.fit(x_train_pad[train], y_train[train],\n\u001b[1;32m     18\u001b[0m                   epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True, verbose=2, callbacks=[tb])\n",
      "\u001b[0;32m<ipython-input-42-58bd5f78da89>\u001b[0m in \u001b[0;36mget_nn_model\u001b[0;34m(embedding_matrix, pad_len)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mPRE_TRAINED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         net = Embedding(input_dim=num_words,\n\u001b[1;32m      7\u001b[0m                         \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PRE_TRAINED' is not defined"
     ]
    }
   ],
   "source": [
    "# keras module\n",
    "from tensorflow.python.keras.models import Model, load_model\n",
    "from tensorflow.python.keras.layers import Dense, LSTM, Embedding, Input, Conv1D, MaxPool1D, concatenate, Flatten, Dropout\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "#sklearn module\n",
    "from sklearn.metrics import roc_auc_score, auc, roc_curve\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "model, acc_avg = k_fold_cross_val(x_train_pad, y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index.get('得')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
