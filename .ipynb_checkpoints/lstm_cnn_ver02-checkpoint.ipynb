{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/yyliu/code/NLP/data/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u74f96b08eeb68fe4b0ac4c13a6f276ed.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of train set: 784\n",
      "sentence number of dementia subject: 394\n",
      "sentence number of control normal subject: 390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.164 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of train set: 89\n",
      "sentence number of dementia subject: 48\n",
      "sentence number of control normal subject: 41\n"
     ]
    }
   ],
   "source": [
    "import data_preprocess\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# global variables\n",
    "SEQUENCE_LENGTH = 17\n",
    "EMBEDDING_DIM = 100\n",
    "CONTROL_TRAIN = 'control.txt'\n",
    "DEMENTIA_TRAIN = 'dementia.txt'\n",
    "CONTROL_TEST = 'control_test.txt'\n",
    "DEMENTIA_TEST = 'dementia_test.txt'\n",
    "# W2V_MODEL = '100features_20context_20mincount_zht'\n",
    "\n",
    "# data preprocessing, load model, w2v_lookup_table, train_x, train_y, train_x_seg, train_x_onehot\n",
    "\n",
    "# w2v_model = data_preprocess.load_wordvec_model(W2V_MODEL)\n",
    "train_x, train_y = data_preprocess.read_sentence_single_label(DEMENTIA_TRAIN, CONTROL_TRAIN)\n",
    "train_x_seg = data_preprocess.segmentation(train_x)\n",
    "test_x, test_y = data_preprocess.read_sentence_single_label(DEMENTIA_TEST, CONTROL_TEST)\n",
    "test_x_seg = data_preprocess.segmentation(test_x)\n",
    "\n",
    "# x_onehot, vocab_processor = data_preprocess.text_to_onehot(x_seg)\n",
    "\n",
    "# Split data into train and validate part\n",
    "\n",
    "# x_train, x_dev, y_train, y_dev = data_preprocess.cross_validate_data(\n",
    "#     x_onehot, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 1000\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(train_x_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tokens = tokenizer.texts_to_sequences(train_x_seg)\n",
    "x_test_tokens = tokenizer.texts_to_sequences(test_x_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max token number: 17\n"
     ]
    }
   ],
   "source": [
    "num_tokens = [len(tokens) for tokens in x_train_tokens+x_test_tokens]\n",
    "max_tokens = np.max(num_tokens)\n",
    "print('max token number: '+str(max_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = 'post'\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens, padding=pad, truncating=pad)\n",
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens, padding=pad, truncating=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 17)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 17)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([399, 400,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_string(tokens):\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'一個 媽媽 兩個 小孩'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_seg[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'一個 媽媽 兩個 小孩'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_string(x_train_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(input_dim=num_words, \n",
    "                   output_dim=embedding_size, \n",
    "                   input_length=max_tokens, \n",
    "                   name='layer_embedding'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-33c388a57799>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model.add(Conv1D(kernel_size=3, strides=1, filters=64, padding='same', \n\u001b[1;32m      5\u001b[0m                 activation='relu', name='layer_conv1'))\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Model'"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.layers import Reshape, MaxPooling1D\n",
    "from tensorflow.python.keras.layers import Conv1D, Dense, Flatten, concatenate\n",
    "model.add(Reshape((17,8)))\n",
    "model.add(Conv1D(kernel_size=3, strides=1, filters=64, padding='same', \n",
    "                activation='relu', name='layer_conv1'))\n",
    "model.add(MaxPooling1D(pool_size=17))\n",
    "# model.add(Flatten())\n",
    "model.add(GRU(units=16, return_sequences=True))\n",
    "model.add(GRU(units=8, return_sequences=True))\n",
    "model.add(GRU(units=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 17, 8)        8000        input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 17, 64)       1600        embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 17, 64)       2112        embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D) (None, 1, 64)        0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling1D) (None, 1, 64)        0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 1, 128)       0           max_pooling1d_30[0][0]           \n",
      "                                                                 max_pooling1d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1, 1)         129         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 1)            0           dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 11,841\n",
      "Trainable params: 11,841\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 744 samples, validate on 40 samples\n",
      "Epoch 1/10\n",
      "744/744 [==============================]744/744 [==============================] - 1s 2ms/step - loss: 0.6845 - acc: 0.5470 - val_loss: 0.7036 - val_acc: 0.4500\n",
      "\n",
      "Epoch 2/10\n",
      "744/744 [==============================]744/744 [==============================] - 0s 218us/step - loss: 0.6648 - acc: 0.6358 - val_loss: 0.7864 - val_acc: 0.1250\n",
      "\n",
      "Epoch 3/10\n",
      "744/744 [==============================]744/744 [==============================] - 0s 210us/step - loss: 0.6493 - acc: 0.6828 - val_loss: 0.7016 - val_acc: 0.4500\n",
      "\n",
      "Epoch 4/10\n",
      "744/744 [==============================]744/744 [==============================] - 0s 268us/step - loss: 0.6291 - acc: 0.7083 - val_loss: 0.6631 - val_acc: 0.7000\n",
      "\n",
      "Epoch 5/10\n",
      "744/744 [==============================]744/744 [==============================] - 0s 246us/step - loss: 0.6050 - acc: 0.7634 - val_loss: 0.6320 - val_acc: 0.7000\n",
      "\n",
      "Epoch 6/10\n",
      "744/744 [==============================]744/744 [==============================] - 0s 205us/step - loss: 0.5776 - acc: 0.7823 - val_loss: 0.6719 - val_acc: 0.6250\n",
      "\n",
      "Epoch 7/10\n",
      "744/744 [==============================]744/744 [==============================] - 0s 217us/step - loss: 0.5497 - acc: 0.7823 - val_loss: 0.7000 - val_acc: 0.5750\n",
      "\n",
      "Epoch 8/10\n",
      "744/744 [==============================]744/744 [==============================] - 0s 216us/step - loss: 0.5156 - acc: 0.7944 - val_loss: 0.6455 - val_acc: 0.6250\n",
      "\n",
      "Epoch 9/10\n",
      "744/744 [==============================]744/744 [==============================] - 0s 210us/step - loss: 0.4827 - acc: 0.7970 - val_loss: 0.5744 - val_acc: 0.7250\n",
      "\n",
      "Epoch 10/10\n",
      "744/744 [==============================]744/744 [==============================] - 0s 215us/step - loss: 0.4349 - acc: 0.8374 - val_loss: 0.5686 - val_acc: 0.7250\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x7f18740aa048>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = Input(shape=(SEQUENCE_LENGTH,))\n",
    "net = inputs\n",
    "net = Embedding(input_dim=num_words, \n",
    "                   output_dim=embedding_size, \n",
    "                   input_length=max_tokens)(net)\n",
    "pathway1 = Conv1D(kernel_size=3, strides=1, filters=64, padding='same', \n",
    "            activation='relu')(net)\n",
    "pathway1 = MaxPooling1D(pool_size=SEQUENCE_LENGTH)(pathway1)\n",
    "pathway2 = Conv1D(kernel_size=4, strides=1, filters=64, padding='same', \n",
    "            activation='relu')(net)\n",
    "pathway2 = MaxPooling1D(pool_size=SEQUENCE_LENGTH)(pathway2)\n",
    "net = concatenate([pathway1, pathway2], axis=2)\n",
    "net = Dense(1, activation='sigmoid')(net)\n",
    "net = Flatten()(net)\n",
    "outputs = net\n",
    "model2 = Model(inputs=inputs, outputs=outputs)\n",
    "model2.summary()\n",
    "model2.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model2.fit(x_train_pad, train_y,\n",
    "          validation_split=0.05, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yyliu/anaconda3/envs/NLP/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py:1557: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_embedding (Embedding)  (None, 17, 8)             8000      \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 17, 8)             0         \n",
      "_________________________________________________________________\n",
      "layer_conv1 (Conv1D)         (None, 17, 64)            1600      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, None, 16)          3888      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, None, 8)           600       \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 4)                 156       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 14,249\n",
      "Trainable params: 14,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))\n",
    "optimizer = Adam(lr=1e-3)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 744 samples, validate on 40 samples\n",
      "Epoch 1/10\n",
      "744/744 [==============================]744/744 [==============================] - 3s 4ms/step - loss: 0.6920 - acc: 0.5296 - val_loss: 0.7465 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 2/10\n",
      "744/744 [==============================]744/744 [==============================] - 0s 373us/step - loss: 0.6891 - acc: 0.5296 - val_loss: 0.7315 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 3/10\n",
      "744/744 [==============================]744/744 [==============================] - 0s 391us/step - loss: 0.6852 - acc: 0.5511 - val_loss: 0.7041 - val_acc: 0.3000\n",
      "\n",
      "Epoch 4/10\n",
      "744/744 [==============================]744/744 [==============================] - 0s 510us/step - loss: 0.6706 - acc: 0.5914 - val_loss: 0.7136 - val_acc: 0.3000\n",
      "\n",
      "Epoch 5/10\n",
      "744/744 [==============================]744/744 [==============================] - 0s 392us/step - loss: 0.6228 - acc: 0.7487 - val_loss: 0.6285 - val_acc: 0.6750\n",
      "\n",
      "Epoch 6/10\n",
      "744/744 [==============================]744/744 [==============================] - 0s 380us/step - loss: 0.5295 - acc: 0.8024 - val_loss: 0.5767 - val_acc: 0.7250\n",
      "\n",
      "Epoch 7/10\n",
      "744/744 [==============================]744/744 [==============================] - 0s 396us/step - loss: 0.4463 - acc: 0.8414 - val_loss: 0.4367 - val_acc: 0.8250\n",
      "\n",
      "Epoch 8/10\n",
      "744/744 [==============================]744/744 [==============================] - 0s 381us/step - loss: 0.3767 - acc: 0.8683 - val_loss: 0.5654 - val_acc: 0.7250\n",
      "\n",
      "Epoch 9/10\n",
      "744/744 [==============================]744/744 [==============================] - 0s 369us/step - loss: 0.3293 - acc: 0.8925 - val_loss: 0.5534 - val_acc: 0.7250\n",
      "\n",
      "Epoch 10/10\n",
      "744/744 [==============================]744/744 [==============================] - 0s 386us/step - loss: 0.2942 - acc: 0.9086 - val_loss: 0.8698 - val_acc: 0.6000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x7f18a7163f98>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_pad, train_y,\n",
    "          validation_split=0.05, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================]89/89 [==============================] - 0s 203us/step\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(x_test_pad, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.03%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x=x_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in y_pred])\n",
    "cls_true = np.array(test_y)\n",
    "\n",
    "incorrect = np.where(cls_pred != cls_true)\n",
    "incorrect = incorrect[0]\n",
    "print(len(incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 49, 52, 54, 55, 57, 62, 63, 67, 68, 69, 71, 72, 73, 74, 75, 76,\n",
       "       78, 80, 81, 82, 84, 86, 87])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_idx = [test_x_seg[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17404304],\n",
       "       [0.92354447],\n",
       "       [0.65409404],\n",
       "       [0.6800942 ],\n",
       "       [0.9180694 ],\n",
       "       [0.9157247 ],\n",
       "       [0.76814157],\n",
       "       [0.76862955],\n",
       "       [0.89766973],\n",
       "       [0.5328914 ],\n",
       "       [0.9062824 ],\n",
       "       [0.7374913 ],\n",
       "       [0.73067755],\n",
       "       [0.905675  ],\n",
       "       [0.8665853 ],\n",
       "       [0.5985602 ],\n",
       "       [0.92865694],\n",
       "       [0.88582766],\n",
       "       [0.92613363],\n",
       "       [0.9160244 ],\n",
       "       [0.9262324 ],\n",
       "       [0.5835277 ],\n",
       "       [0.9099484 ],\n",
       "       [0.9149707 ]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_true[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['流理台 的 櫃子',\n",
       " '威廉 太太 實在 太高興 了',\n",
       " '後頭 的 小 湯姆 和 珍妮 趁著 威廉 太太 不 注意 的 時候',\n",
       " '還秀 了 一下 單腳 椅 站立 的 特技',\n",
       " '也許 可以 給 爸爸 看看',\n",
       " '再 一個 廚房 內',\n",
       " '甚至 淹到 地面',\n",
       " '在 媽媽 背後',\n",
       " '準備 要 摔 落 了',\n",
       " '小男孩 在 站 椅子 上 想 拿 東西',\n",
       " '椅子 搖晃 快 掉下來 了',\n",
       " '想 幫 男孩 拿 東西',\n",
       " '媽媽 在 擦乾 盤子',\n",
       " '水龍頭 沒 關',\n",
       " '水從 洗手 槽 滿出',\n",
       " '媽媽 在 洗碗',\n",
       " '水沒關 都 滿 出來 了',\n",
       " '媽媽 在 擦 盤子',\n",
       " '水已 滿 出來 了',\n",
       " '流 滿地',\n",
       " '媽媽 不 知道 想 什麼',\n",
       " '兄妹 在 玩耍',\n",
       " '打開 了 櫃子 要 拿 東西',\n",
       " '要 妹妹 接著']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
