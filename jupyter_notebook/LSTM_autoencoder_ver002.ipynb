{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gensim\n",
    "import numpy as np\n",
    "import string\n",
    "from opencc import OpenCC\n",
    "import ckip\n",
    "import jieba\n",
    "# Path of files\n",
    "SENTENCE_DICT = \"../../pickle/sentence_dict.pickle\"\n",
    "WORDVEC_MODEL = '../../wordvec_model/'\n",
    "# Variables\n",
    "DEMENTIA_NUM = 51\n",
    "CONTROL_NUM = 51\n",
    "WV_DIIM = 500\n",
    "\n",
    "def read_sentence_file(file_name=None):\n",
    "    with open(SENTENCE_DICT, 'rb') as f:\n",
    "        sentence_dict = pickle.load(f)\n",
    "        print(\"Load sentence text data ...\")\n",
    "    return sentence_dict\n",
    "\n",
    "def load_wordvec_model(file_name):\n",
    "    w2v_model = gensim.models.Word2Vec.load(WORDVEC_MODEL+file_name)\n",
    "    words = []\n",
    "    for word in w2v_model.wv.vocab:\n",
    "        words.append(word)\n",
    "    print('Load word2vec model sucess ...')\n",
    "    print('Number of token: {}'.format(len(words)))\n",
    "    print('Dimensions of word vector: {}'.format(len(w2v_model[words[0]])))\n",
    "    return w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load sentence text data ...\n"
     ]
    }
   ],
   "source": [
    "sentence_dict = read_sentence_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load word2vec model sucess ...\n",
      "Number of token: 259638\n",
      "Dimensions of word vector: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yyliu/anaconda3/envs/NLP/lib/python3.6/site-packages/ipykernel_launcher.py:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "w2v_model = load_wordvec_model('500features_20context_20mincount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import Dense, LSTM, Embedding, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = np.asarray(w2v_model.wv.syn0)\n",
    "# layer = Embedding(input_dim=weights.shape[0], \n",
    "#                  output_dim=weights.shape[1], \n",
    "#                  weights = [weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3個人', '一個媽媽兩個小孩', '小孩站在椅子上要拿西點', '椅子都快倒下來了', '在拿這個西點餅乾要吃', '手下還拿著一塊']\n"
     ]
    }
   ],
   "source": [
    "def split_punctuation(sentence):\n",
    "    punctuation = set(string.punctuation+\"，\"+\"、\"+\"」\"+\"「\"+\"。\"+\" \"+\"！\")\n",
    "    sentence_split = []\n",
    "    tmp = ''\n",
    "    for i in sentence:\n",
    "        if i not in punctuation:\n",
    "            tmp += i\n",
    "        else:\n",
    "            sentence_split.append(tmp)\n",
    "            tmp = ''\n",
    "    return sentence_split\n",
    "sentence = '3個人，一個媽媽兩個小孩，小孩站在椅子上要拿西點，椅子都快倒下來了，在拿這個西點餅乾要吃，手下還拿著一塊，'\n",
    "print(split_punctuation(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of train set: 873\n",
      "sentence number of dementia subject: 442\n",
      "sentence number of control normal subject: 431\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/dementia.txt', encoding='utf8') as f:\n",
    "    dementia_txt = f.readlines()\n",
    "sentence = []\n",
    "for i in range(len(dementia_txt)):\n",
    "    if i%2==0:\n",
    "        sentence.extend(split_punctuation(dementia_txt[i+1]))\n",
    "dementia_num = len(sentence)\n",
    "with open('../../data/control_51.txt', encoding='utf8') as f:\n",
    "    control_txt = f.readlines()\n",
    "for i in range(len(control_txt)):\n",
    "    if i%2==0:\n",
    "        sentence.extend(split_punctuation(control_txt[i+1]))\n",
    "control_num = len(sentence) - dementia_num\n",
    "############\n",
    "# train set#\n",
    "############\n",
    "train_data = np.array(sentence)\n",
    "train_y = np.zeros((train_data.shape[0]))\n",
    "train_y[dementia_num:] = 1.0\n",
    "print('total number of train set: {}'.format(train_data.shape[0]))\n",
    "print('sentence number of dementia subject: {}'.format(len(train_y[train_y==0])))\n",
    "print('sentence number of control normal subject: {}'.format(len(train_y[train_y==1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/yyliu/code/NLP/data/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u74f96b08eeb68fe4b0ac4c13a6f276ed.cache\n",
      "Loading model cost 1.162 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['手下', '還拿著', '一塊']\n"
     ]
    }
   ],
   "source": [
    "JIEBA_DICT = '../../data/dict.txt.big'\n",
    "jieba.set_dictionary(JIEBA_DICT)\n",
    "train_data_seg = []\n",
    "for i in train_data:\n",
    "    train_data_seg.append(jieba.lcut(i))\n",
    "print(train_data_seg[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token number of sentence: 17\n",
      "Min token number of sentence: 1\n",
      "Mean token number of sentence: 5.747995418098511\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LENGTH = 20\n",
    "VOCAB_DIM = 500\n",
    "train_data_seg_array = np.array(train_data_seg)\n",
    "l = []\n",
    "for i in range(len(train_data_seg_array)):\n",
    "    l.append(len(train_data_seg_array[i]))\n",
    "#     if len(train_data_seg_array[i])==1:\n",
    "#         print(i, train_data_seg_array[i])\n",
    "print('Max token number of sentence: {}'.format(np.max(l)))\n",
    "print('Min token number of sentence: {}'.format(np.min(l)))\n",
    "print('Mean token number of sentence: {}'.format(np.mean(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = []\n",
    "for s in train_data_seg:\n",
    "    token_list = []\n",
    "    for token in s:\n",
    "        if token in w2v_model.wv.vocab:\n",
    "            token_list.append(np.asarray(w2v_model.wv[token]))\n",
    "    if len(token_list) < SEQUENCE_LENGTH:\n",
    "        for i in range(SEQUENCE_LENGTH - len(token_list)):\n",
    "            token_list.append(np.zeros(shape=VOCAB_DIM))\n",
    "    train_vec.append([token_list[0:SEQUENCE_LENGTH]])\n",
    "# seg_sentence_vec = []\n",
    "# for key, s in seg_sentence.items():\n",
    "#     token_list = []\n",
    "#     for token in s:\n",
    "#         if token in w2v_model.wv.vocab:\n",
    "#             token_list.append(np.asarray(w2v_model.wv[token]))\n",
    "#     if len(token_list) < SEQUENCE_LENGTH:\n",
    "#         for i in range(SEQUENCE_LENGTH - len(token_list)):\n",
    "#             token_list.append(np.zeros(shape=VOCAB_DIM))\n",
    "# #             token_list.append(np.zeros(shape=(VOCAB_DIM, 1)).tolist())\n",
    "#     seg_sentence_vec.append([token_list[0:SEQUENCE_LENGTH]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(873, 20, 500)\n",
      "[[-7.92692825e-02  1.57746319e-02  5.27736768e-02 ...  3.76927592e-02\n",
      "   7.03343600e-02  3.61793354e-04]\n",
      " [ 1.03810102e-01 -3.23520899e-02  1.29130320e-03 ... -2.70457361e-02\n",
      "  -3.82427163e-02  1.65345892e-02]\n",
      " [-1.31187662e-02 -1.90271542e-03 -6.02233633e-02 ...  2.55492123e-05\n",
      "   2.02863179e-02  1.29380105e-02]\n",
      " ...\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "train_vec = np.asarray(train_vec).reshape(len(train_data_seg),-1,500)\n",
    "print(train_vec.shape)\n",
    "print(train_vec[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def length(sequence):\n",
    "#   used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
    "#   length = tf.reduce_sum(used, 1)\n",
    "#   length = tf.cast(length, tf.int32)\n",
    "#   return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # encoder\n",
    "# output, state = tf.nn.dynamic_rnn(\n",
    "#     tf.contrib.rnn.GRUCell(200),\n",
    "#     tf_x,\n",
    "#     dtype=tf.float32,\n",
    "#     sequence_length=length(tf_x)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCH = 10\n",
    "SAVE = 0\n",
    "TOTAL_NUM = train_vec.shape[0]\n",
    "from tensorflow.python.keras import optimizers\n",
    "def cnn(text_vec):\n",
    "    data = text_vec\n",
    "    data = np.asarray(data).reshape(TOTAL_NUM, SEQUENCE_LENGTH, WV_DIIM)\n",
    "    print('Data shape: ', len(data))\n",
    "    print(data.shape)\n",
    "    seq_input = Input(shape=(SEQUENCE_LENGTH, WV_DIIM))\n",
    "    encoded = LSTM(layer1, input_shape=(SEQUENCE_LENGTH, WV_DIIM), \n",
    "                  return_sequences=True)(seq_input)\n",
    "    decoded = LSTM(layer1, return_sequences=True)(encoded)\n",
    "    decoded2 = LSTM(WV_DIIM, return_sequences=True)(decoded)\n",
    "    autoencoder = Model(seq_input, decoded2)\n",
    "    encoder = Model(seq_input, encoded)\n",
    "    rmsprop = optimizers.RMSprop(lr=1e-5)\n",
    "    autoencoder.compile(loss='cosine', optimizer=rmsprop)\n",
    "    train_history = autoencoder.fit(data, data, batch_size = BATCH_SIZE, \n",
    "                                    epochs=EPOCH)\n",
    "    encoder_op = encoder.predict(data)\n",
    "    print(autoencoder.summary())\n",
    "    if SAVE==1:\n",
    "        np.savetxt('encoder_dim500_'+str(layer1)+'.csv', encoder_op[:,-1], delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCH = 10\n",
    "SAVE = 0\n",
    "TOTAL_NUM = train_vec.shape[0]\n",
    "from tensorflow.python.keras import optimizers\n",
    "def get_sentence_vec(text_vec, layer1):\n",
    "    data = text_vec\n",
    "    data = np.asarray(data).reshape(TOTAL_NUM, SEQUENCE_LENGTH, WV_DIIM)\n",
    "    print('Data shape: ', len(data))\n",
    "    print(data.shape)\n",
    "    seq_input = Input(shape=(SEQUENCE_LENGTH, WV_DIIM))\n",
    "    encoded = LSTM(layer1, input_shape=(SEQUENCE_LENGTH, WV_DIIM), \n",
    "                  return_sequences=True)(seq_input)\n",
    "    decoded = LSTM(layer1, return_sequences=True)(encoded)\n",
    "    decoded2 = LSTM(WV_DIIM, return_sequences=True)(decoded)\n",
    "    autoencoder = Model(seq_input, decoded2)\n",
    "    encoder = Model(seq_input, encoded)\n",
    "    rmsprop = optimizers.RMSprop(lr=1e-5)\n",
    "    autoencoder.compile(loss='cosine', optimizer=rmsprop)\n",
    "    train_history = autoencoder.fit(data, data, batch_size = BATCH_SIZE, \n",
    "                                    epochs=EPOCH)\n",
    "    encoder_op = encoder.predict(data)\n",
    "    print(autoencoder.summary())\n",
    "    if SAVE==1:\n",
    "        np.savetxt('encoder_dim500_'+str(layer1)+'.csv', encoder_op[:,-1], delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  873\n",
      "(873, 20, 500)\n",
      "WARNING:tensorflow:From /home/yyliu/anaconda3/envs/NLP/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py:1456: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/yyliu/anaconda3/envs/NLP/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py:3229: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "WARNING:tensorflow:From /home/yyliu/anaconda3/envs/NLP/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py:1557: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/10\n",
      "873/873 [==============================]873/873 [==============================] - 23s 26ms/step - loss: -0.0176\n",
      "\n",
      "Epoch 2/10\n",
      "873/873 [==============================]873/873 [==============================] - 2s 3ms/step - loss: -0.0368\n",
      "\n",
      "Epoch 3/10\n",
      "873/873 [==============================]873/873 [==============================] - 2s 3ms/step - loss: -0.0465\n",
      "\n",
      "Epoch 4/10\n",
      "873/873 [==============================]873/873 [==============================] - 2s 3ms/step - loss: -0.0524\n",
      "\n",
      "Epoch 5/10\n",
      "873/873 [==============================]873/873 [==============================] - 2s 3ms/step - loss: -0.0564\n",
      "\n",
      "Epoch 6/10\n",
      "873/873 [==============================]873/873 [==============================] - 2s 2ms/step - loss: -0.0590\n",
      "\n",
      "Epoch 7/10\n",
      "873/873 [==============================]873/873 [==============================] - 3s 3ms/step - loss: -0.0609\n",
      "\n",
      "Epoch 8/10\n",
      "873/873 [==============================]873/873 [==============================] - 2s 3ms/step - loss: -0.0623\n",
      "\n",
      "Epoch 9/10\n",
      "873/873 [==============================]873/873 [==============================] - 2s 3ms/step - loss: -0.0634\n",
      "\n",
      "Epoch 10/10\n",
      "873/873 [==============================]873/873 [==============================] - 2s 3ms/step - loss: -0.0642\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 20, 500)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 20)          41680     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 20)          3280      \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, None, 500)         1042000   \n",
      "=================================================================\n",
      "Total params: 1,086,960\n",
      "Trainable params: 1,086,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "get_sentence_vec(train_vec, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sentence_distance(layer1):\n",
    "    sentence_vec = np.genfromtxt('encoder_dim500_'+str(layer1)+'.csv', delimiter=',')\n",
    "    print(sentence_vec.shape)\n",
    "    distance = []\n",
    "    for i in range(len(sentence_vec)-1):\n",
    "        distance.append(np.linalg.norm(sentence_vec[i]-sentence_vec[i+1]))\n",
    "    print(distance)\n",
    "    print(len(distance))\n",
    "    print('Mean: ', np.mean(distance))\n",
    "    print('Std: ', np.std(distance))\n",
    "    print('Max: ', np.max(distance))\n",
    "    print('Min: ', np.min(distance))\n",
    "    return sentence_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102, 20)\n",
      "[0.0004196578998904263, 1.8948946152676097e-10, 5.042602148925244e-09, 4.841067297379638e-09, 2.7801395007616955e-10, 9.23215523685905e-11, 3.8221787179297974e-05, 3.734022007900015e-05, 1.491073903668597e-06, 2.921503111853068e-10, 7.332580706405269e-11, 6.355123107876756e-10, 5.93693174271185e-10, 2.3655256092037394e-10, 2.794384544492862e-10, 1.4169448764114675e-10, 1.6248303243287842e-08, 1.626332293167452e-08, 3.8911070317032684e-10, 3.114245855108703e-10, 3.812074107534059e-10, 4.256435204297304e-10, 1.5379584988387959e-09, 1.7228288089068883e-09, 4.689607273756498e-10, 1.411854533117837e-09, 1.5114083546534478e-09, 2.5900476613639328e-08, 2.604353106505707e-08, 1.5744506533070236e-10, 4.826872694137851e-10, 3.0939935024097383e-10, 3.1751219461794665e-09, 3.227877548095271e-09, 2.2885980793722103e-09, 2.3794940083810614e-09, 3.1856731626597516e-10, 1.459278509728871e-10, 2.5094411795704767e-08, 2.4981518366557376e-08, 3.798193178297237e-09, 3.6364910242861593e-09, 3.005463739438402e-10, 2.1356670849215696e-10, 7.780704582445343e-10, 6.557987590187487e-09, 7.283468210382388e-09, 2.0180127828438218e-10, 9.923083196180127e-11, 2.6425477727323415e-10, 1.9760119846537652e-10, 1.5757110542987149e-10, 3.3099751320234846e-07, 3.3100180838831297e-07, 5.089531050914276e-10, 2.684473411409019e-10, 9.267925054051009e-11, 8.003553375601768e-11, 1.3096723705530167e-10, 1.1393554894002217e-09, 1.0616230549356259e-09, 3.866539993203019e-10, 2.8233520297084485e-07, 2.083270934236775e-07, 4.4813299891769773e-07, 1.9832457399486195e-09, 2.0722177650200955e-09, 5.836792380218953e-07, 5.823705219194689e-07, 5.415200804028608e-09, 3.5646623539677066e-10, 3.4449892129798335e-09, 3.418632223211802e-09, 1.709559750424757e-10, 6.763283419554349e-09, 6.446141946557044e-06, 6.4514737396587135e-06, 2.1275334277841963e-09, 0.09209652501404773, 0.09209652502578308, 2.724933553171271e-09, 2.7947941996569493e-09, 7.734449036604674e-11, 9.319547430968304e-11, 6.30413023074815e-10, 2.2203583695451108e-08, 2.2731737293313603e-08, 4.31495573231448e-10, 4.028147824477174e-10, 1.4737149849538208e-06, 1.4723901453445095e-06, 2.807155065519924e-09, 2.420008003473426e-10, 5.455027610216803e-10, 8.56717423484459e-07, 8.570232929707118e-07, 1.5019982363877e-08, 1.4465625853065433e-08, 5.831783288260997e-10, 1.5127169089313094e-10, 5.662289337587347e-10]\n",
      "101\n",
      "Mean:  0.001828815768128857\n",
      "Std:  0.012830161564690414\n",
      "Max:  0.09209652502578308\n",
      "Min:  7.332580706405269e-11\n"
     ]
    }
   ],
   "source": [
    "sentence_vec_lstm = print_sentence_distance(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.45270387e-04, -3.21196130e-04,  2.73606274e-04, -4.51140630e-04,\n",
       "        7.26244587e-04, -3.58796999e-04,  6.66977197e-04, -7.66090889e-05,\n",
       "        6.96329516e-04,  5.22243208e-04, -1.99893893e-05, -7.37876864e-04,\n",
       "       -2.89513904e-04, -6.82613172e-04,  5.55726234e-04, -4.92067193e-04,\n",
       "        3.89824912e-04, -5.84699563e-04,  2.70588091e-04,  1.15227493e-04])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_vec_lstm[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean = KMeans(n_clusters=2).fit(sentence_vec_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dementia = kmean.labels_[:DEMENTIA_NUM]\n",
    "control = kmean.labels_[DEMENTIA_NUM:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(dementia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "sentence2vec_array = sentence_vec_lstm\n",
    "file = open('s2v_lstm_array_zht_500dim.pickle', 'wb')\n",
    "pickle.dump(sentence2vec_array, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
