{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = '../wordvec_model/500features_20context_20mincount_zht'\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 259425\n",
      "Dimensions of a word vector: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yyliu/anaconda3/envs/NLP/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "w2v_model = gensim.models.Word2Vec.load(MODEL)\n",
    "words = []\n",
    "for word in w2v_model.wv.vocab:\n",
    "    words.append(word)\n",
    "print(\"Number of tokens: {}\".format(len(words)))\n",
    "print(\"Dimensions of a word vector: {}\".format(len(w2v_model[words[0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PICKLE = \"../pickle/sentence_dict.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "sentence_dict = {}\n",
    "with open(DATA_PICKLE, 'rb') as f:\n",
    "    sentence_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jieba\n",
    "from ckip import CkipSegmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "# from opencc import OpenCC \n",
    "# openCC = OpenCC('tw2s') \n",
    "exclude = set(string.punctuation+'，'+'。'+'、'+'「'+'」'+'？'+'！')\n",
    "segmenter = CkipSegmenter()\n",
    "def sentence2vec(sentence):\n",
    "#     sentence = openCC.convert(sentence)\n",
    "    vector = np.zeros((w2v_model[words[0]].shape))\n",
    "    oov_num = 0\n",
    "    token_sentence = segmenter.seg(sentence)\n",
    "    token_sentence = [t for t in token_sentence.tok if not t in exclude]\n",
    "    for token in token_sentence:\n",
    "        if token in w2v_model.wv.vocab:\n",
    "            vector += w2v_model[token]\n",
    "        else:\n",
    "            oov_num += 1\n",
    "    vector /= len(token_sentence)\n",
    "    return vector, oov_num, token_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_dict = {}\n",
    "oov_dict = {}\n",
    "sentence_token_dict = {}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yyliu/anaconda3/envs/NLP/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n",
      "/home/yyliu/anaconda3/envs/NLP/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "002.\n",
      "003.\n",
      "004.\n",
      "005.\n",
      "006. \n",
      "007.\n",
      "008.\n",
      "009.\n",
      "010.\n",
      "011.\n",
      "012.\n",
      "013.\n",
      "015.\n",
      "016.\n",
      "018.\n",
      "019.\n",
      "020.\n",
      "021.\n",
      "023.\n",
      "024.\n",
      "025.\n",
      "026.\n",
      "027.\n",
      "029.\n",
      "030.\n",
      "032.\n",
      "034.\n",
      "035.\n",
      "037.\n",
      "038.\n",
      "039.\n",
      "041.\n",
      "044.\n",
      "045.\n",
      "046.\n",
      "048.\n",
      "049.\n",
      "050.\n",
      "052.\n",
      "054.\n",
      "055.\n",
      "056.\n",
      "057.\n",
      "058.\n",
      "059.\n",
      "061.\n",
      "061_.\n",
      "063.\n",
      "064.\n",
      "065.\n",
      "066.\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "for k, s in sentence_dict.items():\n",
    "    sv, oov, sentence_token = sentence2vec(s)\n",
    "    print(k)\n",
    "    sv_dict[k] = sv\n",
    "    oov_dict[k] = oov\n",
    "    sentence_token_dict[k] = sentence_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 500)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_dict_array = np.asarray([i for i in sv_dict.values()])\n",
    "sv_dict_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump sentence vector to pickle\n",
    "import pickle\n",
    "sentence2vec_array = sv_dict_array\n",
    "file = open('s2v_array_zht_500dim.pickle', 'wb')\n",
    "pickle.dump(sentence2vec_array, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2).fit(sv_dict_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dementia = model.labels_[:52]\n",
    "control = model.labels_[52:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dementia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00516384, -0.00295752, -0.00394899, -0.00274872,  0.02647947,\n",
       "        0.01796424, -0.01912892,  0.00503733, -0.02157154, -0.01144844])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_dict_array[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.16384397e-03, -2.95752213e-03, -3.94899220e-03, ...,\n",
       "         6.75638869e-03, -1.47672626e-02,  8.33184066e-03],\n",
       "       [-1.48730062e-02, -3.08964702e-03,  4.42411941e-03, ...,\n",
       "         1.76242552e-02, -2.78394790e-02,  6.23656808e-03],\n",
       "       [-2.86147214e-02,  4.22696097e-03, -4.91361124e-03, ...,\n",
       "        -7.36085130e-03, -2.60000491e-02,  1.15221587e-02],\n",
       "       ...,\n",
       "       [-6.87681307e-03, -5.72362235e-03, -2.93707725e-03, ...,\n",
       "         1.11952716e-04, -6.86286214e-03,  1.11245029e-02],\n",
       "       [-1.17271107e-02,  5.78078371e-05, -5.20638298e-03, ...,\n",
       "        -4.69085202e-03,  1.57508070e-04,  4.52246486e-03],\n",
       "       [-4.86595570e-03, -1.47958575e-02,  1.41751823e-04, ...,\n",
       "         1.83943657e-03,  1.99028795e-03, -1.88987839e-02]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_dict_array[:10][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP: (array([ 7,  8,  9, 10, 15, 16, 19, 23, 24, 26, 27, 28, 30, 32, 33, 39, 42,\n",
      "       46, 51]),)19\n",
      "FN: (array([40]),)\n",
      "sklearn kmeans score: -2.9029217995680967\n",
      "Silhouette score: 0.13250291049382792\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "dementia = np.asarray(dementia)\n",
    "control = np.asarray(control)\n",
    "\n",
    "fp_sklearn = np.where(dementia==1)\n",
    "fn_sklearn = np.where(control==0)\n",
    "print(\"FP: \" + str(fp_sklearn)+ str(len(fp_sklearn[0])))\n",
    "print(\"FN: \" + str(fn_sklearn))\n",
    "print(\"sklearn kmeans score: {}\".format(model.score(sv_dict_array)))\n",
    "silhouette_score = metrics.silhouette_score(sv_dict_array, model.labels_, \n",
    "                                           metric='euclidean')\n",
    "print(\"Silhouette score: {}\".format(silhouette_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "kclusterer = KMeansClusterer(2, distance=nltk.cluster.util.cosine_distance,\n",
    "                            repeats=1000)\n",
    "assigned_clusters = kclusterer.cluster(sv_dict_array, assign_clusters=True)\n",
    "print(assigned_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP: (array([ 7,  9, 10, 15, 16, 24, 27, 28, 30, 33, 39, 42, 51]),)\n",
      "FN: (array([15, 40]),)\n"
     ]
    }
   ],
   "source": [
    "dement = np.asarray(assigned_clusters[:52])\n",
    "contr = np.asarray(assigned_clusters[52:])\n",
    "# dement = np.asarray(dement)\n",
    "\n",
    "fp = np.where(dement==1)\n",
    "fn = np.where(contr==0)\n",
    "print(\"FP: \" + str(fp))\n",
    "print(\"FN: \" + str(fn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_keys = []\n",
    "for key in w2v_model.wv.vocab.keys():\n",
    "    vocab_keys.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259425"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yyliu/anaconda3/envs/NLP/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# TSNE to reduce dimension\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vocab = vocab_keys[:1000]\n",
    "X = w2v_model[vocab]\n",
    "tsne = TSNE(n_components=2)\n",
    "result = tsne.fit_transform(X)\n",
    "df = pd.DataFrame(result, index=vocab, columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the scatter word2vec\n",
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png',fonts=None):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    plt.figure(figsize=(25, 25))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                    fontproperties=fonts,\n",
    "                    xy=(x, y),\n",
    "                    xytext=(5, 2),\n",
    "                    textcoords='offset points',\n",
    "                    ha='right',\n",
    "                    va='bottom')\n",
    "    plt.savefig(filename,dpi=600)\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "#为了在图片上能显示出中文\n",
    "font = FontProperties(fname=\"../data/simsun.ttf\", size=14)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "labels = vocab\n",
    "plot_with_labels(result, vocab,fonts=font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
