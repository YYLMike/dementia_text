{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOOKUP_TABLE = \"../data/wiki.zh.vec\"\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 332647\n",
      "Dimensions of a word vector: 300\n"
     ]
    }
   ],
   "source": [
    "w2v_model = KeyedVectors.load_word2vec_format(LOOKUP_TABLE)\n",
    "words = []\n",
    "for word in w2v_model.vocab:\n",
    "    words.append(word)\n",
    "print(\"Number of tokens: {}\".format(len(words)))\n",
    "print(\"Dimensions of a word vector: {}\".format(len(w2v_model[words[0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'的'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model[words[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PICKLE = \"../pickle/sentence_dict.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "sentence_dict = {}\n",
    "with open(DATA_PICKLE, 'rb') as f:\n",
    "    sentence_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ckip import CkipSegmenter\n",
    "segmenter = CkipSegmenter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "exclude = set(string.punctuation+'，'+'。'+'、'+'「'+'」'+'？'+'！')\n",
    "def sentence2vec(sentence):\n",
    "    vector = np.zeros((w2v_model[words[0]].shape))\n",
    "    oov_num = 0\n",
    "    token_sentence = segmenter.seg(sentence)\n",
    "    token_sentence = token_sentence.tok\n",
    "    token_sentence = [t for t in token_sentence if not t in exclude]\n",
    "    for token in token_sentence:\n",
    "        if token in w2v_model.vocab:\n",
    "            vector += w2v_model[token]\n",
    "        else:\n",
    "            oov_num += 1\n",
    "    vector /= len(token_sentence)\n",
    "    return vector, oov_num, token_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sv, test_sv_oov, test_sentence = sentence2vec(sentence_dict['100'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmeantation of sentence: ['有', '天', '當', '媽媽', '正在', '洗碗', '時', '水龍頭', '竟', '突然', '地', '不', '受', '控制', '地', '嘩啦啦', '的', '流', '著', '水', '但', '媽媽', '卻', '像是', '沒', '看見', '似的', '依然故我', '的', '洗', '著', '碗盤', '轉頭', '一', '看', '弟弟', '踮', '著', '腳', '想', '從', '碗櫥', '拿', '些', '東西', '一', '個', '不', '小心', '便', '差點', '掉下來', '希望', '姐姐', '能', '接住', '他']\n",
      "Sentence vector: [-0.05014453  1.00165772 -0.61897596  0.4185977   0.81403333 -0.43266385\n",
      " -0.62356421  0.03707956  0.29840268 -0.1195136 ], \n",
      "lenght=(300,)\n",
      "Vocabulary not in Lookup table: 1\n"
     ]
    }
   ],
   "source": [
    "# Print sample result of sentence vector\n",
    "print(\"Segmeantation of sentence: {}\".format(test_sentence))\n",
    "print(\"Sentence vector: {0}, \\nlenght={1}\".format(test_sv[:10], w2v_model[words[0]].shape))\n",
    "print(\"Vocabulary not in Lookup table: {}\".format(test_sv_oov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sv_dict = {}\n",
    "oov_dict = {}\n",
    "sentence_token_dict = {}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "002.\n",
      "003.\n",
      "004.\n",
      "005.\n",
      "006. \n",
      "007.\n",
      "008.\n",
      "009.\n",
      "010.\n",
      "011.\n",
      "012.\n",
      "013.\n",
      "015.\n",
      "016.\n",
      "018.\n",
      "019.\n",
      "020.\n",
      "021.\n",
      "023.\n",
      "024.\n",
      "025.\n",
      "026.\n",
      "027.\n",
      "029.\n",
      "030.\n",
      "032.\n",
      "034.\n",
      "035.\n",
      "037.\n",
      "038.\n",
      "039.\n",
      "041.\n",
      "044.\n",
      "045.\n",
      "046.\n",
      "048.\n",
      "049.\n",
      "050.\n",
      "052.\n",
      "054.\n",
      "055.\n",
      "056.\n",
      "057.\n",
      "058.\n",
      "059.\n",
      "061.\n",
      "061_.\n",
      "063.\n",
      "064.\n",
      "065.\n",
      "066.\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "for k, s in sentence_dict.items():\n",
    "    sv, oov, sentence_token = sentence2vec(s)\n",
    "    print(k)\n",
    "    sv_dict[k] = sv\n",
    "    oov_dict[k] = oov\n",
    "    sentence_token_dict[k] = sentence_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_dict_array = np.asarray([i for i in sv_dict.values()])\n",
    "sv_dict_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2).fit(sv_dict_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dementia = model.labels_[:52]\n",
    "control = model.labels_[52:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dementia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09134774,  0.97664812, -0.6161853 ,  0.37809061,  0.80142591,\n",
       "       -0.44241801, -0.67143148,  0.01950247,  0.26056783, -0.11032331])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_dict_array[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09134774,  0.97664812, -0.6161853 , ...,  0.30925858,\n",
       "         1.02234691,  0.13697879],\n",
       "       [-0.08000795,  1.013031  , -0.652757  , ...,  0.3421018 ,\n",
       "         1.06676798,  0.1675923 ],\n",
       "       [-0.09044532,  0.97061959, -0.61444208, ...,  0.29606421,\n",
       "         1.06004   ,  0.14580231],\n",
       "       ...,\n",
       "       [-0.09525553,  0.95463   , -0.57256167, ...,  0.34828403,\n",
       "         1.02017887,  0.13167095],\n",
       "       [-0.08510944,  0.89666519, -0.59682907, ...,  0.25907822,\n",
       "         0.98176092,  0.12500127],\n",
       "       [-0.10120685,  0.98504443, -0.63363148, ...,  0.33146789,\n",
       "         1.01251593,  0.06325852]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_dict_array[:10][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP: (array([ 0,  1,  2,  6,  7,  9, 11, 12, 13, 16, 17, 19, 20, 21, 23, 26, 28,\n",
      "       31, 32, 34, 35, 36, 37, 38, 39, 43, 44, 45, 46, 47, 48, 49, 50]),)\n",
      "FN: (array([ 0,  1,  2,  3,  5,  6,  7,  8, 10, 12, 15, 17, 19, 20, 21, 22, 23,\n",
      "       24, 26, 29, 30, 32, 33, 35, 37, 38, 39, 41, 42, 44, 45, 46, 47, 49]),)\n",
      "sklearn kmeans score: -44.25144683115823\n",
      "Silhouette score: 0.29422869677739705\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "dementia = np.asarray(dementia)\n",
    "control = np.asarray(control)\n",
    "\n",
    "fp_sklearn = np.where(dementia==0)\n",
    "fn_sklearn = np.where(control==1)\n",
    "print(\"FP: \" + str(fp_sklearn))\n",
    "print(\"FN: \" + str(fn_sklearn))\n",
    "print(\"sklearn kmeans score: {}\".format(model.score(sv_dict_array)))\n",
    "silhouette_score = metrics.silhouette_score(sv_dict_array, model.labels_, \n",
    "                                           metric='euclidean')\n",
    "print(\"Silhouette score: {}\".format(silhouette_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "kclusterer = KMeansClusterer(2, distance=nltk.cluster.util.cosine_distance,\n",
    "                            repeats=1000)\n",
    "assigned_clusters = kclusterer.cluster(sv_dict_array, assign_clusters=True)\n",
    "print(assigned_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP: (array([ 7,  8,  9, 10, 15, 16, 18, 24, 27, 28, 30, 32, 33, 39, 42, 51]),)\n",
      "FN: (array([ 3, 40]),)\n"
     ]
    }
   ],
   "source": [
    "dement = np.asarray(assigned_clusters[:52])\n",
    "contr = np.asarray(assigned_clusters[52:])\n",
    "# dement = np.asarray(dement)\n",
    "\n",
    "fp = np.where(dement==1)\n",
    "fn = np.where(contr==0)\n",
    "print(\"FP: \" + str(fp))\n",
    "print(\"FN: \" + str(fn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
