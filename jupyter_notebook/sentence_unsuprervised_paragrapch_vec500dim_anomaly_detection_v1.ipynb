{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = '../../docvec_model/doc_500features_10context_10mincount_zht'\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load paragraph2vec model used 4.132974636\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.process_time()\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec.load(MODEL)\n",
    "end = time.process_time() - start\n",
    "print('load paragraph2vec model used {}'.format(end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PICKLE = \"../../pickle/sentence_dict.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "sentence_dict = {}\n",
    "with open(DATA_PICKLE, 'rb') as f:\n",
    "    sentence_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "# from opencc import OpenCC \n",
    "# openCC = OpenCC('tw2s') \n",
    "# exclude = set(string.punctuation+'，'+'。'+'、'+'「'+'」'+'？'+'！')\n",
    "\n",
    "def sentence2vec(sentence):\n",
    "#     sentence = openCC.convert(sentence)\n",
    "    vector = np.zeros((d2v_model.docvecs[1].shape))\n",
    "    \n",
    "    token_sentence = jieba.lcut(sentence)\n",
    "    token_sentence = [t for t in token_sentence]\n",
    "    vector = d2v_model.infer_vector(token_sentence)\n",
    "    \n",
    "    return vector, token_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_dict = {}\n",
    "sentence_token_dict = {}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.659 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "002.\n",
      "003.\n",
      "004.\n",
      "005.\n",
      "006. \n",
      "007.\n",
      "008.\n",
      "009.\n",
      "010.\n",
      "011.\n",
      "012.\n",
      "013.\n",
      "015.\n",
      "016.\n",
      "018.\n",
      "019.\n",
      "020.\n",
      "021.\n",
      "023.\n",
      "024.\n",
      "025.\n",
      "026.\n",
      "027.\n",
      "029.\n",
      "030.\n",
      "032.\n",
      "034.\n",
      "035.\n",
      "037.\n",
      "038.\n",
      "039.\n",
      "041.\n",
      "044.\n",
      "045.\n",
      "046.\n",
      "048.\n",
      "049.\n",
      "050.\n",
      "052.\n",
      "054.\n",
      "055.\n",
      "056.\n",
      "057.\n",
      "058.\n",
      "059.\n",
      "061.\n",
      "061_.\n",
      "063.\n",
      "064.\n",
      "065.\n",
      "066.\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "for k, s in sentence_dict.items():\n",
    "    sv, sentence_token = sentence2vec(s)\n",
    "    print(k)\n",
    "    sv_dict[k] = sv\n",
    "    sentence_token_dict[k] = sentence_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 500) (51, 500)\n"
     ]
    }
   ],
   "source": [
    "pv_array = np.asarray([i for i in sv_dict.values()])\n",
    "x_train = pv_array[51:]\n",
    "x_test = pv_array[:51]\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([11, 13, 24, 28, 40, 44]),) 6\n",
      "(array([ 1,  2,  6,  9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 24, 26,\n",
      "       30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 46, 47, 48, 49,\n",
      "       50]),) 35\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.OneClassSVM(nu=0.05, kernel='rbf', gamma=0.05)\n",
    "clf.fit(x_train)\n",
    "y_pred_train = clf.predict(x_train)\n",
    "y_pred_test = clf.predict(x_test)\n",
    "fn = np.where(y_pred_train==-1)\n",
    "fp = np.where(y_pred_test==1)\n",
    "print(fn, len(fn[0]))\n",
    "print(fp, len(fp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_ckip = pd.read_csv('../ckip_synt.csv', index_col=0)\n",
    "df_jieba = pd.read_csv('../jieba_synt.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_ckip.values[51:]\n",
    "x_test = df_ckip.values[:51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0, 28, 41, 42, 49]),)\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "clf = svm.OneClassSVM(nu=0.05, kernel='rbf', gamma=0.05)\n",
    "clf.fit(x_train)\n",
    "y_pred_train = clf.predict(x_train)\n",
    "y_pred_test = clf.predict(x_test)\n",
    "print(np.where(y_pred_train==-1))\n",
    "print(len(np.where(y_pred_test==1)[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../../pickle/s2v_array_zht_500dim.pickle', 'rb') as f:\n",
    "    sv_dict_array = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: (51, 500)\n",
      "Train size: (51, 500)\n"
     ]
    }
   ],
   "source": [
    "# TRAIN AND TEST DATA\n",
    "\n",
    "x_test = sv_dict_array[:51]\n",
    "x_train = sv_dict_array[51:]\n",
    "\n",
    "\n",
    "print(\"Test size: {}\".format(test_x.shape))\n",
    "print(\"Train size: {}\".format(train_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 3,  6, 41, 48, 49]),)\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "clf = svm.OneClassSVM(nu=0.05, kernel='rbf', gamma=0.05)\n",
    "clf.fit(x_train)\n",
    "y_pred_train = clf.predict(x_train)\n",
    "y_pred_test = clf.predict(x_test)\n",
    "print(np.where(y_pred_train==-1))\n",
    "print(len(np.where(y_pred_test==1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
