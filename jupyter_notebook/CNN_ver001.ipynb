{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gensim\n",
    "import numpy as np\n",
    "import string\n",
    "from opencc import OpenCC\n",
    "import ckip\n",
    "import jieba\n",
    "# Path of files\n",
    "SENTENCE_DICT = \"../../pickle/sentence_dict.pickle\"\n",
    "WORDVEC_MODEL = '../../wordvec_model/'\n",
    "# Variables\n",
    "DEMENTIA_NUM = 51\n",
    "CONTROL_NUM = 51\n",
    "WV_DIIM = 500\n",
    "\n",
    "def read_sentence_file(file_name=None):\n",
    "    with open(SENTENCE_DICT, 'rb') as f:\n",
    "        sentence_dict = pickle.load(f)\n",
    "        print(\"Load sentence text data ...\")\n",
    "    return sentence_dict\n",
    "\n",
    "def load_wordvec_model(file_name):\n",
    "    w2v_model = gensim.models.Word2Vec.load(WORDVEC_MODEL+file_name)\n",
    "    words = []\n",
    "    for word in w2v_model.wv.vocab:\n",
    "        words.append(word)\n",
    "    print('Load word2vec model sucess ...')\n",
    "    print('Number of token: {}'.format(len(words)))\n",
    "    print('Dimensions of word vector: {}'.format(len(w2v_model[words[0]])))\n",
    "    return w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load sentence text data ...\n"
     ]
    }
   ],
   "source": [
    "sentence_dict = read_sentence_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load word2vec model sucess ...\n",
      "Number of token: 259638\n",
      "Dimensions of word vector: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yyliu/anaconda3/envs/NLP/lib/python3.6/site-packages/ipykernel_launcher.py:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "w2v_model = load_wordvec_model('500features_20context_20mincount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import Dense, LSTM, Embedding, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3個人', '一個媽媽兩個小孩', '小孩站在椅子上要拿西點', '椅子都快倒下來了', '在拿這個西點餅乾要吃', '手下還拿著一塊']\n"
     ]
    }
   ],
   "source": [
    "def split_punctuation(sentence):\n",
    "    punctuation = set(string.punctuation+\"，\"+\"、\"+\"」\"+\"「\"+\"。\"+\" \"+\"！\")\n",
    "    sentence_split = []\n",
    "    tmp = ''\n",
    "    for i in sentence:\n",
    "        if i not in punctuation:\n",
    "            tmp += i\n",
    "        else:\n",
    "            sentence_split.append(tmp)\n",
    "            tmp = ''\n",
    "    return sentence_split\n",
    "sentence = '3個人，一個媽媽兩個小孩，小孩站在椅子上要拿西點，椅子都快倒下來了，在拿這個西點餅乾要吃，手下還拿著一塊，'\n",
    "print(split_punctuation(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of train set: 873\n",
      "sentence number of dementia subject: 442\n",
      "sentence number of control normal subject: 431\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/dementia.txt', encoding='utf8') as f:\n",
    "    dementia_txt = f.readlines()\n",
    "sentence = []\n",
    "for i in range(len(dementia_txt)):\n",
    "    if i%2==0:\n",
    "        sentence.extend(split_punctuation(dementia_txt[i+1]))\n",
    "dementia_num = len(sentence)\n",
    "with open('../../data/control_51.txt', encoding='utf8') as f:\n",
    "    control_txt = f.readlines()\n",
    "for i in range(len(control_txt)):\n",
    "    if i%2==0:\n",
    "        sentence.extend(split_punctuation(control_txt[i+1]))\n",
    "control_num = len(sentence) - dementia_num\n",
    "############\n",
    "# train set#\n",
    "############\n",
    "train_data = np.array(sentence)\n",
    "dementia_labels = [[0, 1] for _ in train_data[:dementia_num]]\n",
    "control_labels = [[1, 0] for _ in train_data[dementia_num:]]\n",
    "print('total number of train set: {}'.format(train_data.shape[0]))\n",
    "print('sentence number of dementia subject: {}'.format(len(dementia_labels)))\n",
    "print('sentence number of control normal subject: {}'.format(len(control_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/yyliu/code/NLP/data/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u74f96b08eeb68fe4b0ac4c13a6f276ed.cache\n",
      "Loading model cost 1.243 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手下 還拿著 一塊\n"
     ]
    }
   ],
   "source": [
    "JIEBA_DICT = '../../data/dict.txt.big'\n",
    "jieba.set_dictionary(JIEBA_DICT)\n",
    "train_data_seg = []\n",
    "for i in train_data:\n",
    "    train_data_seg.append(' '.join(jieba.lcut(i)))\n",
    "print(train_data_seg[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token number of sentence: 17\n",
      "Min token number of sentence: 1\n",
      "Mean token number of sentence: 5.747995418098511\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LENGTH = 17\n",
    "VOCAB_DIM = 500\n",
    "train_data_seg_array = np.array(train_data_seg)\n",
    "l = []\n",
    "for i in range(len(train_data_seg_array)):\n",
    "    l.append(len(train_data_seg_array[i].split(' ')))\n",
    "#     if len(train_data_seg_array[i])==1:\n",
    "#         print(i, train_data_seg_array[i])\n",
    "print('Max token number of sentence: {}'.format(np.max(l)))\n",
    "print('Min token number of sentence: {}'.format(np.min(l)))\n",
    "print('Mean token number of sentence: {}'.format(np.mean(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = []\n",
    "for s in train_data_seg:\n",
    "    token_list = []\n",
    "    for token in s.split(' '):\n",
    "        if token in w2v_model.wv.vocab:\n",
    "            token_list.append(np.asarray(w2v_model.wv[token]))\n",
    "    if len(token_list) < SEQUENCE_LENGTH:\n",
    "        for i in range(SEQUENCE_LENGTH - len(token_list)):\n",
    "            token_list.append(np.zeros(shape=VOCAB_DIM))\n",
    "    train_vec.append([token_list[0:SEQUENCE_LENGTH]])\n",
    "# seg_sentence_vec = []\n",
    "# for key, s in seg_sentence.items():\n",
    "#     token_list = []\n",
    "#     for token in s:\n",
    "#         if token in w2v_model.wv.vocab:\n",
    "#             token_list.append(np.asarray(w2v_model.wv[token]))\n",
    "#     if len(token_list) < SEQUENCE_LENGTH:\n",
    "#         for i in range(SEQUENCE_LENGTH - len(token_list)):\n",
    "#             token_list.append(np.zeros(shape=VOCAB_DIM))\n",
    "# #             token_list.append(np.zeros(shape=(VOCAB_DIM, 1)).tolist())\n",
    "#     seg_sentence_vec.append([token_list[0:SEQUENCE_LENGTH]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(873, 17, 500)\n",
      "[[-7.92692825e-02  1.57746319e-02  5.27736768e-02 ...  3.76927592e-02\n",
      "   7.03343600e-02  3.61793354e-04]\n",
      " [ 1.03810102e-01 -3.23520899e-02  1.29130320e-03 ... -2.70457361e-02\n",
      "  -3.82427163e-02  1.65345892e-02]\n",
      " [-1.31187662e-02 -1.90271542e-03 -6.02233633e-02 ...  2.55492123e-05\n",
      "   2.02863179e-02  1.29380105e-02]\n",
      " ...\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "train_vec = np.asarray(train_vec).reshape(len(train_data_seg),-1,500)\n",
    "print(train_vec.shape)\n",
    "print(train_vec[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = tf.placeholder(tf.int32, [None, SEQUENCE_LENGTH], name='input_x')\n",
    "input_y = tf.placeholder(tf.float32, [None, 2], name='input_y')\n",
    "dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
    "l2_loss = tf.constant(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "max_sentence_length = max([len(x.split(' ')) for x in train_data_seg_array])\n",
    "print(max_sentence_length)\n",
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_sentence_length)\n",
    "x_one_hot = np.array(list(vocab_processor.fit_transform(train_data_seg_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(vocab_processor.vocabulary_)\n",
    "embedding_size = 128\n",
    "with tf.device('/cpu:0'), tf.name_scope('embedding'):\n",
    "    W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name = 'W')\n",
    "    embedded_chars = tf.nn.embedding_lookup(W, input_x)\n",
    "    embedded_chars_expand = tf.expand_dims(embedded_chars, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, SEQUENCE_LENGTH], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate([dementia_labels, control_labels], 0)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x_one_hot[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sample_index = -1 * int(.1*float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "del x_shuffled, y_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 961\n",
      "Train/Dev split : 786/87\n"
     ]
    }
   ],
   "source": [
    "print('vocab_size: {}'.format(len(vocab_processor.vocabulary_)))\n",
    "print('Train/Dev split : {}/{}'.format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871\n",
      "\n",
      "2018-05-01T21:37:52.319723: step 1, loss 2.04894, acc 0.375\n",
      "2018-05-01T21:37:52.340474: step 2, loss 1.69986, acc 0.46875\n",
      "2018-05-01T21:37:52.361404: step 3, loss 1.91794, acc 0.375\n",
      "2018-05-01T21:37:52.375349: step 4, loss 1.88264, acc 0.4375\n",
      "2018-05-01T21:37:52.388279: step 5, loss 1.09086, acc 0.625\n",
      "2018-05-01T21:37:52.402322: step 6, loss 2.01934, acc 0.40625\n",
      "2018-05-01T21:37:52.423391: step 7, loss 1.70861, acc 0.46875\n",
      "2018-05-01T21:37:52.439740: step 8, loss 1.3433, acc 0.5625\n",
      "2018-05-01T21:37:52.454484: step 9, loss 1.3658, acc 0.59375\n",
      "2018-05-01T21:37:52.473596: step 10, loss 1.6873, acc 0.53125\n",
      "2018-05-01T21:37:52.491032: step 11, loss 1.48158, acc 0.59375\n",
      "2018-05-01T21:37:52.507892: step 12, loss 1.24584, acc 0.5\n",
      "2018-05-01T21:37:52.521141: step 13, loss 1.40756, acc 0.53125\n",
      "2018-05-01T21:37:52.544679: step 14, loss 1.27651, acc 0.65625\n",
      "2018-05-01T21:37:52.570149: step 15, loss 1.03082, acc 0.625\n",
      "2018-05-01T21:37:52.584980: step 16, loss 1.32774, acc 0.53125\n",
      "2018-05-01T21:37:52.598596: step 17, loss 1.2398, acc 0.5625\n",
      "2018-05-01T21:37:52.614897: step 18, loss 1.06288, acc 0.625\n",
      "2018-05-01T21:37:52.630698: step 19, loss 0.754972, acc 0.6875\n",
      "2018-05-01T21:37:52.643196: step 20, loss 1.40055, acc 0.6875\n",
      "2018-05-01T21:37:52.661954: step 21, loss 1.33002, acc 0.59375\n",
      "2018-05-01T21:37:52.677396: step 22, loss 1.81738, acc 0.46875\n",
      "2018-05-01T21:37:52.695369: step 23, loss 1.21358, acc 0.6875\n",
      "2018-05-01T21:37:52.711079: step 24, loss 0.905979, acc 0.6875\n",
      "2018-05-01T21:37:52.723535: step 25, loss 1.16418, acc 0.555556\n",
      "2018-05-01T21:37:52.745110: step 26, loss 0.663044, acc 0.6875\n",
      "2018-05-01T21:37:52.761804: step 27, loss 0.967811, acc 0.75\n",
      "2018-05-01T21:37:52.783793: step 28, loss 1.16308, acc 0.59375\n",
      "2018-05-01T21:37:52.820263: step 29, loss 0.620535, acc 0.71875\n",
      "2018-05-01T21:37:52.850979: step 30, loss 0.839715, acc 0.625\n",
      "2018-05-01T21:37:52.868501: step 31, loss 0.715714, acc 0.71875\n",
      "2018-05-01T21:37:52.886902: step 32, loss 0.957482, acc 0.59375\n",
      "2018-05-01T21:37:52.900174: step 33, loss 0.645231, acc 0.71875\n",
      "2018-05-01T21:37:52.915347: step 34, loss 0.747023, acc 0.75\n",
      "2018-05-01T21:37:52.928723: step 35, loss 1.03555, acc 0.75\n",
      "2018-05-01T21:37:52.942704: step 36, loss 0.753105, acc 0.5625\n",
      "2018-05-01T21:37:52.955222: step 37, loss 0.692937, acc 0.65625\n",
      "2018-05-01T21:37:52.968791: step 38, loss 0.577361, acc 0.8125\n",
      "2018-05-01T21:37:52.985948: step 39, loss 0.8963, acc 0.6875\n",
      "2018-05-01T21:37:53.002031: step 40, loss 0.910607, acc 0.65625\n",
      "2018-05-01T21:37:53.018799: step 41, loss 0.750993, acc 0.59375\n",
      "2018-05-01T21:37:53.034740: step 42, loss 0.623839, acc 0.6875\n",
      "2018-05-01T21:37:53.048827: step 43, loss 0.727096, acc 0.71875\n",
      "2018-05-01T21:37:53.064831: step 44, loss 1.06358, acc 0.53125\n",
      "2018-05-01T21:37:53.081019: step 45, loss 0.966964, acc 0.71875\n",
      "2018-05-01T21:37:53.093753: step 46, loss 0.809245, acc 0.71875\n",
      "2018-05-01T21:37:53.106049: step 47, loss 0.352612, acc 0.90625\n",
      "2018-05-01T21:37:53.119501: step 48, loss 0.645826, acc 0.75\n",
      "2018-05-01T21:37:53.133286: step 49, loss 0.544494, acc 0.78125\n",
      "2018-05-01T21:37:53.145375: step 50, loss 0.785694, acc 0.666667\n",
      "2018-05-01T21:37:53.160320: step 51, loss 0.339186, acc 0.875\n",
      "2018-05-01T21:37:53.172977: step 52, loss 0.582845, acc 0.71875\n",
      "2018-05-01T21:37:53.186239: step 53, loss 0.410699, acc 0.84375\n",
      "2018-05-01T21:37:53.198934: step 54, loss 0.828877, acc 0.71875\n",
      "2018-05-01T21:37:53.218304: step 55, loss 0.530538, acc 0.8125\n",
      "2018-05-01T21:37:53.233660: step 56, loss 0.522345, acc 0.84375\n",
      "2018-05-01T21:37:53.253374: step 57, loss 0.85734, acc 0.71875\n",
      "2018-05-01T21:37:53.269452: step 58, loss 0.705265, acc 0.75\n",
      "2018-05-01T21:37:53.288567: step 59, loss 0.629772, acc 0.75\n",
      "2018-05-01T21:37:53.304317: step 60, loss 0.367536, acc 0.84375\n",
      "2018-05-01T21:37:53.322079: step 61, loss 0.416176, acc 0.90625\n",
      "2018-05-01T21:37:53.339104: step 62, loss 0.936312, acc 0.75\n",
      "2018-05-01T21:37:53.357591: step 63, loss 0.50925, acc 0.78125\n",
      "2018-05-01T21:37:53.373723: step 64, loss 0.323021, acc 0.875\n",
      "2018-05-01T21:37:53.390170: step 65, loss 0.203877, acc 0.9375\n",
      "2018-05-01T21:37:53.408119: step 66, loss 0.694421, acc 0.71875\n",
      "2018-05-01T21:37:53.427580: step 67, loss 0.3536, acc 0.8125\n",
      "2018-05-01T21:37:53.444026: step 68, loss 0.53741, acc 0.65625\n",
      "2018-05-01T21:37:53.459995: step 69, loss 0.480488, acc 0.8125\n",
      "2018-05-01T21:37:53.480753: step 70, loss 0.94034, acc 0.625\n",
      "2018-05-01T21:37:53.496780: step 71, loss 0.420146, acc 0.71875\n",
      "2018-05-01T21:37:53.512279: step 72, loss 0.435148, acc 0.75\n",
      "2018-05-01T21:37:53.530127: step 73, loss 0.321863, acc 0.84375\n",
      "2018-05-01T21:37:53.551137: step 74, loss 0.525934, acc 0.75\n",
      "2018-05-01T21:37:53.563971: step 75, loss 0.627941, acc 0.722222\n",
      "2018-05-01T21:37:53.583609: step 76, loss 0.485065, acc 0.78125\n",
      "2018-05-01T21:37:53.598507: step 77, loss 0.444607, acc 0.8125\n",
      "2018-05-01T21:37:53.617494: step 78, loss 0.739636, acc 0.65625\n",
      "2018-05-01T21:37:53.635856: step 79, loss 0.264471, acc 0.875\n",
      "2018-05-01T21:37:53.650884: step 80, loss 0.390736, acc 0.75\n",
      "2018-05-01T21:37:53.666568: step 81, loss 0.47613, acc 0.75\n",
      "2018-05-01T21:37:53.679469: step 82, loss 0.275284, acc 0.8125\n",
      "2018-05-01T21:37:53.693125: step 83, loss 0.268771, acc 0.90625\n",
      "2018-05-01T21:37:53.706977: step 84, loss 0.61722, acc 0.78125\n",
      "2018-05-01T21:37:53.720793: step 85, loss 0.241781, acc 0.84375\n",
      "2018-05-01T21:37:53.734148: step 86, loss 0.0792443, acc 1\n",
      "2018-05-01T21:37:53.746996: step 87, loss 0.400428, acc 0.8125\n",
      "2018-05-01T21:37:53.762151: step 88, loss 0.566223, acc 0.84375\n",
      "2018-05-01T21:37:53.779528: step 89, loss 0.316102, acc 0.9375\n",
      "2018-05-01T21:37:53.796090: step 90, loss 0.392579, acc 0.875\n",
      "2018-05-01T21:37:53.809710: step 91, loss 0.570077, acc 0.78125\n",
      "2018-05-01T21:37:53.824023: step 92, loss 0.407948, acc 0.90625\n",
      "2018-05-01T21:37:53.840539: step 93, loss 0.267535, acc 0.84375\n",
      "2018-05-01T21:37:53.853949: step 94, loss 0.210294, acc 0.9375\n",
      "2018-05-01T21:37:53.868339: step 95, loss 0.536183, acc 0.75\n",
      "2018-05-01T21:37:53.882517: step 96, loss 0.259342, acc 0.875\n",
      "2018-05-01T21:37:53.896042: step 97, loss 0.769303, acc 0.71875\n",
      "2018-05-01T21:37:53.909545: step 98, loss 0.495444, acc 0.84375\n",
      "2018-05-01T21:37:53.922815: step 99, loss 0.208412, acc 0.875\n",
      "2018-05-01T21:37:53.934368: step 100, loss 0.949736, acc 0.555556\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:37:53.956094: step 100, loss 0.476757, acc 0.724138\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-100\n",
      "\n",
      "2018-05-01T21:37:54.027947: step 101, loss 0.16316, acc 0.9375\n",
      "2018-05-01T21:37:54.043172: step 102, loss 0.306615, acc 0.90625\n",
      "2018-05-01T21:37:54.057442: step 103, loss 0.324804, acc 0.8125\n",
      "2018-05-01T21:37:54.073430: step 104, loss 0.327534, acc 0.84375\n",
      "2018-05-01T21:37:54.087574: step 105, loss 0.475429, acc 0.875\n",
      "2018-05-01T21:37:54.100604: step 106, loss 0.396569, acc 0.75\n",
      "2018-05-01T21:37:54.113357: step 107, loss 0.473461, acc 0.8125\n",
      "2018-05-01T21:37:54.128713: step 108, loss 0.397362, acc 0.84375\n",
      "2018-05-01T21:37:54.143106: step 109, loss 0.195337, acc 0.90625\n",
      "2018-05-01T21:37:54.155853: step 110, loss 0.492219, acc 0.78125\n",
      "2018-05-01T21:37:54.170866: step 111, loss 0.175041, acc 0.90625\n",
      "2018-05-01T21:37:54.186774: step 112, loss 0.397323, acc 0.8125\n",
      "2018-05-01T21:37:54.200371: step 113, loss 0.17666, acc 0.9375\n",
      "2018-05-01T21:37:54.214107: step 114, loss 0.137764, acc 1\n",
      "2018-05-01T21:37:54.227023: step 115, loss 0.344394, acc 0.875\n",
      "2018-05-01T21:37:54.240209: step 116, loss 0.227601, acc 0.90625\n",
      "2018-05-01T21:37:54.252884: step 117, loss 0.270338, acc 0.8125\n",
      "2018-05-01T21:37:54.274301: step 118, loss 0.341323, acc 0.84375\n",
      "2018-05-01T21:37:54.292987: step 119, loss 0.283034, acc 0.90625\n",
      "2018-05-01T21:37:54.322754: step 120, loss 0.469942, acc 0.8125\n",
      "2018-05-01T21:37:54.348857: step 121, loss 0.464927, acc 0.8125\n",
      "2018-05-01T21:37:54.370419: step 122, loss 0.371756, acc 0.78125\n",
      "2018-05-01T21:37:54.393339: step 123, loss 0.27437, acc 0.90625\n",
      "2018-05-01T21:37:54.411510: step 124, loss 0.254912, acc 0.90625\n",
      "2018-05-01T21:37:54.425003: step 125, loss 0.314558, acc 0.888889\n",
      "2018-05-01T21:37:54.437516: step 126, loss 0.205373, acc 0.875\n",
      "2018-05-01T21:37:54.453204: step 127, loss 0.502247, acc 0.875\n",
      "2018-05-01T21:37:54.466899: step 128, loss 0.2279, acc 0.90625\n",
      "2018-05-01T21:37:54.482076: step 129, loss 0.214453, acc 0.9375\n",
      "2018-05-01T21:37:54.497388: step 130, loss 0.213886, acc 0.875\n",
      "2018-05-01T21:37:54.513077: step 131, loss 0.237134, acc 0.90625\n",
      "2018-05-01T21:37:54.531007: step 132, loss 0.315046, acc 0.875\n",
      "2018-05-01T21:37:54.544696: step 133, loss 0.18446, acc 0.90625\n",
      "2018-05-01T21:37:54.563332: step 134, loss 0.177661, acc 0.96875\n",
      "2018-05-01T21:37:54.581452: step 135, loss 0.260851, acc 0.875\n",
      "2018-05-01T21:37:54.595870: step 136, loss 0.170382, acc 0.9375\n",
      "2018-05-01T21:37:54.613875: step 137, loss 0.256773, acc 0.875\n",
      "2018-05-01T21:37:54.631122: step 138, loss 0.40305, acc 0.84375\n",
      "2018-05-01T21:37:54.649878: step 139, loss 0.241838, acc 0.9375\n",
      "2018-05-01T21:37:54.663817: step 140, loss 0.197073, acc 0.90625\n",
      "2018-05-01T21:37:54.686191: step 141, loss 0.166928, acc 0.96875\n",
      "2018-05-01T21:37:54.700198: step 142, loss 0.100356, acc 0.96875\n",
      "2018-05-01T21:37:54.720542: step 143, loss 0.106542, acc 0.96875\n",
      "2018-05-01T21:37:54.739709: step 144, loss 0.306026, acc 0.84375\n",
      "2018-05-01T21:37:54.755834: step 145, loss 0.287437, acc 0.90625\n",
      "2018-05-01T21:37:54.776356: step 146, loss 0.119841, acc 0.96875\n",
      "2018-05-01T21:37:54.794919: step 147, loss 0.0962369, acc 0.9375\n",
      "2018-05-01T21:37:54.807879: step 148, loss 0.120474, acc 0.96875\n",
      "2018-05-01T21:37:54.820835: step 149, loss 0.331323, acc 0.875\n",
      "2018-05-01T21:37:54.834401: step 150, loss 0.336163, acc 0.833333\n",
      "2018-05-01T21:37:54.847481: step 151, loss 0.342166, acc 0.78125\n",
      "2018-05-01T21:37:54.861026: step 152, loss 0.185244, acc 0.9375\n",
      "2018-05-01T21:37:54.873980: step 153, loss 0.143039, acc 0.9375\n",
      "2018-05-01T21:37:54.888119: step 154, loss 0.327796, acc 0.875\n",
      "2018-05-01T21:37:54.902626: step 155, loss 0.106862, acc 0.96875\n",
      "2018-05-01T21:37:54.915992: step 156, loss 0.184566, acc 0.90625\n",
      "2018-05-01T21:37:54.928188: step 157, loss 0.159334, acc 0.9375\n",
      "2018-05-01T21:37:54.940893: step 158, loss 0.129043, acc 0.96875\n",
      "2018-05-01T21:37:54.953509: step 159, loss 0.0794041, acc 1\n",
      "2018-05-01T21:37:54.966499: step 160, loss 0.298315, acc 0.875\n",
      "2018-05-01T21:37:54.979743: step 161, loss 0.125823, acc 0.90625\n",
      "2018-05-01T21:37:54.992999: step 162, loss 0.214944, acc 0.90625\n",
      "2018-05-01T21:37:55.008068: step 163, loss 0.0965629, acc 0.96875\n",
      "2018-05-01T21:37:55.022059: step 164, loss 0.316463, acc 0.84375\n",
      "2018-05-01T21:37:55.035440: step 165, loss 0.169518, acc 0.90625\n",
      "2018-05-01T21:37:55.049275: step 166, loss 0.421765, acc 0.875\n",
      "2018-05-01T21:37:55.064949: step 167, loss 0.0740389, acc 0.96875\n",
      "2018-05-01T21:37:55.079293: step 168, loss 0.334676, acc 0.8125\n",
      "2018-05-01T21:37:55.093258: step 169, loss 0.274847, acc 0.84375\n",
      "2018-05-01T21:37:55.106837: step 170, loss 0.270309, acc 0.875\n",
      "2018-05-01T21:37:55.119984: step 171, loss 0.186091, acc 0.90625\n",
      "2018-05-01T21:37:55.135797: step 172, loss 0.3565, acc 0.875\n",
      "2018-05-01T21:37:55.148953: step 173, loss 0.186607, acc 0.90625\n",
      "2018-05-01T21:37:55.162704: step 174, loss 0.339281, acc 0.875\n",
      "2018-05-01T21:37:55.174175: step 175, loss 0.061994, acc 1\n",
      "2018-05-01T21:37:55.187654: step 176, loss 0.31987, acc 0.8125\n",
      "2018-05-01T21:37:55.203741: step 177, loss 0.299774, acc 0.84375\n",
      "2018-05-01T21:37:55.216464: step 178, loss 0.367485, acc 0.84375\n",
      "2018-05-01T21:37:55.230208: step 179, loss 0.106331, acc 0.96875\n",
      "2018-05-01T21:37:55.242764: step 180, loss 0.166298, acc 0.96875\n",
      "2018-05-01T21:37:55.256532: step 181, loss 0.0889178, acc 1\n",
      "2018-05-01T21:37:55.270895: step 182, loss 0.0512732, acc 1\n",
      "2018-05-01T21:37:55.284465: step 183, loss 0.244291, acc 0.9375\n",
      "2018-05-01T21:37:55.297629: step 184, loss 0.250127, acc 0.875\n",
      "2018-05-01T21:37:55.311942: step 185, loss 0.0845842, acc 0.96875\n",
      "2018-05-01T21:37:55.327426: step 186, loss 0.191202, acc 0.90625\n",
      "2018-05-01T21:37:55.340250: step 187, loss 0.048069, acc 1\n",
      "2018-05-01T21:37:55.353939: step 188, loss 0.352536, acc 0.84375\n",
      "2018-05-01T21:37:55.369396: step 189, loss 0.245399, acc 0.875\n",
      "2018-05-01T21:37:55.385696: step 190, loss 0.140303, acc 0.9375\n",
      "2018-05-01T21:37:55.403875: step 191, loss 0.0809913, acc 1\n",
      "2018-05-01T21:37:55.419350: step 192, loss 0.151978, acc 0.9375\n",
      "2018-05-01T21:37:55.432030: step 193, loss 0.109923, acc 0.9375\n",
      "2018-05-01T21:37:55.444766: step 194, loss 0.174262, acc 0.90625\n",
      "2018-05-01T21:37:55.458357: step 195, loss 0.113093, acc 0.96875\n",
      "2018-05-01T21:37:55.471752: step 196, loss 0.150729, acc 0.90625\n",
      "2018-05-01T21:37:55.488001: step 197, loss 0.170113, acc 0.90625\n",
      "2018-05-01T21:37:55.502601: step 198, loss 0.176447, acc 0.96875\n",
      "2018-05-01T21:37:55.516310: step 199, loss 0.141437, acc 0.875\n",
      "2018-05-01T21:37:55.527780: step 200, loss 0.380588, acc 0.833333\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:37:55.532556: step 200, loss 0.423346, acc 0.781609\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-200\n",
      "\n",
      "2018-05-01T21:37:55.593703: step 201, loss 0.284277, acc 0.875\n",
      "2018-05-01T21:37:55.607912: step 202, loss 0.193202, acc 0.90625\n",
      "2018-05-01T21:37:55.620770: step 203, loss 0.0496328, acc 1\n",
      "2018-05-01T21:37:55.636659: step 204, loss 0.109985, acc 0.96875\n",
      "2018-05-01T21:37:55.652281: step 205, loss 0.171323, acc 0.96875\n",
      "2018-05-01T21:37:55.666150: step 206, loss 0.274663, acc 0.875\n",
      "2018-05-01T21:37:55.679466: step 207, loss 0.180484, acc 0.90625\n",
      "2018-05-01T21:37:55.692952: step 208, loss 0.207355, acc 0.875\n",
      "2018-05-01T21:37:55.707357: step 209, loss 0.0623718, acc 1\n",
      "2018-05-01T21:37:55.721716: step 210, loss 0.141211, acc 0.9375\n",
      "2018-05-01T21:37:55.735269: step 211, loss 0.151994, acc 0.9375\n",
      "2018-05-01T21:37:55.749955: step 212, loss 0.198621, acc 0.9375\n",
      "2018-05-01T21:37:55.763990: step 213, loss 0.0876595, acc 1\n",
      "2018-05-01T21:37:55.778406: step 214, loss 0.0971481, acc 0.90625\n",
      "2018-05-01T21:37:55.792307: step 215, loss 0.0662903, acc 0.96875\n",
      "2018-05-01T21:37:55.804882: step 216, loss 0.0525796, acc 0.96875\n",
      "2018-05-01T21:37:55.819571: step 217, loss 0.0681949, acc 1\n",
      "2018-05-01T21:37:55.832034: step 218, loss 0.0597039, acc 1\n",
      "2018-05-01T21:37:55.845420: step 219, loss 0.0750996, acc 0.96875\n",
      "2018-05-01T21:37:55.857824: step 220, loss 0.269256, acc 0.875\n",
      "2018-05-01T21:37:55.871746: step 221, loss 0.123924, acc 0.96875\n",
      "2018-05-01T21:37:55.886257: step 222, loss 0.0417198, acc 1\n",
      "2018-05-01T21:37:55.903294: step 223, loss 0.120346, acc 0.96875\n",
      "2018-05-01T21:37:55.917053: step 224, loss 0.116736, acc 0.96875\n",
      "2018-05-01T21:37:55.930235: step 225, loss 0.0673776, acc 1\n",
      "2018-05-01T21:37:55.943397: step 226, loss 0.0641676, acc 1\n",
      "2018-05-01T21:37:55.957101: step 227, loss 0.0515871, acc 1\n",
      "2018-05-01T21:37:55.971064: step 228, loss 0.168067, acc 0.9375\n",
      "2018-05-01T21:37:55.986146: step 229, loss 0.110019, acc 0.96875\n",
      "2018-05-01T21:37:56.000575: step 230, loss 0.0831638, acc 0.96875\n",
      "2018-05-01T21:37:56.014968: step 231, loss 0.0683152, acc 0.96875\n",
      "2018-05-01T21:37:56.027366: step 232, loss 0.0760999, acc 0.96875\n",
      "2018-05-01T21:37:56.040596: step 233, loss 0.0889999, acc 0.9375\n",
      "2018-05-01T21:37:56.053289: step 234, loss 0.128304, acc 0.9375\n",
      "2018-05-01T21:37:56.067711: step 235, loss 0.0456364, acc 1\n",
      "2018-05-01T21:37:56.080902: step 236, loss 0.102642, acc 0.96875\n",
      "2018-05-01T21:37:56.094422: step 237, loss 0.0609975, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:37:56.107953: step 238, loss 0.0873921, acc 0.96875\n",
      "2018-05-01T21:37:56.120854: step 239, loss 0.105385, acc 0.96875\n",
      "2018-05-01T21:37:56.134801: step 240, loss 0.363075, acc 0.90625\n",
      "2018-05-01T21:37:56.147830: step 241, loss 0.244517, acc 0.90625\n",
      "2018-05-01T21:37:56.160614: step 242, loss 0.0317824, acc 1\n",
      "2018-05-01T21:37:56.173915: step 243, loss 0.100873, acc 0.96875\n",
      "2018-05-01T21:37:56.186382: step 244, loss 0.206963, acc 0.90625\n",
      "2018-05-01T21:37:56.205572: step 245, loss 0.177771, acc 0.90625\n",
      "2018-05-01T21:37:56.221448: step 246, loss 0.0469501, acc 1\n",
      "2018-05-01T21:37:56.236960: step 247, loss 0.0664782, acc 0.96875\n",
      "2018-05-01T21:37:56.254445: step 248, loss 0.0877527, acc 0.96875\n",
      "2018-05-01T21:37:56.270039: step 249, loss 0.0467555, acc 0.96875\n",
      "2018-05-01T21:37:56.282239: step 250, loss 0.04831, acc 1\n",
      "2018-05-01T21:37:56.296027: step 251, loss 0.138856, acc 0.96875\n",
      "2018-05-01T21:37:56.309461: step 252, loss 0.0418017, acc 1\n",
      "2018-05-01T21:37:56.324148: step 253, loss 0.0518299, acc 1\n",
      "2018-05-01T21:37:56.337952: step 254, loss 0.106707, acc 0.96875\n",
      "2018-05-01T21:37:56.353307: step 255, loss 0.0915736, acc 1\n",
      "2018-05-01T21:37:56.368782: step 256, loss 0.176155, acc 0.90625\n",
      "2018-05-01T21:37:56.382070: step 257, loss 0.167163, acc 0.9375\n",
      "2018-05-01T21:37:56.394850: step 258, loss 0.121061, acc 0.96875\n",
      "2018-05-01T21:37:56.408419: step 259, loss 0.189273, acc 0.96875\n",
      "2018-05-01T21:37:56.421599: step 260, loss 0.172155, acc 0.9375\n",
      "2018-05-01T21:37:56.436219: step 261, loss 0.0515306, acc 1\n",
      "2018-05-01T21:37:56.450409: step 262, loss 0.075227, acc 1\n",
      "2018-05-01T21:37:56.463527: step 263, loss 0.0739188, acc 0.9375\n",
      "2018-05-01T21:37:56.475480: step 264, loss 0.177138, acc 0.96875\n",
      "2018-05-01T21:37:56.488499: step 265, loss 0.0772024, acc 0.96875\n",
      "2018-05-01T21:37:56.501172: step 266, loss 0.0954586, acc 0.96875\n",
      "2018-05-01T21:37:56.514384: step 267, loss 0.113318, acc 1\n",
      "2018-05-01T21:37:56.526923: step 268, loss 0.0754152, acc 1\n",
      "2018-05-01T21:37:56.539853: step 269, loss 0.0461787, acc 1\n",
      "2018-05-01T21:37:56.552469: step 270, loss 0.0308292, acc 1\n",
      "2018-05-01T21:37:56.565825: step 271, loss 0.0975015, acc 0.96875\n",
      "2018-05-01T21:37:56.580837: step 272, loss 0.136238, acc 0.9375\n",
      "2018-05-01T21:37:56.594593: step 273, loss 0.224528, acc 0.9375\n",
      "2018-05-01T21:37:56.608225: step 274, loss 0.125233, acc 0.96875\n",
      "2018-05-01T21:37:56.623178: step 275, loss 0.0277165, acc 1\n",
      "2018-05-01T21:37:56.637515: step 276, loss 0.206368, acc 0.9375\n",
      "2018-05-01T21:37:56.653623: step 277, loss 0.111198, acc 0.96875\n",
      "2018-05-01T21:37:56.669108: step 278, loss 0.0462204, acc 1\n",
      "2018-05-01T21:37:56.682602: step 279, loss 0.0665295, acc 1\n",
      "2018-05-01T21:37:56.696508: step 280, loss 0.02109, acc 1\n",
      "2018-05-01T21:37:56.709630: step 281, loss 0.0710935, acc 1\n",
      "2018-05-01T21:37:56.723291: step 282, loss 0.109018, acc 0.96875\n",
      "2018-05-01T21:37:56.736038: step 283, loss 0.167594, acc 0.96875\n",
      "2018-05-01T21:37:56.749709: step 284, loss 0.141031, acc 0.9375\n",
      "2018-05-01T21:37:56.763967: step 285, loss 0.0495917, acc 0.96875\n",
      "2018-05-01T21:37:56.778375: step 286, loss 0.198577, acc 0.90625\n",
      "2018-05-01T21:37:56.791521: step 287, loss 0.075113, acc 0.96875\n",
      "2018-05-01T21:37:56.804327: step 288, loss 0.0902992, acc 0.96875\n",
      "2018-05-01T21:37:56.817854: step 289, loss 0.0651769, acc 0.96875\n",
      "2018-05-01T21:37:56.830979: step 290, loss 0.0626103, acc 1\n",
      "2018-05-01T21:37:56.843297: step 291, loss 0.0412662, acc 1\n",
      "2018-05-01T21:37:56.857352: step 292, loss 0.0909828, acc 0.9375\n",
      "2018-05-01T21:37:56.870260: step 293, loss 0.110085, acc 0.96875\n",
      "2018-05-01T21:37:56.884145: step 294, loss 0.0692673, acc 0.96875\n",
      "2018-05-01T21:37:56.897461: step 295, loss 0.0949139, acc 0.96875\n",
      "2018-05-01T21:37:56.911836: step 296, loss 0.0576451, acc 0.96875\n",
      "2018-05-01T21:37:56.924349: step 297, loss 0.109941, acc 0.9375\n",
      "2018-05-01T21:37:56.938424: step 298, loss 0.188061, acc 0.9375\n",
      "2018-05-01T21:37:56.952955: step 299, loss 0.0958972, acc 0.9375\n",
      "2018-05-01T21:37:56.964838: step 300, loss 0.0344761, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:37:56.969313: step 300, loss 0.443681, acc 0.804598\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-300\n",
      "\n",
      "2018-05-01T21:37:57.035925: step 301, loss 0.128877, acc 0.96875\n",
      "2018-05-01T21:37:57.053687: step 302, loss 0.0947771, acc 0.9375\n",
      "2018-05-01T21:37:57.093424: step 303, loss 0.0246717, acc 1\n",
      "2018-05-01T21:37:57.132646: step 304, loss 0.120655, acc 0.96875\n",
      "2018-05-01T21:37:57.152510: step 305, loss 0.0740783, acc 1\n",
      "2018-05-01T21:37:57.170485: step 306, loss 0.0935146, acc 0.96875\n",
      "2018-05-01T21:37:57.184782: step 307, loss 0.0739852, acc 1\n",
      "2018-05-01T21:37:57.200902: step 308, loss 0.047561, acc 1\n",
      "2018-05-01T21:37:57.217358: step 309, loss 0.0373654, acc 1\n",
      "2018-05-01T21:37:57.232308: step 310, loss 0.0348835, acc 1\n",
      "2018-05-01T21:37:57.255278: step 311, loss 0.0512238, acc 1\n",
      "2018-05-01T21:37:57.270135: step 312, loss 0.157312, acc 0.9375\n",
      "2018-05-01T21:37:57.286763: step 313, loss 0.106381, acc 0.96875\n",
      "2018-05-01T21:37:57.302935: step 314, loss 0.206539, acc 0.9375\n",
      "2018-05-01T21:37:57.317681: step 315, loss 0.126708, acc 0.96875\n",
      "2018-05-01T21:37:57.332297: step 316, loss 0.0589463, acc 0.96875\n",
      "2018-05-01T21:37:57.349179: step 317, loss 0.0658245, acc 1\n",
      "2018-05-01T21:37:57.362183: step 318, loss 0.0852247, acc 0.96875\n",
      "2018-05-01T21:37:57.385252: step 319, loss 0.0822004, acc 0.96875\n",
      "2018-05-01T21:37:57.398623: step 320, loss 0.0321524, acc 1\n",
      "2018-05-01T21:37:57.411435: step 321, loss 0.086567, acc 0.96875\n",
      "2018-05-01T21:37:57.426824: step 322, loss 0.169278, acc 0.9375\n",
      "2018-05-01T21:37:57.441268: step 323, loss 0.142258, acc 0.9375\n",
      "2018-05-01T21:37:57.453919: step 324, loss 0.0826676, acc 0.96875\n",
      "2018-05-01T21:37:57.467572: step 325, loss 0.01905, acc 1\n",
      "2018-05-01T21:37:57.481313: step 326, loss 0.0590929, acc 1\n",
      "2018-05-01T21:37:57.496439: step 327, loss 0.121035, acc 0.96875\n",
      "2018-05-01T21:37:57.509309: step 328, loss 0.0482934, acc 1\n",
      "2018-05-01T21:37:57.522801: step 329, loss 0.090659, acc 0.96875\n",
      "2018-05-01T21:37:57.535951: step 330, loss 0.130413, acc 0.9375\n",
      "2018-05-01T21:37:57.549530: step 331, loss 0.0782394, acc 0.96875\n",
      "2018-05-01T21:37:57.562751: step 332, loss 0.168713, acc 0.90625\n",
      "2018-05-01T21:37:57.576132: step 333, loss 0.101237, acc 0.9375\n",
      "2018-05-01T21:37:57.589053: step 334, loss 0.179793, acc 0.9375\n",
      "2018-05-01T21:37:57.603782: step 335, loss 0.0508247, acc 1\n",
      "2018-05-01T21:37:57.616875: step 336, loss 0.0158563, acc 1\n",
      "2018-05-01T21:37:57.630647: step 337, loss 0.13275, acc 0.96875\n",
      "2018-05-01T21:37:57.643469: step 338, loss 0.241572, acc 0.9375\n",
      "2018-05-01T21:37:57.659091: step 339, loss 0.0394377, acc 1\n",
      "2018-05-01T21:37:57.674678: step 340, loss 0.0678201, acc 0.96875\n",
      "2018-05-01T21:37:57.689703: step 341, loss 0.0439512, acc 0.96875\n",
      "2018-05-01T21:37:57.704082: step 342, loss 0.0173277, acc 1\n",
      "2018-05-01T21:37:57.717288: step 343, loss 0.157516, acc 0.9375\n",
      "2018-05-01T21:37:57.730346: step 344, loss 0.0665402, acc 1\n",
      "2018-05-01T21:37:57.743157: step 345, loss 0.294122, acc 0.90625\n",
      "2018-05-01T21:37:57.756619: step 346, loss 0.183935, acc 0.90625\n",
      "2018-05-01T21:37:57.772234: step 347, loss 0.0740219, acc 0.96875\n",
      "2018-05-01T21:37:57.786441: step 348, loss 0.0281959, acc 1\n",
      "2018-05-01T21:37:57.799630: step 349, loss 0.0213362, acc 1\n",
      "2018-05-01T21:37:57.812678: step 350, loss 0.0661295, acc 0.944444\n",
      "2018-05-01T21:37:57.826544: step 351, loss 0.0284936, acc 1\n",
      "2018-05-01T21:37:57.839324: step 352, loss 0.0308759, acc 1\n",
      "2018-05-01T21:37:57.852297: step 353, loss 0.0476235, acc 1\n",
      "2018-05-01T21:37:57.866068: step 354, loss 0.0526521, acc 0.96875\n",
      "2018-05-01T21:37:57.879022: step 355, loss 0.0553685, acc 1\n",
      "2018-05-01T21:37:57.891931: step 356, loss 0.056862, acc 0.96875\n",
      "2018-05-01T21:37:57.905813: step 357, loss 0.0421045, acc 1\n",
      "2018-05-01T21:37:57.918359: step 358, loss 0.103998, acc 0.9375\n",
      "2018-05-01T21:37:57.931165: step 359, loss 0.042149, acc 1\n",
      "2018-05-01T21:37:57.945620: step 360, loss 0.245487, acc 0.96875\n",
      "2018-05-01T21:37:57.959712: step 361, loss 0.217486, acc 0.9375\n",
      "2018-05-01T21:37:57.973577: step 362, loss 0.0526282, acc 0.96875\n",
      "2018-05-01T21:37:57.988741: step 363, loss 0.159016, acc 0.9375\n",
      "2018-05-01T21:37:58.002245: step 364, loss 0.0277265, acc 1\n",
      "2018-05-01T21:37:58.014726: step 365, loss 0.0785769, acc 0.96875\n",
      "2018-05-01T21:37:58.028549: step 366, loss 0.0399893, acc 1\n",
      "2018-05-01T21:37:58.042317: step 367, loss 0.0411641, acc 1\n",
      "2018-05-01T21:37:58.056051: step 368, loss 0.0569789, acc 1\n",
      "2018-05-01T21:37:58.069231: step 369, loss 0.0245291, acc 1\n",
      "2018-05-01T21:37:58.081724: step 370, loss 0.0761686, acc 1\n",
      "2018-05-01T21:37:58.098969: step 371, loss 0.0969949, acc 0.96875\n",
      "2018-05-01T21:37:58.113737: step 372, loss 0.0947635, acc 1\n",
      "2018-05-01T21:37:58.127397: step 373, loss 0.0648613, acc 0.96875\n",
      "2018-05-01T21:37:58.142789: step 374, loss 0.0317474, acc 1\n",
      "2018-05-01T21:37:58.156622: step 375, loss 0.131424, acc 0.944444\n",
      "2018-05-01T21:37:58.171462: step 376, loss 0.0469404, acc 1\n",
      "2018-05-01T21:37:58.185657: step 377, loss 0.0770583, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:37:58.198237: step 378, loss 0.0466744, acc 1\n",
      "2018-05-01T21:37:58.212061: step 379, loss 0.282856, acc 0.90625\n",
      "2018-05-01T21:37:58.225423: step 380, loss 0.0695216, acc 1\n",
      "2018-05-01T21:37:58.238544: step 381, loss 0.0624709, acc 0.96875\n",
      "2018-05-01T21:37:58.253150: step 382, loss 0.0223133, acc 1\n",
      "2018-05-01T21:37:58.266931: step 383, loss 0.0826437, acc 0.96875\n",
      "2018-05-01T21:37:58.280665: step 384, loss 0.0739082, acc 0.9375\n",
      "2018-05-01T21:37:58.296588: step 385, loss 0.0347336, acc 1\n",
      "2018-05-01T21:37:58.309837: step 386, loss 0.0458677, acc 0.96875\n",
      "2018-05-01T21:37:58.323937: step 387, loss 0.0373501, acc 1\n",
      "2018-05-01T21:37:58.336314: step 388, loss 0.0462491, acc 1\n",
      "2018-05-01T21:37:58.348709: step 389, loss 0.0746973, acc 0.96875\n",
      "2018-05-01T21:37:58.362135: step 390, loss 0.0183565, acc 1\n",
      "2018-05-01T21:37:58.374884: step 391, loss 0.0638987, acc 0.96875\n",
      "2018-05-01T21:37:58.389459: step 392, loss 0.0799711, acc 0.96875\n",
      "2018-05-01T21:37:58.402253: step 393, loss 0.0831333, acc 0.96875\n",
      "2018-05-01T21:37:58.414746: step 394, loss 0.0880471, acc 0.96875\n",
      "2018-05-01T21:37:58.429244: step 395, loss 0.0836379, acc 0.96875\n",
      "2018-05-01T21:37:58.441975: step 396, loss 0.0489609, acc 0.96875\n",
      "2018-05-01T21:37:58.454852: step 397, loss 0.0230127, acc 1\n",
      "2018-05-01T21:37:58.467352: step 398, loss 0.0676777, acc 0.96875\n",
      "2018-05-01T21:37:58.480739: step 399, loss 0.141196, acc 0.96875\n",
      "2018-05-01T21:37:58.494494: step 400, loss 0.0752861, acc 0.944444\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:37:58.498992: step 400, loss 0.447566, acc 0.827586\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-400\n",
      "\n",
      "2018-05-01T21:37:58.561061: step 401, loss 0.0198142, acc 1\n",
      "2018-05-01T21:37:58.574344: step 402, loss 0.0375889, acc 1\n",
      "2018-05-01T21:37:58.587364: step 403, loss 0.0277532, acc 1\n",
      "2018-05-01T21:37:58.600803: step 404, loss 0.0532308, acc 1\n",
      "2018-05-01T21:37:58.614432: step 405, loss 0.040698, acc 1\n",
      "2018-05-01T21:37:58.627543: step 406, loss 0.0721079, acc 0.96875\n",
      "2018-05-01T21:37:58.641266: step 407, loss 0.212469, acc 0.9375\n",
      "2018-05-01T21:37:58.654869: step 408, loss 0.0884453, acc 0.9375\n",
      "2018-05-01T21:37:58.669248: step 409, loss 0.0684776, acc 0.96875\n",
      "2018-05-01T21:37:58.683153: step 410, loss 0.135625, acc 0.96875\n",
      "2018-05-01T21:37:58.696206: step 411, loss 0.0976464, acc 0.96875\n",
      "2018-05-01T21:37:58.710046: step 412, loss 0.0901631, acc 0.96875\n",
      "2018-05-01T21:37:58.726440: step 413, loss 0.0336063, acc 1\n",
      "2018-05-01T21:37:58.740709: step 414, loss 0.0272112, acc 1\n",
      "2018-05-01T21:37:58.754123: step 415, loss 0.166005, acc 0.96875\n",
      "2018-05-01T21:37:58.766974: step 416, loss 0.0339896, acc 1\n",
      "2018-05-01T21:37:58.780594: step 417, loss 0.0379786, acc 1\n",
      "2018-05-01T21:37:58.795027: step 418, loss 0.0211798, acc 1\n",
      "2018-05-01T21:37:58.808046: step 419, loss 0.0260074, acc 1\n",
      "2018-05-01T21:37:58.823945: step 420, loss 0.10119, acc 0.96875\n",
      "2018-05-01T21:37:58.836929: step 421, loss 0.0403272, acc 1\n",
      "2018-05-01T21:37:58.850526: step 422, loss 0.128193, acc 0.9375\n",
      "2018-05-01T21:37:58.864475: step 423, loss 0.0439359, acc 0.96875\n",
      "2018-05-01T21:37:58.880726: step 424, loss 0.078968, acc 1\n",
      "2018-05-01T21:37:58.893517: step 425, loss 0.0158438, acc 1\n",
      "2018-05-01T21:37:58.906255: step 426, loss 0.0282357, acc 1\n",
      "2018-05-01T21:37:58.919485: step 427, loss 0.0363673, acc 1\n",
      "2018-05-01T21:37:58.932131: step 428, loss 0.0314586, acc 1\n",
      "2018-05-01T21:37:58.945242: step 429, loss 0.0300334, acc 1\n",
      "2018-05-01T21:37:58.958485: step 430, loss 0.160132, acc 0.9375\n",
      "2018-05-01T21:37:58.972224: step 431, loss 0.0817501, acc 0.96875\n",
      "2018-05-01T21:37:58.985392: step 432, loss 0.0383662, acc 1\n",
      "2018-05-01T21:37:58.999157: step 433, loss 0.0339313, acc 1\n",
      "2018-05-01T21:37:59.011627: step 434, loss 0.0424944, acc 1\n",
      "2018-05-01T21:37:59.024512: step 435, loss 0.0141271, acc 1\n",
      "2018-05-01T21:37:59.039659: step 436, loss 0.0265477, acc 1\n",
      "2018-05-01T21:37:59.054851: step 437, loss 0.0917529, acc 0.9375\n",
      "2018-05-01T21:37:59.068509: step 438, loss 0.155064, acc 0.96875\n",
      "2018-05-01T21:37:59.081731: step 439, loss 0.029374, acc 1\n",
      "2018-05-01T21:37:59.093763: step 440, loss 0.0340181, acc 1\n",
      "2018-05-01T21:37:59.107645: step 441, loss 0.0642718, acc 0.96875\n",
      "2018-05-01T21:37:59.121067: step 442, loss 0.0435339, acc 1\n",
      "2018-05-01T21:37:59.135538: step 443, loss 0.0699879, acc 0.96875\n",
      "2018-05-01T21:37:59.148960: step 444, loss 0.0495465, acc 0.96875\n",
      "2018-05-01T21:37:59.163138: step 445, loss 0.0298633, acc 1\n",
      "2018-05-01T21:37:59.176064: step 446, loss 0.123123, acc 0.9375\n",
      "2018-05-01T21:37:59.191911: step 447, loss 0.0352577, acc 0.96875\n",
      "2018-05-01T21:37:59.206837: step 448, loss 0.0725221, acc 0.96875\n",
      "2018-05-01T21:37:59.220151: step 449, loss 0.0518118, acc 1\n",
      "2018-05-01T21:37:59.232497: step 450, loss 0.116576, acc 0.944444\n",
      "2018-05-01T21:37:59.247408: step 451, loss 0.0725319, acc 1\n",
      "2018-05-01T21:37:59.260955: step 452, loss 0.0510597, acc 1\n",
      "2018-05-01T21:37:59.276573: step 453, loss 0.0677081, acc 0.96875\n",
      "2018-05-01T21:37:59.289856: step 454, loss 0.0774518, acc 0.96875\n",
      "2018-05-01T21:37:59.303411: step 455, loss 0.0665867, acc 1\n",
      "2018-05-01T21:37:59.316294: step 456, loss 0.0343447, acc 1\n",
      "2018-05-01T21:37:59.330088: step 457, loss 0.0426857, acc 1\n",
      "2018-05-01T21:37:59.344211: step 458, loss 0.0524196, acc 0.96875\n",
      "2018-05-01T21:37:59.359018: step 459, loss 0.113157, acc 0.96875\n",
      "2018-05-01T21:37:59.375731: step 460, loss 0.0253491, acc 1\n",
      "2018-05-01T21:37:59.390210: step 461, loss 0.055341, acc 1\n",
      "2018-05-01T21:37:59.403524: step 462, loss 0.0346153, acc 1\n",
      "2018-05-01T21:37:59.416468: step 463, loss 0.0686381, acc 1\n",
      "2018-05-01T21:37:59.430328: step 464, loss 0.0380591, acc 1\n",
      "2018-05-01T21:37:59.443996: step 465, loss 0.0315675, acc 0.96875\n",
      "2018-05-01T21:37:59.461013: step 466, loss 0.135078, acc 0.96875\n",
      "2018-05-01T21:37:59.474132: step 467, loss 0.114022, acc 0.9375\n",
      "2018-05-01T21:37:59.488351: step 468, loss 0.0265882, acc 1\n",
      "2018-05-01T21:37:59.505180: step 469, loss 0.136904, acc 0.96875\n",
      "2018-05-01T21:37:59.519907: step 470, loss 0.179791, acc 0.96875\n",
      "2018-05-01T21:37:59.533752: step 471, loss 0.0483605, acc 1\n",
      "2018-05-01T21:37:59.547450: step 472, loss 0.0634431, acc 0.96875\n",
      "2018-05-01T21:37:59.564270: step 473, loss 0.0618942, acc 1\n",
      "2018-05-01T21:37:59.578574: step 474, loss 0.017421, acc 1\n",
      "2018-05-01T21:37:59.591304: step 475, loss 0.229443, acc 0.944444\n",
      "2018-05-01T21:37:59.604904: step 476, loss 0.0165232, acc 1\n",
      "2018-05-01T21:37:59.619110: step 477, loss 0.12398, acc 0.9375\n",
      "2018-05-01T21:37:59.633070: step 478, loss 0.0923064, acc 0.96875\n",
      "2018-05-01T21:37:59.647931: step 479, loss 0.0341623, acc 0.96875\n",
      "2018-05-01T21:37:59.662134: step 480, loss 0.0153061, acc 1\n",
      "2018-05-01T21:37:59.676422: step 481, loss 0.0595944, acc 0.96875\n",
      "2018-05-01T21:37:59.689938: step 482, loss 0.0278354, acc 1\n",
      "2018-05-01T21:37:59.702472: step 483, loss 0.0478592, acc 1\n",
      "2018-05-01T21:37:59.715308: step 484, loss 0.0500317, acc 0.96875\n",
      "2018-05-01T21:37:59.730135: step 485, loss 0.200074, acc 0.9375\n",
      "2018-05-01T21:37:59.744580: step 486, loss 0.0721709, acc 0.96875\n",
      "2018-05-01T21:37:59.757734: step 487, loss 0.0106013, acc 1\n",
      "2018-05-01T21:37:59.770442: step 488, loss 0.118105, acc 0.9375\n",
      "2018-05-01T21:37:59.783451: step 489, loss 0.0390185, acc 1\n",
      "2018-05-01T21:37:59.797851: step 490, loss 0.0181427, acc 1\n",
      "2018-05-01T21:37:59.811659: step 491, loss 0.0525482, acc 0.96875\n",
      "2018-05-01T21:37:59.824742: step 492, loss 0.0408137, acc 1\n",
      "2018-05-01T21:37:59.840705: step 493, loss 0.0238149, acc 1\n",
      "2018-05-01T21:37:59.853650: step 494, loss 0.0276676, acc 1\n",
      "2018-05-01T21:37:59.867165: step 495, loss 0.0354171, acc 1\n",
      "2018-05-01T21:37:59.881787: step 496, loss 0.217741, acc 0.9375\n",
      "2018-05-01T21:37:59.894683: step 497, loss 0.0338734, acc 1\n",
      "2018-05-01T21:37:59.908181: step 498, loss 0.0579628, acc 1\n",
      "2018-05-01T21:37:59.923577: step 499, loss 0.0536567, acc 1\n",
      "2018-05-01T21:37:59.936324: step 500, loss 0.0172347, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:37:59.945990: step 500, loss 0.435535, acc 0.83908\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-500\n",
      "\n",
      "2018-05-01T21:38:00.009311: step 501, loss 0.0145706, acc 1\n",
      "2018-05-01T21:38:00.023095: step 502, loss 0.0396696, acc 0.96875\n",
      "2018-05-01T21:38:00.036742: step 503, loss 0.02727, acc 1\n",
      "2018-05-01T21:38:00.051336: step 504, loss 0.0298471, acc 1\n",
      "2018-05-01T21:38:00.064490: step 505, loss 0.0283324, acc 1\n",
      "2018-05-01T21:38:00.078671: step 506, loss 0.0454219, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:00.093613: step 507, loss 0.0373303, acc 1\n",
      "2018-05-01T21:38:00.109715: step 508, loss 0.0154886, acc 1\n",
      "2018-05-01T21:38:00.122947: step 509, loss 0.0229952, acc 1\n",
      "2018-05-01T21:38:00.136118: step 510, loss 0.0673041, acc 0.96875\n",
      "2018-05-01T21:38:00.149558: step 511, loss 0.0137748, acc 1\n",
      "2018-05-01T21:38:00.163173: step 512, loss 0.118633, acc 0.96875\n",
      "2018-05-01T21:38:00.176538: step 513, loss 0.0539771, acc 0.96875\n",
      "2018-05-01T21:38:00.192585: step 514, loss 0.0160605, acc 1\n",
      "2018-05-01T21:38:00.205886: step 515, loss 0.0571255, acc 0.96875\n",
      "2018-05-01T21:38:00.219960: step 516, loss 0.0243812, acc 1\n",
      "2018-05-01T21:38:00.233651: step 517, loss 0.0701495, acc 0.96875\n",
      "2018-05-01T21:38:00.247775: step 518, loss 0.051497, acc 1\n",
      "2018-05-01T21:38:00.265382: step 519, loss 0.163866, acc 0.9375\n",
      "2018-05-01T21:38:00.278595: step 520, loss 0.00532347, acc 1\n",
      "2018-05-01T21:38:00.291972: step 521, loss 0.00899686, acc 1\n",
      "2018-05-01T21:38:00.305886: step 522, loss 0.0685696, acc 0.96875\n",
      "2018-05-01T21:38:00.319814: step 523, loss 0.0354489, acc 1\n",
      "2018-05-01T21:38:00.333047: step 524, loss 0.0648585, acc 1\n",
      "2018-05-01T21:38:00.347105: step 525, loss 0.212834, acc 0.888889\n",
      "2018-05-01T21:38:00.361471: step 526, loss 0.00825353, acc 1\n",
      "2018-05-01T21:38:00.374630: step 527, loss 0.189674, acc 0.9375\n",
      "2018-05-01T21:38:00.387604: step 528, loss 0.0340407, acc 1\n",
      "2018-05-01T21:38:00.401549: step 529, loss 0.177566, acc 0.96875\n",
      "2018-05-01T21:38:00.414553: step 530, loss 0.0164549, acc 1\n",
      "2018-05-01T21:38:00.428685: step 531, loss 0.0430733, acc 0.96875\n",
      "2018-05-01T21:38:00.441948: step 532, loss 0.0887808, acc 0.96875\n",
      "2018-05-01T21:38:00.454795: step 533, loss 0.00423471, acc 1\n",
      "2018-05-01T21:38:00.468210: step 534, loss 0.0681081, acc 0.96875\n",
      "2018-05-01T21:38:00.480970: step 535, loss 0.0953496, acc 0.9375\n",
      "2018-05-01T21:38:00.494348: step 536, loss 0.0122304, acc 1\n",
      "2018-05-01T21:38:00.509271: step 537, loss 0.0868558, acc 0.96875\n",
      "2018-05-01T21:38:00.522334: step 538, loss 0.0510767, acc 1\n",
      "2018-05-01T21:38:00.535269: step 539, loss 0.107812, acc 0.96875\n",
      "2018-05-01T21:38:00.548443: step 540, loss 0.00726062, acc 1\n",
      "2018-05-01T21:38:00.560935: step 541, loss 0.0658663, acc 1\n",
      "2018-05-01T21:38:00.574075: step 542, loss 0.03504, acc 1\n",
      "2018-05-01T21:38:00.587375: step 543, loss 0.0828301, acc 0.96875\n",
      "2018-05-01T21:38:00.601006: step 544, loss 0.0178119, acc 1\n",
      "2018-05-01T21:38:00.614853: step 545, loss 0.0490178, acc 1\n",
      "2018-05-01T21:38:00.627484: step 546, loss 0.0218144, acc 1\n",
      "2018-05-01T21:38:00.641708: step 547, loss 0.0863553, acc 0.96875\n",
      "2018-05-01T21:38:00.655616: step 548, loss 0.0177732, acc 1\n",
      "2018-05-01T21:38:00.669249: step 549, loss 0.0413909, acc 1\n",
      "2018-05-01T21:38:00.681132: step 550, loss 0.00923879, acc 1\n",
      "2018-05-01T21:38:00.694838: step 551, loss 0.116254, acc 0.96875\n",
      "2018-05-01T21:38:00.707573: step 552, loss 0.0108838, acc 1\n",
      "2018-05-01T21:38:00.722299: step 553, loss 0.00260614, acc 1\n",
      "2018-05-01T21:38:00.735212: step 554, loss 0.0467999, acc 1\n",
      "2018-05-01T21:38:00.748250: step 555, loss 0.0172455, acc 1\n",
      "2018-05-01T21:38:00.762273: step 556, loss 0.0263338, acc 1\n",
      "2018-05-01T21:38:00.775088: step 557, loss 0.022407, acc 1\n",
      "2018-05-01T21:38:00.788496: step 558, loss 0.0150474, acc 1\n",
      "2018-05-01T21:38:00.801604: step 559, loss 0.0447385, acc 1\n",
      "2018-05-01T21:38:00.814723: step 560, loss 0.0220423, acc 1\n",
      "2018-05-01T21:38:00.828087: step 561, loss 0.0329842, acc 1\n",
      "2018-05-01T21:38:00.841178: step 562, loss 0.0239178, acc 1\n",
      "2018-05-01T21:38:00.854432: step 563, loss 0.0285763, acc 1\n",
      "2018-05-01T21:38:00.867172: step 564, loss 0.024133, acc 1\n",
      "2018-05-01T21:38:00.880012: step 565, loss 0.0727107, acc 0.96875\n",
      "2018-05-01T21:38:00.892407: step 566, loss 0.0074294, acc 1\n",
      "2018-05-01T21:38:00.905661: step 567, loss 0.0683642, acc 0.96875\n",
      "2018-05-01T21:38:00.918473: step 568, loss 0.0123775, acc 1\n",
      "2018-05-01T21:38:00.933314: step 569, loss 0.203912, acc 0.9375\n",
      "2018-05-01T21:38:00.945828: step 570, loss 0.0182041, acc 1\n",
      "2018-05-01T21:38:00.959207: step 571, loss 0.0632502, acc 0.96875\n",
      "2018-05-01T21:38:00.973139: step 572, loss 0.0329583, acc 1\n",
      "2018-05-01T21:38:00.988202: step 573, loss 0.0167543, acc 1\n",
      "2018-05-01T21:38:01.010190: step 574, loss 0.176204, acc 0.9375\n",
      "2018-05-01T21:38:01.027124: step 575, loss 0.00541717, acc 1\n",
      "2018-05-01T21:38:01.041917: step 576, loss 0.0501848, acc 0.96875\n",
      "2018-05-01T21:38:01.055596: step 577, loss 0.129864, acc 0.96875\n",
      "2018-05-01T21:38:01.072288: step 578, loss 0.0208831, acc 1\n",
      "2018-05-01T21:38:01.087074: step 579, loss 0.0230736, acc 1\n",
      "2018-05-01T21:38:01.101380: step 580, loss 0.0306881, acc 1\n",
      "2018-05-01T21:38:01.114627: step 581, loss 0.021144, acc 1\n",
      "2018-05-01T21:38:01.127735: step 582, loss 0.053489, acc 1\n",
      "2018-05-01T21:38:01.143309: step 583, loss 0.0638119, acc 0.96875\n",
      "2018-05-01T21:38:01.157673: step 584, loss 0.0119426, acc 1\n",
      "2018-05-01T21:38:01.171428: step 585, loss 0.152643, acc 0.96875\n",
      "2018-05-01T21:38:01.184668: step 586, loss 0.0188568, acc 1\n",
      "2018-05-01T21:38:01.199443: step 587, loss 0.0348542, acc 1\n",
      "2018-05-01T21:38:01.212816: step 588, loss 0.0121156, acc 1\n",
      "2018-05-01T21:38:01.226359: step 589, loss 0.018056, acc 1\n",
      "2018-05-01T21:38:01.239864: step 590, loss 0.0615143, acc 0.96875\n",
      "2018-05-01T21:38:01.253672: step 591, loss 0.0073363, acc 1\n",
      "2018-05-01T21:38:01.268420: step 592, loss 0.0680454, acc 0.96875\n",
      "2018-05-01T21:38:01.282820: step 593, loss 0.0753821, acc 0.96875\n",
      "2018-05-01T21:38:01.295686: step 594, loss 0.0138234, acc 1\n",
      "2018-05-01T21:38:01.309385: step 595, loss 0.0172537, acc 1\n",
      "2018-05-01T21:38:01.324098: step 596, loss 0.058167, acc 1\n",
      "2018-05-01T21:38:01.337475: step 597, loss 0.0306047, acc 1\n",
      "2018-05-01T21:38:01.353245: step 598, loss 0.0506001, acc 0.96875\n",
      "2018-05-01T21:38:01.367599: step 599, loss 0.0858074, acc 0.9375\n",
      "2018-05-01T21:38:01.383928: step 600, loss 0.0443309, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:01.389330: step 600, loss 0.463621, acc 0.804598\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-600\n",
      "\n",
      "2018-05-01T21:38:01.484111: step 601, loss 0.0267034, acc 1\n",
      "2018-05-01T21:38:01.509624: step 602, loss 0.0355101, acc 1\n",
      "2018-05-01T21:38:01.531906: step 603, loss 0.0223569, acc 1\n",
      "2018-05-01T21:38:01.568423: step 604, loss 0.0595663, acc 1\n",
      "2018-05-01T21:38:01.590283: step 605, loss 0.0591551, acc 0.96875\n",
      "2018-05-01T21:38:01.633684: step 606, loss 0.0249325, acc 1\n",
      "2018-05-01T21:38:01.649776: step 607, loss 0.017671, acc 1\n",
      "2018-05-01T21:38:01.672356: step 608, loss 0.0154386, acc 1\n",
      "2018-05-01T21:38:01.687661: step 609, loss 0.0374606, acc 0.96875\n",
      "2018-05-01T21:38:01.708100: step 610, loss 0.0366064, acc 1\n",
      "2018-05-01T21:38:01.730491: step 611, loss 0.00980951, acc 1\n",
      "2018-05-01T21:38:01.747584: step 612, loss 0.0551496, acc 1\n",
      "2018-05-01T21:38:01.772119: step 613, loss 0.0112755, acc 1\n",
      "2018-05-01T21:38:01.787769: step 614, loss 0.244719, acc 0.9375\n",
      "2018-05-01T21:38:01.801957: step 615, loss 0.117284, acc 0.96875\n",
      "2018-05-01T21:38:01.815169: step 616, loss 0.0100424, acc 1\n",
      "2018-05-01T21:38:01.828507: step 617, loss 0.0291661, acc 1\n",
      "2018-05-01T21:38:01.841517: step 618, loss 0.0164809, acc 1\n",
      "2018-05-01T21:38:01.854751: step 619, loss 0.0154907, acc 1\n",
      "2018-05-01T21:38:01.867575: step 620, loss 0.0359182, acc 1\n",
      "2018-05-01T21:38:01.882013: step 621, loss 0.00375412, acc 1\n",
      "2018-05-01T21:38:01.895953: step 622, loss 0.020135, acc 1\n",
      "2018-05-01T21:38:01.910557: step 623, loss 0.0228845, acc 1\n",
      "2018-05-01T21:38:01.923321: step 624, loss 0.0854381, acc 0.96875\n",
      "2018-05-01T21:38:01.935113: step 625, loss 0.012769, acc 1\n",
      "2018-05-01T21:38:01.950760: step 626, loss 0.00695485, acc 1\n",
      "2018-05-01T21:38:01.964238: step 627, loss 0.0248801, acc 1\n",
      "2018-05-01T21:38:01.977419: step 628, loss 0.0247757, acc 1\n",
      "2018-05-01T21:38:01.991116: step 629, loss 0.0237884, acc 1\n",
      "2018-05-01T21:38:02.004096: step 630, loss 0.0419637, acc 0.96875\n",
      "2018-05-01T21:38:02.018639: step 631, loss 0.0269319, acc 1\n",
      "2018-05-01T21:38:02.033117: step 632, loss 0.0159712, acc 1\n",
      "2018-05-01T21:38:02.046507: step 633, loss 0.0343017, acc 1\n",
      "2018-05-01T21:38:02.062071: step 634, loss 0.0083387, acc 1\n",
      "2018-05-01T21:38:02.075935: step 635, loss 0.010063, acc 1\n",
      "2018-05-01T21:38:02.089289: step 636, loss 0.0209824, acc 1\n",
      "2018-05-01T21:38:02.102036: step 637, loss 0.0137975, acc 1\n",
      "2018-05-01T21:38:02.115027: step 638, loss 0.0417595, acc 0.96875\n",
      "2018-05-01T21:38:02.128716: step 639, loss 0.191894, acc 0.96875\n",
      "2018-05-01T21:38:02.143451: step 640, loss 0.0971284, acc 0.96875\n",
      "2018-05-01T21:38:02.156681: step 641, loss 0.0033168, acc 1\n",
      "2018-05-01T21:38:02.170092: step 642, loss 0.0227157, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:02.184533: step 643, loss 0.0582846, acc 0.96875\n",
      "2018-05-01T21:38:02.197603: step 644, loss 0.00252551, acc 1\n",
      "2018-05-01T21:38:02.210780: step 645, loss 0.00785925, acc 1\n",
      "2018-05-01T21:38:02.225578: step 646, loss 0.0228937, acc 1\n",
      "2018-05-01T21:38:02.240709: step 647, loss 0.0351251, acc 1\n",
      "2018-05-01T21:38:02.254957: step 648, loss 0.120224, acc 0.96875\n",
      "2018-05-01T21:38:02.269320: step 649, loss 0.0104174, acc 1\n",
      "2018-05-01T21:38:02.282570: step 650, loss 0.0310342, acc 1\n",
      "2018-05-01T21:38:02.296750: step 651, loss 0.0443622, acc 0.96875\n",
      "2018-05-01T21:38:02.310297: step 652, loss 0.0282008, acc 1\n",
      "2018-05-01T21:38:02.327032: step 653, loss 0.0434843, acc 0.96875\n",
      "2018-05-01T21:38:02.342146: step 654, loss 0.0324007, acc 1\n",
      "2018-05-01T21:38:02.357104: step 655, loss 0.016351, acc 1\n",
      "2018-05-01T21:38:02.371151: step 656, loss 0.0176446, acc 1\n",
      "2018-05-01T21:38:02.386352: step 657, loss 0.0160071, acc 1\n",
      "2018-05-01T21:38:02.402916: step 658, loss 0.0443961, acc 0.96875\n",
      "2018-05-01T21:38:02.416311: step 659, loss 0.0029929, acc 1\n",
      "2018-05-01T21:38:02.429729: step 660, loss 0.023844, acc 1\n",
      "2018-05-01T21:38:02.443150: step 661, loss 0.0448686, acc 0.96875\n",
      "2018-05-01T21:38:02.456875: step 662, loss 0.0232876, acc 1\n",
      "2018-05-01T21:38:02.470245: step 663, loss 0.00997221, acc 1\n",
      "2018-05-01T21:38:02.482844: step 664, loss 0.00856452, acc 1\n",
      "2018-05-01T21:38:02.496174: step 665, loss 0.0080247, acc 1\n",
      "2018-05-01T21:38:02.511994: step 666, loss 0.0863919, acc 0.96875\n",
      "2018-05-01T21:38:02.527102: step 667, loss 0.0445132, acc 1\n",
      "2018-05-01T21:38:02.540111: step 668, loss 0.0547152, acc 1\n",
      "2018-05-01T21:38:02.552782: step 669, loss 0.0147708, acc 1\n",
      "2018-05-01T21:38:02.565521: step 670, loss 0.0157212, acc 1\n",
      "2018-05-01T21:38:02.580371: step 671, loss 0.0148657, acc 1\n",
      "2018-05-01T21:38:02.595357: step 672, loss 0.0166386, acc 1\n",
      "2018-05-01T21:38:02.608382: step 673, loss 0.00718139, acc 1\n",
      "2018-05-01T21:38:02.622210: step 674, loss 0.070869, acc 0.96875\n",
      "2018-05-01T21:38:02.633997: step 675, loss 0.00546328, acc 1\n",
      "2018-05-01T21:38:02.647131: step 676, loss 0.063253, acc 0.96875\n",
      "2018-05-01T21:38:02.664667: step 677, loss 0.058402, acc 0.96875\n",
      "2018-05-01T21:38:02.724949: step 678, loss 0.0171, acc 1\n",
      "2018-05-01T21:38:02.773032: step 679, loss 0.0285827, acc 1\n",
      "2018-05-01T21:38:02.801801: step 680, loss 0.0686041, acc 0.96875\n",
      "2018-05-01T21:38:02.824407: step 681, loss 0.0134494, acc 1\n",
      "2018-05-01T21:38:02.844721: step 682, loss 0.0242111, acc 1\n",
      "2018-05-01T21:38:02.884069: step 683, loss 0.00318559, acc 1\n",
      "2018-05-01T21:38:02.908982: step 684, loss 0.0203596, acc 1\n",
      "2018-05-01T21:38:02.948758: step 685, loss 0.144114, acc 0.96875\n",
      "2018-05-01T21:38:02.971038: step 686, loss 0.0174876, acc 1\n",
      "2018-05-01T21:38:02.995540: step 687, loss 0.0304487, acc 1\n",
      "2018-05-01T21:38:03.017343: step 688, loss 0.019949, acc 1\n",
      "2018-05-01T21:38:03.035606: step 689, loss 0.102485, acc 0.96875\n",
      "2018-05-01T21:38:03.051750: step 690, loss 0.0217374, acc 1\n",
      "2018-05-01T21:38:03.067422: step 691, loss 0.0212609, acc 1\n",
      "2018-05-01T21:38:03.082458: step 692, loss 0.117311, acc 0.96875\n",
      "2018-05-01T21:38:03.096514: step 693, loss 0.0129096, acc 1\n",
      "2018-05-01T21:38:03.109578: step 694, loss 0.0295567, acc 1\n",
      "2018-05-01T21:38:03.124149: step 695, loss 0.0536592, acc 0.96875\n",
      "2018-05-01T21:38:03.136834: step 696, loss 0.00285224, acc 1\n",
      "2018-05-01T21:38:03.149696: step 697, loss 0.015987, acc 1\n",
      "2018-05-01T21:38:03.163195: step 698, loss 0.0148903, acc 1\n",
      "2018-05-01T21:38:03.176947: step 699, loss 0.0805775, acc 0.96875\n",
      "2018-05-01T21:38:03.188808: step 700, loss 0.0230368, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:03.193145: step 700, loss 0.475223, acc 0.781609\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-700\n",
      "\n",
      "2018-05-01T21:38:03.261872: step 701, loss 0.00917726, acc 1\n",
      "2018-05-01T21:38:03.277934: step 702, loss 0.062973, acc 0.96875\n",
      "2018-05-01T21:38:03.292041: step 703, loss 0.017439, acc 1\n",
      "2018-05-01T21:38:03.308730: step 704, loss 0.0288961, acc 1\n",
      "2018-05-01T21:38:03.324901: step 705, loss 0.0104143, acc 1\n",
      "2018-05-01T21:38:03.357927: step 706, loss 0.00365708, acc 1\n",
      "2018-05-01T21:38:03.386352: step 707, loss 0.0228193, acc 1\n",
      "2018-05-01T21:38:03.406426: step 708, loss 0.0424929, acc 0.96875\n",
      "2018-05-01T21:38:03.428667: step 709, loss 0.0674847, acc 0.96875\n",
      "2018-05-01T21:38:03.448848: step 710, loss 0.00433845, acc 1\n",
      "2018-05-01T21:38:03.470533: step 711, loss 0.0134361, acc 1\n",
      "2018-05-01T21:38:03.491496: step 712, loss 0.108351, acc 0.96875\n",
      "2018-05-01T21:38:03.514951: step 713, loss 0.0185496, acc 1\n",
      "2018-05-01T21:38:03.534853: step 714, loss 0.0222828, acc 1\n",
      "2018-05-01T21:38:03.552978: step 715, loss 0.0226178, acc 1\n",
      "2018-05-01T21:38:03.573162: step 716, loss 0.106286, acc 0.96875\n",
      "2018-05-01T21:38:03.591283: step 717, loss 0.158817, acc 0.96875\n",
      "2018-05-01T21:38:03.607455: step 718, loss 0.00984893, acc 1\n",
      "2018-05-01T21:38:03.627359: step 719, loss 0.0108229, acc 1\n",
      "2018-05-01T21:38:03.643789: step 720, loss 0.0172032, acc 1\n",
      "2018-05-01T21:38:03.658893: step 721, loss 0.0093061, acc 1\n",
      "2018-05-01T21:38:03.673507: step 722, loss 0.0386117, acc 1\n",
      "2018-05-01T21:38:03.687500: step 723, loss 0.0752824, acc 0.96875\n",
      "2018-05-01T21:38:03.707001: step 724, loss 0.00754251, acc 1\n",
      "2018-05-01T21:38:03.720518: step 725, loss 0.0136401, acc 1\n",
      "2018-05-01T21:38:03.735308: step 726, loss 0.0247072, acc 1\n",
      "2018-05-01T21:38:03.752003: step 727, loss 0.0177994, acc 1\n",
      "2018-05-01T21:38:03.767315: step 728, loss 0.017531, acc 1\n",
      "2018-05-01T21:38:03.802832: step 729, loss 0.0620175, acc 0.96875\n",
      "2018-05-01T21:38:03.827197: step 730, loss 0.023283, acc 1\n",
      "2018-05-01T21:38:03.864724: step 731, loss 0.0110282, acc 1\n",
      "2018-05-01T21:38:03.886413: step 732, loss 0.00455965, acc 1\n",
      "2018-05-01T21:38:03.905983: step 733, loss 0.0914527, acc 0.96875\n",
      "2018-05-01T21:38:03.930570: step 734, loss 0.029674, acc 1\n",
      "2018-05-01T21:38:03.951623: step 735, loss 0.0100996, acc 1\n",
      "2018-05-01T21:38:03.975038: step 736, loss 0.00902834, acc 1\n",
      "2018-05-01T21:38:04.049988: step 737, loss 0.0127305, acc 1\n",
      "2018-05-01T21:38:04.070397: step 738, loss 0.0229915, acc 1\n",
      "2018-05-01T21:38:04.125519: step 739, loss 0.0118503, acc 1\n",
      "2018-05-01T21:38:04.138230: step 740, loss 0.0202278, acc 1\n",
      "2018-05-01T21:38:04.151611: step 741, loss 0.0165646, acc 1\n",
      "2018-05-01T21:38:04.164768: step 742, loss 0.0692347, acc 0.96875\n",
      "2018-05-01T21:38:04.179553: step 743, loss 0.00792206, acc 1\n",
      "2018-05-01T21:38:04.192864: step 744, loss 0.129062, acc 0.96875\n",
      "2018-05-01T21:38:04.207434: step 745, loss 0.0404413, acc 0.96875\n",
      "2018-05-01T21:38:04.221239: step 746, loss 0.00477559, acc 1\n",
      "2018-05-01T21:38:04.234715: step 747, loss 0.0105683, acc 1\n",
      "2018-05-01T21:38:04.251097: step 748, loss 0.0124531, acc 1\n",
      "2018-05-01T21:38:04.265345: step 749, loss 0.135649, acc 0.96875\n",
      "2018-05-01T21:38:04.282236: step 750, loss 0.0335274, acc 1\n",
      "2018-05-01T21:38:04.298291: step 751, loss 0.0885883, acc 0.96875\n",
      "2018-05-01T21:38:04.313729: step 752, loss 0.0110591, acc 1\n",
      "2018-05-01T21:38:04.330661: step 753, loss 0.0030163, acc 1\n",
      "2018-05-01T21:38:04.351261: step 754, loss 0.0184357, acc 1\n",
      "2018-05-01T21:38:04.366178: step 755, loss 0.00873813, acc 1\n",
      "2018-05-01T21:38:04.384818: step 756, loss 0.00968487, acc 1\n",
      "2018-05-01T21:38:04.402914: step 757, loss 0.0117215, acc 1\n",
      "2018-05-01T21:38:04.420423: step 758, loss 0.162478, acc 0.96875\n",
      "2018-05-01T21:38:04.440875: step 759, loss 0.0109347, acc 1\n",
      "2018-05-01T21:38:04.458968: step 760, loss 0.0246404, acc 1\n",
      "2018-05-01T21:38:04.474825: step 761, loss 0.0149052, acc 1\n",
      "2018-05-01T21:38:04.497251: step 762, loss 0.0178597, acc 1\n",
      "2018-05-01T21:38:04.517847: step 763, loss 0.0226417, acc 1\n",
      "2018-05-01T21:38:04.533803: step 764, loss 0.0490939, acc 0.96875\n",
      "2018-05-01T21:38:04.548603: step 765, loss 0.0211423, acc 1\n",
      "2018-05-01T21:38:04.563967: step 766, loss 0.1337, acc 0.9375\n",
      "2018-05-01T21:38:04.580452: step 767, loss 0.00847874, acc 1\n",
      "2018-05-01T21:38:04.595877: step 768, loss 0.0145851, acc 1\n",
      "2018-05-01T21:38:04.611281: step 769, loss 0.0295189, acc 0.96875\n",
      "2018-05-01T21:38:04.625170: step 770, loss 0.0043914, acc 1\n",
      "2018-05-01T21:38:04.643282: step 771, loss 0.0361019, acc 1\n",
      "2018-05-01T21:38:04.662619: step 772, loss 0.0241932, acc 1\n",
      "2018-05-01T21:38:04.680273: step 773, loss 0.0283276, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:04.698639: step 774, loss 0.00359685, acc 1\n",
      "2018-05-01T21:38:04.719746: step 775, loss 0.0512554, acc 0.944444\n",
      "2018-05-01T21:38:04.738264: step 776, loss 0.0204451, acc 1\n",
      "2018-05-01T21:38:04.754584: step 777, loss 0.00901874, acc 1\n",
      "2018-05-01T21:38:04.772396: step 778, loss 0.0514724, acc 0.96875\n",
      "2018-05-01T21:38:04.792327: step 779, loss 0.0197423, acc 1\n",
      "2018-05-01T21:38:04.806573: step 780, loss 0.158483, acc 0.9375\n",
      "2018-05-01T21:38:04.821203: step 781, loss 0.00332128, acc 1\n",
      "2018-05-01T21:38:04.834212: step 782, loss 0.0193736, acc 1\n",
      "2018-05-01T21:38:04.848285: step 783, loss 0.291186, acc 0.9375\n",
      "2018-05-01T21:38:04.862003: step 784, loss 0.0086989, acc 1\n",
      "2018-05-01T21:38:04.875609: step 785, loss 0.0239352, acc 1\n",
      "2018-05-01T21:38:04.890466: step 786, loss 0.0740218, acc 0.96875\n",
      "2018-05-01T21:38:04.903567: step 787, loss 0.0069848, acc 1\n",
      "2018-05-01T21:38:04.917608: step 788, loss 0.0194539, acc 1\n",
      "2018-05-01T21:38:04.934332: step 789, loss 0.0704861, acc 0.96875\n",
      "2018-05-01T21:38:04.947749: step 790, loss 0.0199197, acc 1\n",
      "2018-05-01T21:38:04.968947: step 791, loss 0.0144041, acc 1\n",
      "2018-05-01T21:38:04.992227: step 792, loss 0.00360153, acc 1\n",
      "2018-05-01T21:38:05.009025: step 793, loss 0.0517023, acc 1\n",
      "2018-05-01T21:38:05.026660: step 794, loss 0.0146524, acc 1\n",
      "2018-05-01T21:38:05.042887: step 795, loss 0.00445129, acc 1\n",
      "2018-05-01T21:38:05.064445: step 796, loss 0.00150311, acc 1\n",
      "2018-05-01T21:38:05.083238: step 797, loss 0.0382853, acc 0.96875\n",
      "2018-05-01T21:38:05.098421: step 798, loss 0.0116913, acc 1\n",
      "2018-05-01T21:38:05.113065: step 799, loss 0.0378456, acc 1\n",
      "2018-05-01T21:38:05.128272: step 800, loss 0.0070602, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:05.132578: step 800, loss 0.46261, acc 0.850575\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-800\n",
      "\n",
      "2018-05-01T21:38:05.206418: step 801, loss 0.125643, acc 0.96875\n",
      "2018-05-01T21:38:05.221347: step 802, loss 0.0545398, acc 0.96875\n",
      "2018-05-01T21:38:05.240470: step 803, loss 0.00827954, acc 1\n",
      "2018-05-01T21:38:05.256352: step 804, loss 0.00558853, acc 1\n",
      "2018-05-01T21:38:05.275607: step 805, loss 0.0208658, acc 1\n",
      "2018-05-01T21:38:05.291555: step 806, loss 0.0299572, acc 1\n",
      "2018-05-01T21:38:05.309107: step 807, loss 0.0551903, acc 0.96875\n",
      "2018-05-01T21:38:05.328734: step 808, loss 0.00900971, acc 1\n",
      "2018-05-01T21:38:05.345389: step 809, loss 0.00962267, acc 1\n",
      "2018-05-01T21:38:05.363370: step 810, loss 0.0140598, acc 1\n",
      "2018-05-01T21:38:05.381319: step 811, loss 0.0197245, acc 1\n",
      "2018-05-01T21:38:05.398916: step 812, loss 0.0130714, acc 1\n",
      "2018-05-01T21:38:05.414563: step 813, loss 0.0111679, acc 1\n",
      "2018-05-01T21:38:05.432033: step 814, loss 0.00735281, acc 1\n",
      "2018-05-01T21:38:05.446197: step 815, loss 0.148877, acc 0.96875\n",
      "2018-05-01T21:38:05.465048: step 816, loss 0.0399535, acc 0.96875\n",
      "2018-05-01T21:38:05.480354: step 817, loss 0.00711876, acc 1\n",
      "2018-05-01T21:38:05.493350: step 818, loss 0.0324812, acc 0.96875\n",
      "2018-05-01T21:38:05.506453: step 819, loss 0.0149355, acc 1\n",
      "2018-05-01T21:38:05.520627: step 820, loss 0.0145528, acc 1\n",
      "2018-05-01T21:38:05.534504: step 821, loss 0.0241443, acc 1\n",
      "2018-05-01T21:38:05.552120: step 822, loss 0.17632, acc 0.9375\n",
      "2018-05-01T21:38:05.566870: step 823, loss 0.0126573, acc 1\n",
      "2018-05-01T21:38:05.581361: step 824, loss 0.0136699, acc 1\n",
      "2018-05-01T21:38:05.594159: step 825, loss 0.00941839, acc 1\n",
      "2018-05-01T21:38:05.606518: step 826, loss 0.0406687, acc 1\n",
      "2018-05-01T21:38:05.620644: step 827, loss 0.0316903, acc 1\n",
      "2018-05-01T21:38:05.635056: step 828, loss 0.00663461, acc 1\n",
      "2018-05-01T21:38:05.648287: step 829, loss 0.00345819, acc 1\n",
      "2018-05-01T21:38:05.662324: step 830, loss 0.00840037, acc 1\n",
      "2018-05-01T21:38:05.680027: step 831, loss 0.01029, acc 1\n",
      "2018-05-01T21:38:05.695076: step 832, loss 0.0184287, acc 1\n",
      "2018-05-01T21:38:05.709595: step 833, loss 0.00755738, acc 1\n",
      "2018-05-01T21:38:05.725953: step 834, loss 0.0050903, acc 1\n",
      "2018-05-01T21:38:05.740270: step 835, loss 0.00564846, acc 1\n",
      "2018-05-01T21:38:05.754738: step 836, loss 0.0150438, acc 1\n",
      "2018-05-01T21:38:05.768780: step 837, loss 0.0233854, acc 1\n",
      "2018-05-01T21:38:05.782119: step 838, loss 0.0086226, acc 1\n",
      "2018-05-01T21:38:05.795623: step 839, loss 0.0796182, acc 0.96875\n",
      "2018-05-01T21:38:05.810389: step 840, loss 0.0077488, acc 1\n",
      "2018-05-01T21:38:05.825441: step 841, loss 0.0344365, acc 0.96875\n",
      "2018-05-01T21:38:05.839148: step 842, loss 0.167791, acc 0.9375\n",
      "2018-05-01T21:38:05.852019: step 843, loss 0.00195855, acc 1\n",
      "2018-05-01T21:38:05.866628: step 844, loss 0.0699159, acc 0.96875\n",
      "2018-05-01T21:38:05.883627: step 845, loss 0.0107188, acc 1\n",
      "2018-05-01T21:38:05.897437: step 846, loss 0.0294339, acc 1\n",
      "2018-05-01T21:38:05.910551: step 847, loss 0.0037928, acc 1\n",
      "2018-05-01T21:38:05.923935: step 848, loss 0.0103918, acc 1\n",
      "2018-05-01T21:38:05.938690: step 849, loss 0.00583964, acc 1\n",
      "2018-05-01T21:38:05.952611: step 850, loss 0.0100602, acc 1\n",
      "2018-05-01T21:38:05.967414: step 851, loss 0.0100112, acc 1\n",
      "2018-05-01T21:38:05.981117: step 852, loss 0.0152805, acc 1\n",
      "2018-05-01T21:38:05.994922: step 853, loss 0.00535439, acc 1\n",
      "2018-05-01T21:38:06.008136: step 854, loss 0.0144881, acc 1\n",
      "2018-05-01T21:38:06.022102: step 855, loss 0.0126722, acc 1\n",
      "2018-05-01T21:38:06.035992: step 856, loss 0.0917417, acc 0.96875\n",
      "2018-05-01T21:38:06.050479: step 857, loss 0.00410447, acc 1\n",
      "2018-05-01T21:38:06.067643: step 858, loss 0.0105831, acc 1\n",
      "2018-05-01T21:38:06.083936: step 859, loss 0.0341776, acc 0.96875\n",
      "2018-05-01T21:38:06.097105: step 860, loss 0.00250802, acc 1\n",
      "2018-05-01T21:38:06.109930: step 861, loss 0.00421731, acc 1\n",
      "2018-05-01T21:38:06.122855: step 862, loss 0.00575721, acc 1\n",
      "2018-05-01T21:38:06.139020: step 863, loss 0.00684589, acc 1\n",
      "2018-05-01T21:38:06.154246: step 864, loss 0.0473176, acc 0.96875\n",
      "2018-05-01T21:38:06.169263: step 865, loss 0.00212666, acc 1\n",
      "2018-05-01T21:38:06.184031: step 866, loss 0.103384, acc 0.9375\n",
      "2018-05-01T21:38:06.200889: step 867, loss 0.00721778, acc 1\n",
      "2018-05-01T21:38:06.214345: step 868, loss 0.00601198, acc 1\n",
      "2018-05-01T21:38:06.228140: step 869, loss 0.0122207, acc 1\n",
      "2018-05-01T21:38:06.241800: step 870, loss 0.0128852, acc 1\n",
      "2018-05-01T21:38:06.254479: step 871, loss 0.0179935, acc 1\n",
      "2018-05-01T21:38:06.267189: step 872, loss 0.0917014, acc 0.9375\n",
      "2018-05-01T21:38:06.282333: step 873, loss 0.234649, acc 0.96875\n",
      "2018-05-01T21:38:06.295235: step 874, loss 0.016206, acc 1\n",
      "2018-05-01T21:38:06.308136: step 875, loss 0.059366, acc 0.944444\n",
      "2018-05-01T21:38:06.322295: step 876, loss 0.00634985, acc 1\n",
      "2018-05-01T21:38:06.335596: step 877, loss 0.00710096, acc 1\n",
      "2018-05-01T21:38:06.348398: step 878, loss 0.00153295, acc 1\n",
      "2018-05-01T21:38:06.361554: step 879, loss 0.0301543, acc 1\n",
      "2018-05-01T21:38:06.375164: step 880, loss 0.122354, acc 0.96875\n",
      "2018-05-01T21:38:06.390516: step 881, loss 0.00775967, acc 1\n",
      "2018-05-01T21:38:06.403393: step 882, loss 0.00537461, acc 1\n",
      "2018-05-01T21:38:06.416403: step 883, loss 0.0187188, acc 1\n",
      "2018-05-01T21:38:06.430413: step 884, loss 0.0978642, acc 0.96875\n",
      "2018-05-01T21:38:06.443904: step 885, loss 0.0740909, acc 0.96875\n",
      "2018-05-01T21:38:06.457119: step 886, loss 0.0260331, acc 1\n",
      "2018-05-01T21:38:06.470109: step 887, loss 0.00858397, acc 1\n",
      "2018-05-01T21:38:06.485964: step 888, loss 0.0376679, acc 0.96875\n",
      "2018-05-01T21:38:06.499676: step 889, loss 0.0258469, acc 1\n",
      "2018-05-01T21:38:06.513229: step 890, loss 0.0465035, acc 0.96875\n",
      "2018-05-01T21:38:06.526762: step 891, loss 0.00603396, acc 1\n",
      "2018-05-01T21:38:06.540186: step 892, loss 0.0133019, acc 1\n",
      "2018-05-01T21:38:06.554576: step 893, loss 0.0198172, acc 1\n",
      "2018-05-01T21:38:06.567946: step 894, loss 0.00690537, acc 1\n",
      "2018-05-01T21:38:06.582452: step 895, loss 0.0179955, acc 1\n",
      "2018-05-01T21:38:06.596738: step 896, loss 0.0163642, acc 1\n",
      "2018-05-01T21:38:06.613653: step 897, loss 0.137303, acc 0.96875\n",
      "2018-05-01T21:38:06.626988: step 898, loss 0.0484052, acc 0.96875\n",
      "2018-05-01T21:38:06.641712: step 899, loss 0.0041039, acc 1\n",
      "2018-05-01T21:38:06.654680: step 900, loss 0.0371833, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:06.659530: step 900, loss 0.483884, acc 0.804598\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-900\n",
      "\n",
      "2018-05-01T21:38:06.723392: step 901, loss 0.00264634, acc 1\n",
      "2018-05-01T21:38:06.737703: step 902, loss 0.0578866, acc 0.96875\n",
      "2018-05-01T21:38:06.751684: step 903, loss 0.00802849, acc 1\n",
      "2018-05-01T21:38:06.765005: step 904, loss 0.0158479, acc 1\n",
      "2018-05-01T21:38:06.779151: step 905, loss 0.0448121, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:06.791593: step 906, loss 0.0338916, acc 1\n",
      "2018-05-01T21:38:06.803970: step 907, loss 0.00957722, acc 1\n",
      "2018-05-01T21:38:06.817422: step 908, loss 0.0362232, acc 0.96875\n",
      "2018-05-01T21:38:06.831322: step 909, loss 0.0188886, acc 1\n",
      "2018-05-01T21:38:06.846159: step 910, loss 0.00507661, acc 1\n",
      "2018-05-01T21:38:06.859992: step 911, loss 0.0120641, acc 1\n",
      "2018-05-01T21:38:06.872522: step 912, loss 0.0123332, acc 1\n",
      "2018-05-01T21:38:06.887015: step 913, loss 0.0239675, acc 1\n",
      "2018-05-01T21:38:06.901790: step 914, loss 0.0131914, acc 1\n",
      "2018-05-01T21:38:06.918743: step 915, loss 0.0143696, acc 1\n",
      "2018-05-01T21:38:06.936960: step 916, loss 0.00703626, acc 1\n",
      "2018-05-01T21:38:06.953868: step 917, loss 0.0272794, acc 1\n",
      "2018-05-01T21:38:06.970727: step 918, loss 0.0404529, acc 0.96875\n",
      "2018-05-01T21:38:06.984543: step 919, loss 0.0229576, acc 1\n",
      "2018-05-01T21:38:07.023431: step 920, loss 0.0968819, acc 0.96875\n",
      "2018-05-01T21:38:07.051328: step 921, loss 0.0362032, acc 1\n",
      "2018-05-01T21:38:07.068930: step 922, loss 0.00252564, acc 1\n",
      "2018-05-01T21:38:07.087388: step 923, loss 0.02673, acc 1\n",
      "2018-05-01T21:38:07.113361: step 924, loss 0.0145851, acc 1\n",
      "2018-05-01T21:38:07.128257: step 925, loss 0.180208, acc 0.888889\n",
      "2018-05-01T21:38:07.147476: step 926, loss 0.0617409, acc 0.9375\n",
      "2018-05-01T21:38:07.169274: step 927, loss 0.0892588, acc 0.9375\n",
      "2018-05-01T21:38:07.189710: step 928, loss 0.0292608, acc 0.96875\n",
      "2018-05-01T21:38:07.205140: step 929, loss 0.026096, acc 1\n",
      "2018-05-01T21:38:07.222426: step 930, loss 0.00903478, acc 1\n",
      "2018-05-01T21:38:07.241197: step 931, loss 0.0108049, acc 1\n",
      "2018-05-01T21:38:07.255429: step 932, loss 0.00482348, acc 1\n",
      "2018-05-01T21:38:07.276286: step 933, loss 0.040634, acc 0.96875\n",
      "2018-05-01T21:38:07.291025: step 934, loss 0.0293319, acc 1\n",
      "2018-05-01T21:38:07.304743: step 935, loss 0.022777, acc 1\n",
      "2018-05-01T21:38:07.323727: step 936, loss 0.180859, acc 0.96875\n",
      "2018-05-01T21:38:07.339994: step 937, loss 0.0208073, acc 1\n",
      "2018-05-01T21:38:07.361766: step 938, loss 0.0550298, acc 0.96875\n",
      "2018-05-01T21:38:07.376054: step 939, loss 0.0617165, acc 0.96875\n",
      "2018-05-01T21:38:07.389655: step 940, loss 0.030829, acc 0.96875\n",
      "2018-05-01T21:38:07.402848: step 941, loss 0.216339, acc 0.96875\n",
      "2018-05-01T21:38:07.416191: step 942, loss 0.00578274, acc 1\n",
      "2018-05-01T21:38:07.430328: step 943, loss 0.00706327, acc 1\n",
      "2018-05-01T21:38:07.447001: step 944, loss 0.00275833, acc 1\n",
      "2018-05-01T21:38:07.462050: step 945, loss 0.0182023, acc 1\n",
      "2018-05-01T21:38:07.477656: step 946, loss 0.0146908, acc 1\n",
      "2018-05-01T21:38:07.494106: step 947, loss 0.0137599, acc 1\n",
      "2018-05-01T21:38:07.507853: step 948, loss 0.0800246, acc 0.96875\n",
      "2018-05-01T21:38:07.520983: step 949, loss 0.0594773, acc 0.96875\n",
      "2018-05-01T21:38:07.533302: step 950, loss 0.0233264, acc 1\n",
      "2018-05-01T21:38:07.547643: step 951, loss 0.00361802, acc 1\n",
      "2018-05-01T21:38:07.563480: step 952, loss 0.0199159, acc 1\n",
      "2018-05-01T21:38:07.578575: step 953, loss 0.00122594, acc 1\n",
      "2018-05-01T21:38:07.593172: step 954, loss 0.120796, acc 0.96875\n",
      "2018-05-01T21:38:07.607180: step 955, loss 0.00710918, acc 1\n",
      "2018-05-01T21:38:07.621451: step 956, loss 0.171754, acc 0.96875\n",
      "2018-05-01T21:38:07.637981: step 957, loss 0.0738528, acc 0.96875\n",
      "2018-05-01T21:38:07.651859: step 958, loss 0.0838864, acc 0.96875\n",
      "2018-05-01T21:38:07.665955: step 959, loss 0.0169934, acc 1\n",
      "2018-05-01T21:38:07.681035: step 960, loss 0.00855304, acc 1\n",
      "2018-05-01T21:38:07.695798: step 961, loss 0.00816993, acc 1\n",
      "2018-05-01T21:38:07.709725: step 962, loss 0.0193759, acc 1\n",
      "2018-05-01T21:38:07.724291: step 963, loss 0.185431, acc 0.96875\n",
      "2018-05-01T21:38:07.738512: step 964, loss 0.0253066, acc 1\n",
      "2018-05-01T21:38:07.755077: step 965, loss 0.00285709, acc 1\n",
      "2018-05-01T21:38:07.770577: step 966, loss 0.00543497, acc 1\n",
      "2018-05-01T21:38:07.784236: step 967, loss 0.0449017, acc 0.96875\n",
      "2018-05-01T21:38:07.798582: step 968, loss 0.00487513, acc 1\n",
      "2018-05-01T21:38:07.814708: step 969, loss 0.0876008, acc 0.96875\n",
      "2018-05-01T21:38:07.828846: step 970, loss 0.0211179, acc 1\n",
      "2018-05-01T21:38:07.843510: step 971, loss 0.00921242, acc 1\n",
      "2018-05-01T21:38:07.858587: step 972, loss 0.029754, acc 1\n",
      "2018-05-01T21:38:07.873859: step 973, loss 0.0375688, acc 0.96875\n",
      "2018-05-01T21:38:07.886796: step 974, loss 0.0373497, acc 1\n",
      "2018-05-01T21:38:07.899655: step 975, loss 0.111792, acc 0.944444\n",
      "2018-05-01T21:38:07.912877: step 976, loss 0.00200344, acc 1\n",
      "2018-05-01T21:38:07.928668: step 977, loss 0.123401, acc 0.96875\n",
      "2018-05-01T21:38:07.942376: step 978, loss 0.00358513, acc 1\n",
      "2018-05-01T21:38:07.954648: step 979, loss 0.00962706, acc 1\n",
      "2018-05-01T21:38:07.968620: step 980, loss 0.00108871, acc 1\n",
      "2018-05-01T21:38:07.982187: step 981, loss 0.0049867, acc 1\n",
      "2018-05-01T21:38:07.997452: step 982, loss 0.147687, acc 0.9375\n",
      "2018-05-01T21:38:08.013832: step 983, loss 0.205196, acc 0.96875\n",
      "2018-05-01T21:38:08.028017: step 984, loss 0.00597621, acc 1\n",
      "2018-05-01T21:38:08.041708: step 985, loss 0.0182787, acc 1\n",
      "2018-05-01T21:38:08.054735: step 986, loss 0.00496843, acc 1\n",
      "2018-05-01T21:38:08.068288: step 987, loss 0.0523344, acc 0.96875\n",
      "2018-05-01T21:38:08.082274: step 988, loss 0.0716549, acc 0.96875\n",
      "2018-05-01T21:38:08.098812: step 989, loss 0.0102334, acc 1\n",
      "2018-05-01T21:38:08.114346: step 990, loss 0.0142342, acc 1\n",
      "2018-05-01T21:38:08.128189: step 991, loss 0.272469, acc 0.96875\n",
      "2018-05-01T21:38:08.142916: step 992, loss 0.00280858, acc 1\n",
      "2018-05-01T21:38:08.156890: step 993, loss 0.0142045, acc 1\n",
      "2018-05-01T21:38:08.169923: step 994, loss 0.0207383, acc 1\n",
      "2018-05-01T21:38:08.183066: step 995, loss 0.00336664, acc 1\n",
      "2018-05-01T21:38:08.197092: step 996, loss 0.00706797, acc 1\n",
      "2018-05-01T21:38:08.210004: step 997, loss 0.231212, acc 0.96875\n",
      "2018-05-01T21:38:08.224863: step 998, loss 0.014596, acc 1\n",
      "2018-05-01T21:38:08.238639: step 999, loss 0.00630902, acc 1\n",
      "2018-05-01T21:38:08.252856: step 1000, loss 0.0161454, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:08.257924: step 1000, loss 0.483217, acc 0.804598\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-1000\n",
      "\n",
      "2018-05-01T21:38:08.354774: step 1001, loss 0.00502068, acc 1\n",
      "2018-05-01T21:38:08.372436: step 1002, loss 0.129718, acc 0.96875\n",
      "2018-05-01T21:38:08.394340: step 1003, loss 0.127714, acc 0.96875\n",
      "2018-05-01T21:38:08.412709: step 1004, loss 0.0136438, acc 1\n",
      "2018-05-01T21:38:08.480007: step 1005, loss 0.00751126, acc 1\n",
      "2018-05-01T21:38:08.501384: step 1006, loss 0.0241803, acc 1\n",
      "2018-05-01T21:38:08.518149: step 1007, loss 0.00260569, acc 1\n",
      "2018-05-01T21:38:08.536786: step 1008, loss 0.0546012, acc 0.96875\n",
      "2018-05-01T21:38:08.555528: step 1009, loss 0.133336, acc 0.9375\n",
      "2018-05-01T21:38:08.573328: step 1010, loss 0.0304676, acc 1\n",
      "2018-05-01T21:38:08.591605: step 1011, loss 0.0044485, acc 1\n",
      "2018-05-01T21:38:08.608500: step 1012, loss 0.00297921, acc 1\n",
      "2018-05-01T21:38:08.626613: step 1013, loss 0.0164487, acc 1\n",
      "2018-05-01T21:38:08.643955: step 1014, loss 0.00429142, acc 1\n",
      "2018-05-01T21:38:08.659938: step 1015, loss 0.0233938, acc 1\n",
      "2018-05-01T21:38:08.673321: step 1016, loss 0.0056244, acc 1\n",
      "2018-05-01T21:38:08.689262: step 1017, loss 0.00261611, acc 1\n",
      "2018-05-01T21:38:08.705837: step 1018, loss 0.000968522, acc 1\n",
      "2018-05-01T21:38:08.719846: step 1019, loss 0.0222523, acc 1\n",
      "2018-05-01T21:38:08.733194: step 1020, loss 0.00214507, acc 1\n",
      "2018-05-01T21:38:08.747734: step 1021, loss 0.00268357, acc 1\n",
      "2018-05-01T21:38:08.761296: step 1022, loss 0.178842, acc 0.9375\n",
      "2018-05-01T21:38:08.773974: step 1023, loss 0.0131901, acc 1\n",
      "2018-05-01T21:38:08.787668: step 1024, loss 0.0119882, acc 1\n",
      "2018-05-01T21:38:08.800498: step 1025, loss 0.0280437, acc 1\n",
      "2018-05-01T21:38:08.814324: step 1026, loss 0.00664524, acc 1\n",
      "2018-05-01T21:38:08.828488: step 1027, loss 0.00326622, acc 1\n",
      "2018-05-01T21:38:08.843020: step 1028, loss 0.121264, acc 0.96875\n",
      "2018-05-01T21:38:08.856745: step 1029, loss 0.0225703, acc 1\n",
      "2018-05-01T21:38:08.870555: step 1030, loss 0.00745913, acc 1\n",
      "2018-05-01T21:38:08.883984: step 1031, loss 0.120733, acc 0.96875\n",
      "2018-05-01T21:38:08.897541: step 1032, loss 0.00469103, acc 1\n",
      "2018-05-01T21:38:08.912628: step 1033, loss 0.051852, acc 0.96875\n",
      "2018-05-01T21:38:08.927722: step 1034, loss 0.0171528, acc 1\n",
      "2018-05-01T21:38:08.940746: step 1035, loss 0.0149644, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:08.954983: step 1036, loss 0.00451625, acc 1\n",
      "2018-05-01T21:38:08.969206: step 1037, loss 0.0310643, acc 1\n",
      "2018-05-01T21:38:08.983749: step 1038, loss 0.0390597, acc 0.96875\n",
      "2018-05-01T21:38:08.996568: step 1039, loss 0.0041052, acc 1\n",
      "2018-05-01T21:38:09.011691: step 1040, loss 0.011475, acc 1\n",
      "2018-05-01T21:38:09.025523: step 1041, loss 0.00615971, acc 1\n",
      "2018-05-01T21:38:09.040083: step 1042, loss 0.013648, acc 1\n",
      "2018-05-01T21:38:09.053046: step 1043, loss 0.131613, acc 0.96875\n",
      "2018-05-01T21:38:09.067220: step 1044, loss 0.00131901, acc 1\n",
      "2018-05-01T21:38:09.084439: step 1045, loss 0.00867871, acc 1\n",
      "2018-05-01T21:38:09.098922: step 1046, loss 0.00265243, acc 1\n",
      "2018-05-01T21:38:09.113372: step 1047, loss 0.0311111, acc 0.96875\n",
      "2018-05-01T21:38:09.127051: step 1048, loss 0.00739779, acc 1\n",
      "2018-05-01T21:38:09.140343: step 1049, loss 0.0011276, acc 1\n",
      "2018-05-01T21:38:09.154280: step 1050, loss 0.0652315, acc 0.944444\n",
      "2018-05-01T21:38:09.168033: step 1051, loss 0.00323932, acc 1\n",
      "2018-05-01T21:38:09.182510: step 1052, loss 0.0307572, acc 0.96875\n",
      "2018-05-01T21:38:09.200106: step 1053, loss 0.0720453, acc 0.96875\n",
      "2018-05-01T21:38:09.213227: step 1054, loss 0.00329336, acc 1\n",
      "2018-05-01T21:38:09.227587: step 1055, loss 0.000912253, acc 1\n",
      "2018-05-01T21:38:09.240019: step 1056, loss 0.0110471, acc 1\n",
      "2018-05-01T21:38:09.252934: step 1057, loss 0.0454815, acc 0.96875\n",
      "2018-05-01T21:38:09.266537: step 1058, loss 0.0138377, acc 1\n",
      "2018-05-01T21:38:09.280507: step 1059, loss 0.161508, acc 0.96875\n",
      "2018-05-01T21:38:09.294156: step 1060, loss 0.00816133, acc 1\n",
      "2018-05-01T21:38:09.307399: step 1061, loss 0.00658286, acc 1\n",
      "2018-05-01T21:38:09.320336: step 1062, loss 0.00243034, acc 1\n",
      "2018-05-01T21:38:09.333312: step 1063, loss 0.0110766, acc 1\n",
      "2018-05-01T21:38:09.346565: step 1064, loss 0.00670417, acc 1\n",
      "2018-05-01T21:38:09.361515: step 1065, loss 0.0200926, acc 1\n",
      "2018-05-01T21:38:09.375643: step 1066, loss 0.020449, acc 1\n",
      "2018-05-01T21:38:09.389443: step 1067, loss 0.129159, acc 0.96875\n",
      "2018-05-01T21:38:09.404182: step 1068, loss 0.0202943, acc 1\n",
      "2018-05-01T21:38:09.417936: step 1069, loss 0.0266605, acc 1\n",
      "2018-05-01T21:38:09.430476: step 1070, loss 0.0302273, acc 0.96875\n",
      "2018-05-01T21:38:09.444981: step 1071, loss 0.06058, acc 0.96875\n",
      "2018-05-01T21:38:09.459577: step 1072, loss 0.00999237, acc 1\n",
      "2018-05-01T21:38:09.473695: step 1073, loss 0.00390685, acc 1\n",
      "2018-05-01T21:38:09.486427: step 1074, loss 0.00740964, acc 1\n",
      "2018-05-01T21:38:09.498138: step 1075, loss 0.0201797, acc 1\n",
      "2018-05-01T21:38:09.511792: step 1076, loss 0.00973212, acc 1\n",
      "2018-05-01T21:38:09.525731: step 1077, loss 0.00576334, acc 1\n",
      "2018-05-01T21:38:09.541479: step 1078, loss 0.000829288, acc 1\n",
      "2018-05-01T21:38:09.555303: step 1079, loss 0.00976514, acc 1\n",
      "2018-05-01T21:38:09.568581: step 1080, loss 0.00584491, acc 1\n",
      "2018-05-01T21:38:09.584121: step 1081, loss 0.0750589, acc 0.96875\n",
      "2018-05-01T21:38:09.597728: step 1082, loss 0.0157963, acc 1\n",
      "2018-05-01T21:38:09.610828: step 1083, loss 0.107034, acc 0.96875\n",
      "2018-05-01T21:38:09.623484: step 1084, loss 0.00443336, acc 1\n",
      "2018-05-01T21:38:09.636668: step 1085, loss 0.0174028, acc 1\n",
      "2018-05-01T21:38:09.655775: step 1086, loss 0.121235, acc 0.9375\n",
      "2018-05-01T21:38:09.671599: step 1087, loss 0.0104502, acc 1\n",
      "2018-05-01T21:38:09.686004: step 1088, loss 0.12567, acc 0.9375\n",
      "2018-05-01T21:38:09.698774: step 1089, loss 0.00652412, acc 1\n",
      "2018-05-01T21:38:09.713383: step 1090, loss 0.0180265, acc 1\n",
      "2018-05-01T21:38:09.726571: step 1091, loss 0.00543872, acc 1\n",
      "2018-05-01T21:38:09.763608: step 1092, loss 0.00406375, acc 1\n",
      "2018-05-01T21:38:09.789025: step 1093, loss 0.0216361, acc 1\n",
      "2018-05-01T21:38:09.826786: step 1094, loss 0.0735914, acc 0.96875\n",
      "2018-05-01T21:38:09.850298: step 1095, loss 0.00291586, acc 1\n",
      "2018-05-01T21:38:09.872578: step 1096, loss 0.00737331, acc 1\n",
      "2018-05-01T21:38:09.890421: step 1097, loss 0.00548323, acc 1\n",
      "2018-05-01T21:38:09.913041: step 1098, loss 0.00398251, acc 1\n",
      "2018-05-01T21:38:09.933365: step 1099, loss 0.0555115, acc 0.9375\n",
      "2018-05-01T21:38:09.951257: step 1100, loss 0.0122227, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:09.956904: step 1100, loss 0.509138, acc 0.827586\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-1100\n",
      "\n",
      "2018-05-01T21:38:10.027416: step 1101, loss 0.0081646, acc 1\n",
      "2018-05-01T21:38:10.042269: step 1102, loss 0.00902507, acc 1\n",
      "2018-05-01T21:38:10.060767: step 1103, loss 0.0156131, acc 1\n",
      "2018-05-01T21:38:10.082869: step 1104, loss 0.0657014, acc 0.96875\n",
      "2018-05-01T21:38:10.099594: step 1105, loss 0.00440933, acc 1\n",
      "2018-05-01T21:38:10.112393: step 1106, loss 0.00777291, acc 1\n",
      "2018-05-01T21:38:10.126020: step 1107, loss 0.002771, acc 1\n",
      "2018-05-01T21:38:10.140735: step 1108, loss 0.00824781, acc 1\n",
      "2018-05-01T21:38:10.153869: step 1109, loss 0.00371732, acc 1\n",
      "2018-05-01T21:38:10.166736: step 1110, loss 0.044855, acc 0.96875\n",
      "2018-05-01T21:38:10.178913: step 1111, loss 0.0159629, acc 1\n",
      "2018-05-01T21:38:10.192860: step 1112, loss 0.0413228, acc 0.96875\n",
      "2018-05-01T21:38:10.205743: step 1113, loss 0.0619755, acc 0.96875\n",
      "2018-05-01T21:38:10.220892: step 1114, loss 0.00667165, acc 1\n",
      "2018-05-01T21:38:10.236634: step 1115, loss 0.00961159, acc 1\n",
      "2018-05-01T21:38:10.250399: step 1116, loss 0.0149255, acc 1\n",
      "2018-05-01T21:38:10.265730: step 1117, loss 0.0244963, acc 1\n",
      "2018-05-01T21:38:10.280151: step 1118, loss 0.00145115, acc 1\n",
      "2018-05-01T21:38:10.298686: step 1119, loss 0.0760687, acc 0.96875\n",
      "2018-05-01T21:38:10.314171: step 1120, loss 0.00640833, acc 1\n",
      "2018-05-01T21:38:10.334001: step 1121, loss 0.036141, acc 0.96875\n",
      "2018-05-01T21:38:10.350884: step 1122, loss 0.110173, acc 0.96875\n",
      "2018-05-01T21:38:10.363779: step 1123, loss 0.00288169, acc 1\n",
      "2018-05-01T21:38:10.377703: step 1124, loss 0.00893494, acc 1\n",
      "2018-05-01T21:38:10.392402: step 1125, loss 0.0244328, acc 1\n",
      "2018-05-01T21:38:10.406737: step 1126, loss 0.0204547, acc 1\n",
      "2018-05-01T21:38:10.420101: step 1127, loss 0.0152832, acc 1\n",
      "2018-05-01T21:38:10.435961: step 1128, loss 0.0336793, acc 0.96875\n",
      "2018-05-01T21:38:10.452177: step 1129, loss 0.139757, acc 0.96875\n",
      "2018-05-01T21:38:10.473192: step 1130, loss 0.00341112, acc 1\n",
      "2018-05-01T21:38:10.495918: step 1131, loss 0.0126676, acc 1\n",
      "2018-05-01T21:38:10.514600: step 1132, loss 0.00757783, acc 1\n",
      "2018-05-01T21:38:10.535321: step 1133, loss 0.0143744, acc 1\n",
      "2018-05-01T21:38:10.556185: step 1134, loss 0.00416675, acc 1\n",
      "2018-05-01T21:38:10.578791: step 1135, loss 0.00656163, acc 1\n",
      "2018-05-01T21:38:10.598719: step 1136, loss 0.00423213, acc 1\n",
      "2018-05-01T21:38:10.620239: step 1137, loss 0.00204522, acc 1\n",
      "2018-05-01T21:38:10.637504: step 1138, loss 0.00273363, acc 1\n",
      "2018-05-01T21:38:10.654792: step 1139, loss 0.0292286, acc 0.96875\n",
      "2018-05-01T21:38:10.676592: step 1140, loss 0.0273163, acc 0.96875\n",
      "2018-05-01T21:38:10.691805: step 1141, loss 0.0679794, acc 0.96875\n",
      "2018-05-01T21:38:10.709805: step 1142, loss 0.00397602, acc 1\n",
      "2018-05-01T21:38:10.732024: step 1143, loss 0.00243779, acc 1\n",
      "2018-05-01T21:38:10.745769: step 1144, loss 0.0224751, acc 1\n",
      "2018-05-01T21:38:10.760946: step 1145, loss 0.00684829, acc 1\n",
      "2018-05-01T21:38:10.776961: step 1146, loss 0.00439007, acc 1\n",
      "2018-05-01T21:38:10.791301: step 1147, loss 0.140194, acc 0.96875\n",
      "2018-05-01T21:38:10.806299: step 1148, loss 0.0313654, acc 0.96875\n",
      "2018-05-01T21:38:10.820800: step 1149, loss 0.0119786, acc 1\n",
      "2018-05-01T21:38:10.833506: step 1150, loss 0.0185144, acc 1\n",
      "2018-05-01T21:38:10.849817: step 1151, loss 0.00789278, acc 1\n",
      "2018-05-01T21:38:10.863528: step 1152, loss 0.008142, acc 1\n",
      "2018-05-01T21:38:10.876959: step 1153, loss 0.0081912, acc 1\n",
      "2018-05-01T21:38:10.889872: step 1154, loss 0.00485527, acc 1\n",
      "2018-05-01T21:38:10.904828: step 1155, loss 0.0244787, acc 1\n",
      "2018-05-01T21:38:10.920308: step 1156, loss 0.00288208, acc 1\n",
      "2018-05-01T21:38:10.935026: step 1157, loss 0.00173207, acc 1\n",
      "2018-05-01T21:38:10.948813: step 1158, loss 0.00256189, acc 1\n",
      "2018-05-01T21:38:10.965257: step 1159, loss 0.0158753, acc 1\n",
      "2018-05-01T21:38:10.978666: step 1160, loss 0.12471, acc 0.96875\n",
      "2018-05-01T21:38:10.992366: step 1161, loss 0.0272393, acc 0.96875\n",
      "2018-05-01T21:38:11.005997: step 1162, loss 0.00791569, acc 1\n",
      "2018-05-01T21:38:11.020590: step 1163, loss 0.0097616, acc 1\n",
      "2018-05-01T21:38:11.035609: step 1164, loss 0.00239325, acc 1\n",
      "2018-05-01T21:38:11.049332: step 1165, loss 0.0775035, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:11.064164: step 1166, loss 0.0124374, acc 1\n",
      "2018-05-01T21:38:11.081616: step 1167, loss 0.064119, acc 0.96875\n",
      "2018-05-01T21:38:11.095543: step 1168, loss 0.150723, acc 0.96875\n",
      "2018-05-01T21:38:11.109949: step 1169, loss 0.00225712, acc 1\n",
      "2018-05-01T21:38:11.123625: step 1170, loss 0.00925323, acc 1\n",
      "2018-05-01T21:38:11.137886: step 1171, loss 0.12904, acc 0.96875\n",
      "2018-05-01T21:38:11.151082: step 1172, loss 0.000569805, acc 1\n",
      "2018-05-01T21:38:11.163932: step 1173, loss 0.179563, acc 0.96875\n",
      "2018-05-01T21:38:11.177485: step 1174, loss 0.00452321, acc 1\n",
      "2018-05-01T21:38:11.189427: step 1175, loss 0.0613724, acc 0.944444\n",
      "2018-05-01T21:38:11.202268: step 1176, loss 0.00350781, acc 1\n",
      "2018-05-01T21:38:11.214965: step 1177, loss 0.00264542, acc 1\n",
      "2018-05-01T21:38:11.228110: step 1178, loss 0.00940103, acc 1\n",
      "2018-05-01T21:38:11.243727: step 1179, loss 0.0912605, acc 0.96875\n",
      "2018-05-01T21:38:11.257598: step 1180, loss 0.0715292, acc 0.9375\n",
      "2018-05-01T21:38:11.272319: step 1181, loss 0.0116396, acc 1\n",
      "2018-05-01T21:38:11.285035: step 1182, loss 0.0207053, acc 1\n",
      "2018-05-01T21:38:11.303102: step 1183, loss 0.0133884, acc 1\n",
      "2018-05-01T21:38:11.324017: step 1184, loss 0.101488, acc 0.9375\n",
      "2018-05-01T21:38:11.339514: step 1185, loss 0.0324431, acc 0.96875\n",
      "2018-05-01T21:38:11.370210: step 1186, loss 0.00904102, acc 1\n",
      "2018-05-01T21:38:11.406978: step 1187, loss 0.00262549, acc 1\n",
      "2018-05-01T21:38:11.423403: step 1188, loss 0.00783586, acc 1\n",
      "2018-05-01T21:38:11.439996: step 1189, loss 0.00945312, acc 1\n",
      "2018-05-01T21:38:11.457963: step 1190, loss 0.00903552, acc 1\n",
      "2018-05-01T21:38:11.485870: step 1191, loss 0.00647295, acc 1\n",
      "2018-05-01T21:38:11.510152: step 1192, loss 0.00264469, acc 1\n",
      "2018-05-01T21:38:11.530337: step 1193, loss 0.157717, acc 0.96875\n",
      "2018-05-01T21:38:11.553002: step 1194, loss 0.00494571, acc 1\n",
      "2018-05-01T21:38:11.569058: step 1195, loss 0.00572627, acc 1\n",
      "2018-05-01T21:38:11.583718: step 1196, loss 0.00076566, acc 1\n",
      "2018-05-01T21:38:11.599823: step 1197, loss 0.0210523, acc 1\n",
      "2018-05-01T21:38:11.619390: step 1198, loss 0.192294, acc 0.96875\n",
      "2018-05-01T21:38:11.635568: step 1199, loss 0.00360553, acc 1\n",
      "2018-05-01T21:38:11.649583: step 1200, loss 0.00374834, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:11.654524: step 1200, loss 0.497971, acc 0.83908\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-1200\n",
      "\n",
      "2018-05-01T21:38:11.733218: step 1201, loss 0.00991243, acc 1\n",
      "2018-05-01T21:38:11.758299: step 1202, loss 0.00514898, acc 1\n",
      "2018-05-01T21:38:11.773826: step 1203, loss 0.00127081, acc 1\n",
      "2018-05-01T21:38:11.787353: step 1204, loss 0.233037, acc 0.96875\n",
      "2018-05-01T21:38:11.803005: step 1205, loss 0.00641315, acc 1\n",
      "2018-05-01T21:38:11.817703: step 1206, loss 0.0123851, acc 1\n",
      "2018-05-01T21:38:11.831203: step 1207, loss 0.0206679, acc 1\n",
      "2018-05-01T21:38:11.847605: step 1208, loss 0.0152982, acc 1\n",
      "2018-05-01T21:38:11.863119: step 1209, loss 0.00331188, acc 1\n",
      "2018-05-01T21:38:11.876882: step 1210, loss 0.0427591, acc 0.96875\n",
      "2018-05-01T21:38:11.889716: step 1211, loss 0.00475337, acc 1\n",
      "2018-05-01T21:38:11.904558: step 1212, loss 0.0442661, acc 0.96875\n",
      "2018-05-01T21:38:11.917965: step 1213, loss 0.00111597, acc 1\n",
      "2018-05-01T21:38:11.932247: step 1214, loss 0.00460926, acc 1\n",
      "2018-05-01T21:38:11.947319: step 1215, loss 0.00810017, acc 1\n",
      "2018-05-01T21:38:11.961888: step 1216, loss 0.0136778, acc 1\n",
      "2018-05-01T21:38:11.975677: step 1217, loss 0.00223272, acc 1\n",
      "2018-05-01T21:38:11.991190: step 1218, loss 0.017213, acc 1\n",
      "2018-05-01T21:38:12.005281: step 1219, loss 0.019015, acc 1\n",
      "2018-05-01T21:38:12.019375: step 1220, loss 0.0262942, acc 0.96875\n",
      "2018-05-01T21:38:12.032613: step 1221, loss 0.0281007, acc 0.96875\n",
      "2018-05-01T21:38:12.047653: step 1222, loss 0.0171829, acc 1\n",
      "2018-05-01T21:38:12.061993: step 1223, loss 0.00310722, acc 1\n",
      "2018-05-01T21:38:12.077179: step 1224, loss 0.00628049, acc 1\n",
      "2018-05-01T21:38:12.089566: step 1225, loss 0.00185939, acc 1\n",
      "2018-05-01T21:38:12.104568: step 1226, loss 0.00652755, acc 1\n",
      "2018-05-01T21:38:12.119700: step 1227, loss 0.0486364, acc 0.96875\n",
      "2018-05-01T21:38:12.134675: step 1228, loss 0.0729697, acc 0.96875\n",
      "2018-05-01T21:38:12.147280: step 1229, loss 0.0546917, acc 0.96875\n",
      "2018-05-01T21:38:12.161859: step 1230, loss 0.00104823, acc 1\n",
      "2018-05-01T21:38:12.175429: step 1231, loss 0.0281733, acc 1\n",
      "2018-05-01T21:38:12.188894: step 1232, loss 0.00730177, acc 1\n",
      "2018-05-01T21:38:12.204082: step 1233, loss 0.0301919, acc 1\n",
      "2018-05-01T21:38:12.218225: step 1234, loss 0.0148458, acc 1\n",
      "2018-05-01T21:38:12.231287: step 1235, loss 0.0097935, acc 1\n",
      "2018-05-01T21:38:12.245799: step 1236, loss 0.00628218, acc 1\n",
      "2018-05-01T21:38:12.258708: step 1237, loss 0.000896751, acc 1\n",
      "2018-05-01T21:38:12.272259: step 1238, loss 0.00497971, acc 1\n",
      "2018-05-01T21:38:12.287911: step 1239, loss 0.00370941, acc 1\n",
      "2018-05-01T21:38:12.303590: step 1240, loss 0.00624213, acc 1\n",
      "2018-05-01T21:38:12.317576: step 1241, loss 0.00570208, acc 1\n",
      "2018-05-01T21:38:12.331211: step 1242, loss 0.00154451, acc 1\n",
      "2018-05-01T21:38:12.347240: step 1243, loss 0.0145017, acc 1\n",
      "2018-05-01T21:38:12.361450: step 1244, loss 0.00268021, acc 1\n",
      "2018-05-01T21:38:12.376523: step 1245, loss 0.101706, acc 0.96875\n",
      "2018-05-01T21:38:12.389800: step 1246, loss 0.00157397, acc 1\n",
      "2018-05-01T21:38:12.403572: step 1247, loss 0.164419, acc 0.96875\n",
      "2018-05-01T21:38:12.416572: step 1248, loss 0.00677479, acc 1\n",
      "2018-05-01T21:38:12.430220: step 1249, loss 0.00901914, acc 1\n",
      "2018-05-01T21:38:12.442353: step 1250, loss 0.00161891, acc 1\n",
      "2018-05-01T21:38:12.456943: step 1251, loss 0.00346505, acc 1\n",
      "2018-05-01T21:38:12.470623: step 1252, loss 0.00372291, acc 1\n",
      "2018-05-01T21:38:12.484082: step 1253, loss 0.00459559, acc 1\n",
      "2018-05-01T21:38:12.497165: step 1254, loss 0.0363289, acc 0.96875\n",
      "2018-05-01T21:38:12.512008: step 1255, loss 0.0206558, acc 1\n",
      "2018-05-01T21:38:12.526463: step 1256, loss 0.00661473, acc 1\n",
      "2018-05-01T21:38:12.540535: step 1257, loss 0.0124471, acc 1\n",
      "2018-05-01T21:38:12.554698: step 1258, loss 0.0724629, acc 0.96875\n",
      "2018-05-01T21:38:12.567347: step 1259, loss 0.00233109, acc 1\n",
      "2018-05-01T21:38:12.581083: step 1260, loss 0.0250712, acc 0.96875\n",
      "2018-05-01T21:38:12.595858: step 1261, loss 0.0027149, acc 1\n",
      "2018-05-01T21:38:12.610046: step 1262, loss 0.0106623, acc 1\n",
      "2018-05-01T21:38:12.625328: step 1263, loss 0.023258, acc 1\n",
      "2018-05-01T21:38:12.640156: step 1264, loss 0.00521279, acc 1\n",
      "2018-05-01T21:38:12.664681: step 1265, loss 0.00403614, acc 1\n",
      "2018-05-01T21:38:12.678310: step 1266, loss 0.00567034, acc 1\n",
      "2018-05-01T21:38:12.692675: step 1267, loss 0.00360308, acc 1\n",
      "2018-05-01T21:38:12.706234: step 1268, loss 0.00473662, acc 1\n",
      "2018-05-01T21:38:12.719729: step 1269, loss 0.215805, acc 0.96875\n",
      "2018-05-01T21:38:12.733652: step 1270, loss 0.00568593, acc 1\n",
      "2018-05-01T21:38:12.750796: step 1271, loss 0.0317409, acc 0.96875\n",
      "2018-05-01T21:38:12.767666: step 1272, loss 0.00430998, acc 1\n",
      "2018-05-01T21:38:12.787090: step 1273, loss 0.00520466, acc 1\n",
      "2018-05-01T21:38:12.805254: step 1274, loss 0.00631925, acc 1\n",
      "2018-05-01T21:38:12.819775: step 1275, loss 0.117338, acc 0.944444\n",
      "2018-05-01T21:38:12.903708: step 1276, loss 0.0074516, acc 1\n",
      "2018-05-01T21:38:12.922208: step 1277, loss 0.0604621, acc 0.96875\n",
      "2018-05-01T21:38:12.937984: step 1278, loss 0.0307226, acc 1\n",
      "2018-05-01T21:38:12.959726: step 1279, loss 0.0120355, acc 1\n",
      "2018-05-01T21:38:12.975629: step 1280, loss 0.00257542, acc 1\n",
      "2018-05-01T21:38:12.992639: step 1281, loss 0.0250116, acc 1\n",
      "2018-05-01T21:38:13.009781: step 1282, loss 0.00425944, acc 1\n",
      "2018-05-01T21:38:13.032007: step 1283, loss 0.0038966, acc 1\n",
      "2018-05-01T21:38:13.050607: step 1284, loss 0.0299591, acc 1\n",
      "2018-05-01T21:38:13.065667: step 1285, loss 0.0585065, acc 0.96875\n",
      "2018-05-01T21:38:13.079471: step 1286, loss 0.00488397, acc 1\n",
      "2018-05-01T21:38:13.092488: step 1287, loss 0.0132957, acc 1\n",
      "2018-05-01T21:38:13.106076: step 1288, loss 0.00863883, acc 1\n",
      "2018-05-01T21:38:13.121874: step 1289, loss 0.0232456, acc 1\n",
      "2018-05-01T21:38:13.139024: step 1290, loss 0.0240737, acc 1\n",
      "2018-05-01T21:38:13.155003: step 1291, loss 0.148988, acc 0.96875\n",
      "2018-05-01T21:38:13.171466: step 1292, loss 0.00230526, acc 1\n",
      "2018-05-01T21:38:13.188457: step 1293, loss 0.00507642, acc 1\n",
      "2018-05-01T21:38:13.204015: step 1294, loss 0.00504319, acc 1\n",
      "2018-05-01T21:38:13.218935: step 1295, loss 0.00761821, acc 1\n",
      "2018-05-01T21:38:13.233284: step 1296, loss 0.0112071, acc 1\n",
      "2018-05-01T21:38:13.246345: step 1297, loss 0.00668789, acc 1\n",
      "2018-05-01T21:38:13.259893: step 1298, loss 0.0320096, acc 1\n",
      "2018-05-01T21:38:13.273189: step 1299, loss 0.0046986, acc 1\n",
      "2018-05-01T21:38:13.288476: step 1300, loss 0.00174266, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:13.292498: step 1300, loss 0.559418, acc 0.793103\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-1300\n",
      "\n",
      "2018-05-01T21:38:13.361939: step 1301, loss 0.00654901, acc 1\n",
      "2018-05-01T21:38:13.377190: step 1302, loss 0.00338269, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:13.391702: step 1303, loss 0.0442935, acc 0.96875\n",
      "2018-05-01T21:38:13.405630: step 1304, loss 0.0179462, acc 1\n",
      "2018-05-01T21:38:13.420261: step 1305, loss 0.0364912, acc 0.96875\n",
      "2018-05-01T21:38:13.434675: step 1306, loss 0.0041966, acc 1\n",
      "2018-05-01T21:38:13.450383: step 1307, loss 0.00680606, acc 1\n",
      "2018-05-01T21:38:13.463780: step 1308, loss 0.0511445, acc 0.96875\n",
      "2018-05-01T21:38:13.479764: step 1309, loss 0.00359177, acc 1\n",
      "2018-05-01T21:38:13.495556: step 1310, loss 0.167279, acc 0.90625\n",
      "2018-05-01T21:38:13.510371: step 1311, loss 0.000615435, acc 1\n",
      "2018-05-01T21:38:13.525339: step 1312, loss 0.00189316, acc 1\n",
      "2018-05-01T21:38:13.539896: step 1313, loss 0.0102677, acc 1\n",
      "2018-05-01T21:38:13.554258: step 1314, loss 0.00665563, acc 1\n",
      "2018-05-01T21:38:13.567179: step 1315, loss 0.00127745, acc 1\n",
      "2018-05-01T21:38:13.579700: step 1316, loss 0.0381749, acc 0.96875\n",
      "2018-05-01T21:38:13.592829: step 1317, loss 0.010596, acc 1\n",
      "2018-05-01T21:38:13.607785: step 1318, loss 0.00342829, acc 1\n",
      "2018-05-01T21:38:13.622383: step 1319, loss 0.0134294, acc 1\n",
      "2018-05-01T21:38:13.636550: step 1320, loss 0.00207327, acc 1\n",
      "2018-05-01T21:38:13.651481: step 1321, loss 0.0837017, acc 0.96875\n",
      "2018-05-01T21:38:13.664657: step 1322, loss 0.00251722, acc 1\n",
      "2018-05-01T21:38:13.677392: step 1323, loss 0.00156876, acc 1\n",
      "2018-05-01T21:38:13.690153: step 1324, loss 0.0902122, acc 0.96875\n",
      "2018-05-01T21:38:13.704244: step 1325, loss 0.00791211, acc 1\n",
      "2018-05-01T21:38:13.718599: step 1326, loss 0.139383, acc 0.96875\n",
      "2018-05-01T21:38:13.731414: step 1327, loss 0.0113045, acc 1\n",
      "2018-05-01T21:38:13.745595: step 1328, loss 0.0358576, acc 0.96875\n",
      "2018-05-01T21:38:13.761183: step 1329, loss 0.0157483, acc 1\n",
      "2018-05-01T21:38:13.774799: step 1330, loss 0.0185738, acc 1\n",
      "2018-05-01T21:38:13.787666: step 1331, loss 0.00702979, acc 1\n",
      "2018-05-01T21:38:13.802703: step 1332, loss 0.15773, acc 0.9375\n",
      "2018-05-01T21:38:13.817890: step 1333, loss 0.00598106, acc 1\n",
      "2018-05-01T21:38:13.830450: step 1334, loss 0.00204764, acc 1\n",
      "2018-05-01T21:38:13.843402: step 1335, loss 0.0025469, acc 1\n",
      "2018-05-01T21:38:13.856282: step 1336, loss 0.00429663, acc 1\n",
      "2018-05-01T21:38:13.870303: step 1337, loss 0.00651978, acc 1\n",
      "2018-05-01T21:38:13.883914: step 1338, loss 0.00675244, acc 1\n",
      "2018-05-01T21:38:13.896826: step 1339, loss 0.0050184, acc 1\n",
      "2018-05-01T21:38:13.912990: step 1340, loss 0.00280351, acc 1\n",
      "2018-05-01T21:38:13.927295: step 1341, loss 0.00245011, acc 1\n",
      "2018-05-01T21:38:13.940999: step 1342, loss 0.00673426, acc 1\n",
      "2018-05-01T21:38:13.955623: step 1343, loss 0.0117395, acc 1\n",
      "2018-05-01T21:38:13.969181: step 1344, loss 0.0132553, acc 1\n",
      "2018-05-01T21:38:13.982762: step 1345, loss 0.145355, acc 0.96875\n",
      "2018-05-01T21:38:13.996927: step 1346, loss 0.0040509, acc 1\n",
      "2018-05-01T21:38:14.012003: step 1347, loss 0.026061, acc 0.96875\n",
      "2018-05-01T21:38:14.026650: step 1348, loss 0.141534, acc 0.96875\n",
      "2018-05-01T21:38:14.040917: step 1349, loss 0.0653785, acc 0.96875\n",
      "2018-05-01T21:38:14.053440: step 1350, loss 0.00298186, acc 1\n",
      "2018-05-01T21:38:14.067057: step 1351, loss 0.03986, acc 0.96875\n",
      "2018-05-01T21:38:14.080995: step 1352, loss 0.0760288, acc 0.96875\n",
      "2018-05-01T21:38:14.094379: step 1353, loss 0.00577569, acc 1\n",
      "2018-05-01T21:38:14.107630: step 1354, loss 0.00353758, acc 1\n",
      "2018-05-01T21:38:14.121033: step 1355, loss 0.0148772, acc 1\n",
      "2018-05-01T21:38:14.138586: step 1356, loss 0.079539, acc 0.96875\n",
      "2018-05-01T21:38:14.159710: step 1357, loss 0.141513, acc 0.96875\n",
      "2018-05-01T21:38:14.173554: step 1358, loss 0.00356837, acc 1\n",
      "2018-05-01T21:38:14.189425: step 1359, loss 0.0266356, acc 1\n",
      "2018-05-01T21:38:14.206582: step 1360, loss 0.0110855, acc 1\n",
      "2018-05-01T21:38:14.236393: step 1361, loss 0.0328854, acc 0.96875\n",
      "2018-05-01T21:38:14.258793: step 1362, loss 0.00531778, acc 1\n",
      "2018-05-01T21:38:14.292127: step 1363, loss 0.0144969, acc 1\n",
      "2018-05-01T21:38:14.322283: step 1364, loss 0.16867, acc 0.9375\n",
      "2018-05-01T21:38:14.340233: step 1365, loss 0.00483697, acc 1\n",
      "2018-05-01T21:38:14.359249: step 1366, loss 0.0148305, acc 1\n",
      "2018-05-01T21:38:14.376397: step 1367, loss 0.0173824, acc 1\n",
      "2018-05-01T21:38:14.394210: step 1368, loss 0.00686879, acc 1\n",
      "2018-05-01T21:38:14.414896: step 1369, loss 0.0126635, acc 1\n",
      "2018-05-01T21:38:14.433328: step 1370, loss 0.028795, acc 0.96875\n",
      "2018-05-01T21:38:14.451479: step 1371, loss 0.0067388, acc 1\n",
      "2018-05-01T21:38:14.470089: step 1372, loss 0.00344863, acc 1\n",
      "2018-05-01T21:38:14.485828: step 1373, loss 0.00448446, acc 1\n",
      "2018-05-01T21:38:14.503224: step 1374, loss 0.0288326, acc 1\n",
      "2018-05-01T21:38:14.517725: step 1375, loss 0.00494681, acc 1\n",
      "2018-05-01T21:38:14.535410: step 1376, loss 0.0783821, acc 0.9375\n",
      "2018-05-01T21:38:14.556495: step 1377, loss 0.00329774, acc 1\n",
      "2018-05-01T21:38:14.570978: step 1378, loss 0.00212125, acc 1\n",
      "2018-05-01T21:38:14.584073: step 1379, loss 0.0090252, acc 1\n",
      "2018-05-01T21:38:14.599857: step 1380, loss 0.00346725, acc 1\n",
      "2018-05-01T21:38:14.614347: step 1381, loss 0.018657, acc 1\n",
      "2018-05-01T21:38:14.628595: step 1382, loss 0.00182262, acc 1\n",
      "2018-05-01T21:38:14.642734: step 1383, loss 0.0074437, acc 1\n",
      "2018-05-01T21:38:14.659461: step 1384, loss 0.156532, acc 0.96875\n",
      "2018-05-01T21:38:14.673503: step 1385, loss 0.0173931, acc 1\n",
      "2018-05-01T21:38:14.687456: step 1386, loss 0.00223256, acc 1\n",
      "2018-05-01T21:38:14.700392: step 1387, loss 0.0021135, acc 1\n",
      "2018-05-01T21:38:14.715014: step 1388, loss 0.00924031, acc 1\n",
      "2018-05-01T21:38:14.730140: step 1389, loss 0.0033172, acc 1\n",
      "2018-05-01T21:38:14.742740: step 1390, loss 0.00352326, acc 1\n",
      "2018-05-01T21:38:14.759098: step 1391, loss 0.00648753, acc 1\n",
      "2018-05-01T21:38:14.773918: step 1392, loss 0.000503685, acc 1\n",
      "2018-05-01T21:38:14.788374: step 1393, loss 0.00266197, acc 1\n",
      "2018-05-01T21:38:14.801109: step 1394, loss 0.00108162, acc 1\n",
      "2018-05-01T21:38:14.814539: step 1395, loss 0.00238038, acc 1\n",
      "2018-05-01T21:38:14.827380: step 1396, loss 0.0572441, acc 0.96875\n",
      "2018-05-01T21:38:14.842683: step 1397, loss 0.003234, acc 1\n",
      "2018-05-01T21:38:14.856546: step 1398, loss 0.00179117, acc 1\n",
      "2018-05-01T21:38:14.870220: step 1399, loss 0.0242759, acc 1\n",
      "2018-05-01T21:38:14.882013: step 1400, loss 0.00140915, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:14.886405: step 1400, loss 0.606061, acc 0.804598\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-1400\n",
      "\n",
      "2018-05-01T21:38:14.950895: step 1401, loss 0.00710423, acc 1\n",
      "2018-05-01T21:38:14.964992: step 1402, loss 0.000674642, acc 1\n",
      "2018-05-01T21:38:14.977830: step 1403, loss 0.030047, acc 0.96875\n",
      "2018-05-01T21:38:14.992885: step 1404, loss 0.00545667, acc 1\n",
      "2018-05-01T21:38:15.008544: step 1405, loss 0.0251159, acc 1\n",
      "2018-05-01T21:38:15.024106: step 1406, loss 0.0115559, acc 1\n",
      "2018-05-01T21:38:15.037807: step 1407, loss 0.0276694, acc 0.96875\n",
      "2018-05-01T21:38:15.051453: step 1408, loss 0.0163691, acc 1\n",
      "2018-05-01T21:38:15.066687: step 1409, loss 0.00435853, acc 1\n",
      "2018-05-01T21:38:15.080417: step 1410, loss 0.00348523, acc 1\n",
      "2018-05-01T21:38:15.093684: step 1411, loss 0.233748, acc 0.9375\n",
      "2018-05-01T21:38:15.107840: step 1412, loss 0.00841269, acc 1\n",
      "2018-05-01T21:38:15.121244: step 1413, loss 0.00867444, acc 1\n",
      "2018-05-01T21:38:15.134397: step 1414, loss 0.00200444, acc 1\n",
      "2018-05-01T21:38:15.147473: step 1415, loss 0.00102521, acc 1\n",
      "2018-05-01T21:38:15.161854: step 1416, loss 0.0142676, acc 1\n",
      "2018-05-01T21:38:15.176718: step 1417, loss 0.0888421, acc 0.96875\n",
      "2018-05-01T21:38:15.190042: step 1418, loss 0.0043542, acc 1\n",
      "2018-05-01T21:38:15.203916: step 1419, loss 0.00434257, acc 1\n",
      "2018-05-01T21:38:15.216863: step 1420, loss 0.10707, acc 0.96875\n",
      "2018-05-01T21:38:15.230437: step 1421, loss 0.00399067, acc 1\n",
      "2018-05-01T21:38:15.243457: step 1422, loss 0.00135823, acc 1\n",
      "2018-05-01T21:38:15.258934: step 1423, loss 0.00180568, acc 1\n",
      "2018-05-01T21:38:15.273657: step 1424, loss 0.0048901, acc 1\n",
      "2018-05-01T21:38:15.287210: step 1425, loss 0.00125698, acc 1\n",
      "2018-05-01T21:38:15.299999: step 1426, loss 0.00665449, acc 1\n",
      "2018-05-01T21:38:15.319408: step 1427, loss 0.0307824, acc 0.96875\n",
      "2018-05-01T21:38:15.333254: step 1428, loss 0.00330279, acc 1\n",
      "2018-05-01T21:38:15.347932: step 1429, loss 0.125345, acc 0.9375\n",
      "2018-05-01T21:38:15.361205: step 1430, loss 0.0508972, acc 0.96875\n",
      "2018-05-01T21:38:15.375318: step 1431, loss 0.00706379, acc 1\n",
      "2018-05-01T21:38:15.389462: step 1432, loss 0.00549872, acc 1\n",
      "2018-05-01T21:38:15.404475: step 1433, loss 0.0047334, acc 1\n",
      "2018-05-01T21:38:15.417663: step 1434, loss 0.0151091, acc 1\n",
      "2018-05-01T21:38:15.430869: step 1435, loss 0.0115676, acc 1\n",
      "2018-05-01T21:38:15.443547: step 1436, loss 0.166364, acc 0.96875\n",
      "2018-05-01T21:38:15.458678: step 1437, loss 0.065428, acc 0.96875\n",
      "2018-05-01T21:38:15.471928: step 1438, loss 0.00166308, acc 1\n",
      "2018-05-01T21:38:15.485024: step 1439, loss 0.0608058, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:15.500602: step 1440, loss 0.0744443, acc 0.96875\n",
      "2018-05-01T21:38:15.514087: step 1441, loss 0.00189713, acc 1\n",
      "2018-05-01T21:38:15.527450: step 1442, loss 0.00187774, acc 1\n",
      "2018-05-01T21:38:15.543168: step 1443, loss 0.009576, acc 1\n",
      "2018-05-01T21:38:15.558514: step 1444, loss 0.00952756, acc 1\n",
      "2018-05-01T21:38:15.574379: step 1445, loss 0.0314142, acc 1\n",
      "2018-05-01T21:38:15.589431: step 1446, loss 0.00134803, acc 1\n",
      "2018-05-01T21:38:15.605289: step 1447, loss 0.00314742, acc 1\n",
      "2018-05-01T21:38:15.619769: step 1448, loss 0.0124858, acc 1\n",
      "2018-05-01T21:38:15.634139: step 1449, loss 0.00331389, acc 1\n",
      "2018-05-01T21:38:15.646270: step 1450, loss 0.00230855, acc 1\n",
      "2018-05-01T21:38:15.659948: step 1451, loss 0.00252761, acc 1\n",
      "2018-05-01T21:38:15.673641: step 1452, loss 0.0996227, acc 0.96875\n",
      "2018-05-01T21:38:15.689468: step 1453, loss 0.0140244, acc 1\n",
      "2018-05-01T21:38:15.708294: step 1454, loss 0.00870886, acc 1\n",
      "2018-05-01T21:38:15.725628: step 1455, loss 0.166525, acc 0.96875\n",
      "2018-05-01T21:38:15.741621: step 1456, loss 0.0265662, acc 0.96875\n",
      "2018-05-01T21:38:15.755505: step 1457, loss 0.0142963, acc 1\n",
      "2018-05-01T21:38:15.770143: step 1458, loss 0.160768, acc 0.96875\n",
      "2018-05-01T21:38:15.784622: step 1459, loss 0.0857367, acc 0.96875\n",
      "2018-05-01T21:38:15.797983: step 1460, loss 0.00482832, acc 1\n",
      "2018-05-01T21:38:15.812469: step 1461, loss 0.00403582, acc 1\n",
      "2018-05-01T21:38:15.827433: step 1462, loss 0.00821085, acc 1\n",
      "2018-05-01T21:38:15.841932: step 1463, loss 0.00584136, acc 1\n",
      "2018-05-01T21:38:15.858534: step 1464, loss 0.00160284, acc 1\n",
      "2018-05-01T21:38:15.874166: step 1465, loss 0.0198621, acc 1\n",
      "2018-05-01T21:38:15.893295: step 1466, loss 0.0147125, acc 1\n",
      "2018-05-01T21:38:15.909915: step 1467, loss 0.0986633, acc 0.96875\n",
      "2018-05-01T21:38:15.928314: step 1468, loss 0.173869, acc 0.96875\n",
      "2018-05-01T21:38:15.950605: step 1469, loss 0.00193819, acc 1\n",
      "2018-05-01T21:38:15.971787: step 1470, loss 0.00253817, acc 1\n",
      "2018-05-01T21:38:15.987911: step 1471, loss 0.0135773, acc 1\n",
      "2018-05-01T21:38:16.007503: step 1472, loss 0.00619416, acc 1\n",
      "2018-05-01T21:38:16.024997: step 1473, loss 0.00791521, acc 1\n",
      "2018-05-01T21:38:16.043363: step 1474, loss 0.00570071, acc 1\n",
      "2018-05-01T21:38:16.060661: step 1475, loss 0.00270471, acc 1\n",
      "2018-05-01T21:38:16.078717: step 1476, loss 0.000620834, acc 1\n",
      "2018-05-01T21:38:16.098099: step 1477, loss 0.0673241, acc 0.96875\n",
      "2018-05-01T21:38:16.113738: step 1478, loss 0.00568877, acc 1\n",
      "2018-05-01T21:38:16.130522: step 1479, loss 0.0150976, acc 1\n",
      "2018-05-01T21:38:16.144449: step 1480, loss 0.00374088, acc 1\n",
      "2018-05-01T21:38:16.158514: step 1481, loss 0.0438943, acc 0.96875\n",
      "2018-05-01T21:38:16.171851: step 1482, loss 0.00895595, acc 1\n",
      "2018-05-01T21:38:16.184552: step 1483, loss 0.000923467, acc 1\n",
      "2018-05-01T21:38:16.200124: step 1484, loss 0.11108, acc 0.96875\n",
      "2018-05-01T21:38:16.213888: step 1485, loss 0.00381674, acc 1\n",
      "2018-05-01T21:38:16.227831: step 1486, loss 0.00643125, acc 1\n",
      "2018-05-01T21:38:16.241930: step 1487, loss 0.00443652, acc 1\n",
      "2018-05-01T21:38:16.255437: step 1488, loss 0.00961872, acc 1\n",
      "2018-05-01T21:38:16.269408: step 1489, loss 0.116664, acc 0.96875\n",
      "2018-05-01T21:38:16.283238: step 1490, loss 0.00350438, acc 1\n",
      "2018-05-01T21:38:16.298315: step 1491, loss 0.00504623, acc 1\n",
      "2018-05-01T21:38:16.311903: step 1492, loss 0.0269612, acc 0.96875\n",
      "2018-05-01T21:38:16.327739: step 1493, loss 0.172031, acc 0.96875\n",
      "2018-05-01T21:38:16.342071: step 1494, loss 0.00520185, acc 1\n",
      "2018-05-01T21:38:16.357674: step 1495, loss 0.0035899, acc 1\n",
      "2018-05-01T21:38:16.374566: step 1496, loss 0.0469603, acc 0.96875\n",
      "2018-05-01T21:38:16.389681: step 1497, loss 0.00237193, acc 1\n",
      "2018-05-01T21:38:16.403902: step 1498, loss 0.00342285, acc 1\n",
      "2018-05-01T21:38:16.418120: step 1499, loss 0.00411102, acc 1\n",
      "2018-05-01T21:38:16.432419: step 1500, loss 0.0879961, acc 0.944444\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:16.437679: step 1500, loss 0.558451, acc 0.804598\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-1500\n",
      "\n",
      "2018-05-01T21:38:16.505685: step 1501, loss 0.000757005, acc 1\n",
      "2018-05-01T21:38:16.519966: step 1502, loss 0.00451151, acc 1\n",
      "2018-05-01T21:38:16.534345: step 1503, loss 0.0075219, acc 1\n",
      "2018-05-01T21:38:16.548027: step 1504, loss 0.00145862, acc 1\n",
      "2018-05-01T21:38:16.562804: step 1505, loss 0.00380479, acc 1\n",
      "2018-05-01T21:38:16.577352: step 1506, loss 0.000627479, acc 1\n",
      "2018-05-01T21:38:16.591417: step 1507, loss 0.0108003, acc 1\n",
      "2018-05-01T21:38:16.605301: step 1508, loss 0.00138337, acc 1\n",
      "2018-05-01T21:38:16.620051: step 1509, loss 0.154657, acc 0.96875\n",
      "2018-05-01T21:38:16.635292: step 1510, loss 0.00522721, acc 1\n",
      "2018-05-01T21:38:16.649651: step 1511, loss 0.0013035, acc 1\n",
      "2018-05-01T21:38:16.663315: step 1512, loss 0.00306507, acc 1\n",
      "2018-05-01T21:38:16.677074: step 1513, loss 0.00528149, acc 1\n",
      "2018-05-01T21:38:16.691307: step 1514, loss 0.00266274, acc 1\n",
      "2018-05-01T21:38:16.705465: step 1515, loss 0.00832242, acc 1\n",
      "2018-05-01T21:38:16.720205: step 1516, loss 0.263839, acc 0.96875\n",
      "2018-05-01T21:38:16.736566: step 1517, loss 0.222231, acc 0.96875\n",
      "2018-05-01T21:38:16.751954: step 1518, loss 0.0344524, acc 1\n",
      "2018-05-01T21:38:16.767037: step 1519, loss 0.0300201, acc 1\n",
      "2018-05-01T21:38:16.783732: step 1520, loss 0.00469446, acc 1\n",
      "2018-05-01T21:38:16.798696: step 1521, loss 0.00670607, acc 1\n",
      "2018-05-01T21:38:16.812783: step 1522, loss 0.00222407, acc 1\n",
      "2018-05-01T21:38:16.826962: step 1523, loss 0.0831313, acc 0.96875\n",
      "2018-05-01T21:38:16.843834: step 1524, loss 0.000121992, acc 1\n",
      "2018-05-01T21:38:16.858331: step 1525, loss 0.0851365, acc 0.944444\n",
      "2018-05-01T21:38:16.871621: step 1526, loss 0.0448024, acc 0.96875\n",
      "2018-05-01T21:38:16.884996: step 1527, loss 0.0182718, acc 1\n",
      "2018-05-01T21:38:16.899257: step 1528, loss 0.00895945, acc 1\n",
      "2018-05-01T21:38:16.912947: step 1529, loss 0.0584809, acc 0.96875\n",
      "2018-05-01T21:38:16.926754: step 1530, loss 0.00824068, acc 1\n",
      "2018-05-01T21:38:16.942885: step 1531, loss 0.00244812, acc 1\n",
      "2018-05-01T21:38:16.958753: step 1532, loss 0.00263334, acc 1\n",
      "2018-05-01T21:38:16.973006: step 1533, loss 0.00670932, acc 1\n",
      "2018-05-01T21:38:16.987627: step 1534, loss 0.00178458, acc 1\n",
      "2018-05-01T21:38:17.001659: step 1535, loss 0.0914806, acc 0.96875\n",
      "2018-05-01T21:38:17.015285: step 1536, loss 0.000976149, acc 1\n",
      "2018-05-01T21:38:17.028882: step 1537, loss 0.0135881, acc 1\n",
      "2018-05-01T21:38:17.044161: step 1538, loss 0.00530059, acc 1\n",
      "2018-05-01T21:38:17.059203: step 1539, loss 0.00424897, acc 1\n",
      "2018-05-01T21:38:17.074584: step 1540, loss 0.00440211, acc 1\n",
      "2018-05-01T21:38:17.089958: step 1541, loss 0.0985978, acc 0.96875\n",
      "2018-05-01T21:38:17.103026: step 1542, loss 0.000838781, acc 1\n",
      "2018-05-01T21:38:17.118117: step 1543, loss 0.00188484, acc 1\n",
      "2018-05-01T21:38:17.134003: step 1544, loss 0.116125, acc 0.96875\n",
      "2018-05-01T21:38:17.148559: step 1545, loss 0.0127154, acc 1\n",
      "2018-05-01T21:38:17.161588: step 1546, loss 0.00582082, acc 1\n",
      "2018-05-01T21:38:17.176911: step 1547, loss 0.00293852, acc 1\n",
      "2018-05-01T21:38:17.190428: step 1548, loss 0.0659858, acc 0.9375\n",
      "2018-05-01T21:38:17.205176: step 1549, loss 0.00841975, acc 1\n",
      "2018-05-01T21:38:17.218231: step 1550, loss 0.00192203, acc 1\n",
      "2018-05-01T21:38:17.232959: step 1551, loss 0.00918365, acc 1\n",
      "2018-05-01T21:38:17.248915: step 1552, loss 0.00224882, acc 1\n",
      "2018-05-01T21:38:17.266294: step 1553, loss 0.00656692, acc 1\n",
      "2018-05-01T21:38:17.280504: step 1554, loss 0.00138844, acc 1\n",
      "2018-05-01T21:38:17.298245: step 1555, loss 0.00560571, acc 1\n",
      "2018-05-01T21:38:17.315132: step 1556, loss 0.0177502, acc 1\n",
      "2018-05-01T21:38:17.330786: step 1557, loss 0.00220397, acc 1\n",
      "2018-05-01T21:38:17.344092: step 1558, loss 0.0016084, acc 1\n",
      "2018-05-01T21:38:17.360740: step 1559, loss 0.0304365, acc 0.96875\n",
      "2018-05-01T21:38:17.374482: step 1560, loss 0.00961026, acc 1\n",
      "2018-05-01T21:38:17.390600: step 1561, loss 0.00271872, acc 1\n",
      "2018-05-01T21:38:17.405396: step 1562, loss 0.00486699, acc 1\n",
      "2018-05-01T21:38:17.419239: step 1563, loss 0.045953, acc 0.96875\n",
      "2018-05-01T21:38:17.432865: step 1564, loss 0.00860064, acc 1\n",
      "2018-05-01T21:38:17.449609: step 1565, loss 0.00245222, acc 1\n",
      "2018-05-01T21:38:17.463818: step 1566, loss 0.0282492, acc 1\n",
      "2018-05-01T21:38:17.478435: step 1567, loss 0.0258049, acc 0.96875\n",
      "2018-05-01T21:38:17.494530: step 1568, loss 0.0458669, acc 0.96875\n",
      "2018-05-01T21:38:17.508702: step 1569, loss 0.0897751, acc 0.96875\n",
      "2018-05-01T21:38:17.522510: step 1570, loss 0.0193558, acc 1\n",
      "2018-05-01T21:38:17.535730: step 1571, loss 0.00351569, acc 1\n",
      "2018-05-01T21:38:17.549471: step 1572, loss 0.0361108, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:17.563808: step 1573, loss 0.0129281, acc 1\n",
      "2018-05-01T21:38:17.577178: step 1574, loss 0.0149822, acc 1\n",
      "2018-05-01T21:38:17.590261: step 1575, loss 0.00152255, acc 1\n",
      "2018-05-01T21:38:17.605100: step 1576, loss 0.0863811, acc 0.96875\n",
      "2018-05-01T21:38:17.618830: step 1577, loss 0.00884818, acc 1\n",
      "2018-05-01T21:38:17.633730: step 1578, loss 0.0134025, acc 1\n",
      "2018-05-01T21:38:17.648594: step 1579, loss 0.0270964, acc 1\n",
      "2018-05-01T21:38:17.663180: step 1580, loss 0.00300893, acc 1\n",
      "2018-05-01T21:38:17.681489: step 1581, loss 0.0261462, acc 0.96875\n",
      "2018-05-01T21:38:17.696749: step 1582, loss 0.00272043, acc 1\n",
      "2018-05-01T21:38:17.710883: step 1583, loss 0.0573524, acc 0.96875\n",
      "2018-05-01T21:38:17.726946: step 1584, loss 0.0179798, acc 1\n",
      "2018-05-01T21:38:17.743476: step 1585, loss 0.00244974, acc 1\n",
      "2018-05-01T21:38:17.757116: step 1586, loss 0.00143873, acc 1\n",
      "2018-05-01T21:38:17.772153: step 1587, loss 0.00844074, acc 1\n",
      "2018-05-01T21:38:17.786293: step 1588, loss 0.0338566, acc 0.96875\n",
      "2018-05-01T21:38:17.800063: step 1589, loss 0.0596004, acc 0.96875\n",
      "2018-05-01T21:38:17.813911: step 1590, loss 0.113277, acc 0.96875\n",
      "2018-05-01T21:38:17.829982: step 1591, loss 0.00251507, acc 1\n",
      "2018-05-01T21:38:17.847249: step 1592, loss 0.00288575, acc 1\n",
      "2018-05-01T21:38:17.863125: step 1593, loss 0.0269956, acc 1\n",
      "2018-05-01T21:38:17.877657: step 1594, loss 0.00115834, acc 1\n",
      "2018-05-01T21:38:17.891473: step 1595, loss 0.00331679, acc 1\n",
      "2018-05-01T21:38:17.907750: step 1596, loss 0.00582177, acc 1\n",
      "2018-05-01T21:38:17.921331: step 1597, loss 0.0100588, acc 1\n",
      "2018-05-01T21:38:17.937622: step 1598, loss 0.00779563, acc 1\n",
      "2018-05-01T21:38:17.968399: step 1599, loss 0.00113416, acc 1\n",
      "2018-05-01T21:38:17.991540: step 1600, loss 0.0131564, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:17.998403: step 1600, loss 0.577454, acc 0.804598\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-1600\n",
      "\n",
      "2018-05-01T21:38:18.100461: step 1601, loss 0.0201664, acc 1\n",
      "2018-05-01T21:38:18.120203: step 1602, loss 0.00330505, acc 1\n",
      "2018-05-01T21:38:18.138711: step 1603, loss 0.0756426, acc 0.96875\n",
      "2018-05-01T21:38:18.161688: step 1604, loss 0.000963849, acc 1\n",
      "2018-05-01T21:38:18.179717: step 1605, loss 0.00536944, acc 1\n",
      "2018-05-01T21:38:18.195117: step 1606, loss 0.00472014, acc 1\n",
      "2018-05-01T21:38:18.214168: step 1607, loss 0.141857, acc 0.96875\n",
      "2018-05-01T21:38:18.230862: step 1608, loss 0.0643874, acc 0.96875\n",
      "2018-05-01T21:38:18.248831: step 1609, loss 0.00236936, acc 1\n",
      "2018-05-01T21:38:18.269567: step 1610, loss 0.00368801, acc 1\n",
      "2018-05-01T21:38:18.291862: step 1611, loss 0.00593849, acc 1\n",
      "2018-05-01T21:38:18.308027: step 1612, loss 0.0209344, acc 1\n",
      "2018-05-01T21:38:18.320776: step 1613, loss 0.0118995, acc 1\n",
      "2018-05-01T21:38:18.335187: step 1614, loss 0.012544, acc 1\n",
      "2018-05-01T21:38:18.349884: step 1615, loss 0.00232482, acc 1\n",
      "2018-05-01T21:38:18.363347: step 1616, loss 0.042166, acc 0.96875\n",
      "2018-05-01T21:38:18.377094: step 1617, loss 0.00954044, acc 1\n",
      "2018-05-01T21:38:18.392089: step 1618, loss 0.00846099, acc 1\n",
      "2018-05-01T21:38:18.407098: step 1619, loss 0.0024352, acc 1\n",
      "2018-05-01T21:38:18.423744: step 1620, loss 0.00351128, acc 1\n",
      "2018-05-01T21:38:18.438908: step 1621, loss 0.00318011, acc 1\n",
      "2018-05-01T21:38:18.454621: step 1622, loss 0.0331823, acc 1\n",
      "2018-05-01T21:38:18.468875: step 1623, loss 0.00199944, acc 1\n",
      "2018-05-01T21:38:18.482971: step 1624, loss 0.088904, acc 0.9375\n",
      "2018-05-01T21:38:18.495566: step 1625, loss 0.00103513, acc 1\n",
      "2018-05-01T21:38:18.509550: step 1626, loss 0.00431338, acc 1\n",
      "2018-05-01T21:38:18.524186: step 1627, loss 0.0252995, acc 1\n",
      "2018-05-01T21:38:18.540127: step 1628, loss 0.0179658, acc 1\n",
      "2018-05-01T21:38:18.556129: step 1629, loss 0.0290711, acc 0.96875\n",
      "2018-05-01T21:38:18.572131: step 1630, loss 0.00614198, acc 1\n",
      "2018-05-01T21:38:18.589460: step 1631, loss 0.00125521, acc 1\n",
      "2018-05-01T21:38:18.604857: step 1632, loss 0.00725157, acc 1\n",
      "2018-05-01T21:38:18.620211: step 1633, loss 0.00128056, acc 1\n",
      "2018-05-01T21:38:18.637166: step 1634, loss 0.100542, acc 0.96875\n",
      "2018-05-01T21:38:18.651914: step 1635, loss 0.00291896, acc 1\n",
      "2018-05-01T21:38:18.666209: step 1636, loss 0.120984, acc 0.96875\n",
      "2018-05-01T21:38:18.680858: step 1637, loss 0.00345203, acc 1\n",
      "2018-05-01T21:38:18.698661: step 1638, loss 0.00635867, acc 1\n",
      "2018-05-01T21:38:18.712251: step 1639, loss 0.00132353, acc 1\n",
      "2018-05-01T21:38:18.728423: step 1640, loss 0.0226223, acc 1\n",
      "2018-05-01T21:38:18.744110: step 1641, loss 0.0021946, acc 1\n",
      "2018-05-01T21:38:18.758182: step 1642, loss 0.00182116, acc 1\n",
      "2018-05-01T21:38:18.772233: step 1643, loss 0.104422, acc 0.96875\n",
      "2018-05-01T21:38:18.786491: step 1644, loss 0.00490896, acc 1\n",
      "2018-05-01T21:38:18.801097: step 1645, loss 0.00986612, acc 1\n",
      "2018-05-01T21:38:18.816233: step 1646, loss 0.0645922, acc 0.96875\n",
      "2018-05-01T21:38:18.829661: step 1647, loss 0.00146451, acc 1\n",
      "2018-05-01T21:38:18.845044: step 1648, loss 0.223011, acc 0.96875\n",
      "2018-05-01T21:38:18.859370: step 1649, loss 0.00758904, acc 1\n",
      "2018-05-01T21:38:18.872287: step 1650, loss 0.00150718, acc 1\n",
      "2018-05-01T21:38:18.887330: step 1651, loss 0.00848855, acc 1\n",
      "2018-05-01T21:38:18.901192: step 1652, loss 0.00150294, acc 1\n",
      "2018-05-01T21:38:18.914047: step 1653, loss 0.0039371, acc 1\n",
      "2018-05-01T21:38:18.928076: step 1654, loss 0.177554, acc 0.96875\n",
      "2018-05-01T21:38:18.942787: step 1655, loss 0.019308, acc 1\n",
      "2018-05-01T21:38:18.956173: step 1656, loss 0.00234995, acc 1\n",
      "2018-05-01T21:38:18.970947: step 1657, loss 0.00634313, acc 1\n",
      "2018-05-01T21:38:18.988914: step 1658, loss 0.0019663, acc 1\n",
      "2018-05-01T21:38:19.003228: step 1659, loss 0.00304497, acc 1\n",
      "2018-05-01T21:38:19.016678: step 1660, loss 0.00859443, acc 1\n",
      "2018-05-01T21:38:19.032478: step 1661, loss 0.0215812, acc 1\n",
      "2018-05-01T21:38:19.047628: step 1662, loss 0.00429908, acc 1\n",
      "2018-05-01T21:38:19.068658: step 1663, loss 0.00797424, acc 1\n",
      "2018-05-01T21:38:19.085992: step 1664, loss 0.0205556, acc 1\n",
      "2018-05-01T21:38:19.101897: step 1665, loss 0.00265307, acc 1\n",
      "2018-05-01T21:38:19.117035: step 1666, loss 0.0571033, acc 0.96875\n",
      "2018-05-01T21:38:19.133385: step 1667, loss 0.0465557, acc 0.96875\n",
      "2018-05-01T21:38:19.148482: step 1668, loss 0.0157358, acc 1\n",
      "2018-05-01T21:38:19.162006: step 1669, loss 0.00550774, acc 1\n",
      "2018-05-01T21:38:19.177826: step 1670, loss 0.106263, acc 0.96875\n",
      "2018-05-01T21:38:19.194986: step 1671, loss 0.0220579, acc 1\n",
      "2018-05-01T21:38:19.212547: step 1672, loss 0.00262217, acc 1\n",
      "2018-05-01T21:38:19.232183: step 1673, loss 0.00236401, acc 1\n",
      "2018-05-01T21:38:19.317051: step 1674, loss 0.000380498, acc 1\n",
      "2018-05-01T21:38:19.337669: step 1675, loss 0.00180533, acc 1\n",
      "2018-05-01T21:38:19.358482: step 1676, loss 0.00252516, acc 1\n",
      "2018-05-01T21:38:19.376894: step 1677, loss 0.000399197, acc 1\n",
      "2018-05-01T21:38:19.394618: step 1678, loss 0.0347632, acc 0.96875\n",
      "2018-05-01T21:38:19.412885: step 1679, loss 0.00478283, acc 1\n",
      "2018-05-01T21:38:19.431015: step 1680, loss 0.0317869, acc 0.96875\n",
      "2018-05-01T21:38:19.448711: step 1681, loss 0.0250701, acc 1\n",
      "2018-05-01T21:38:19.468515: step 1682, loss 0.168012, acc 0.96875\n",
      "2018-05-01T21:38:19.487278: step 1683, loss 0.0033806, acc 1\n",
      "2018-05-01T21:38:19.500290: step 1684, loss 0.363958, acc 0.90625\n",
      "2018-05-01T21:38:19.515447: step 1685, loss 0.00104449, acc 1\n",
      "2018-05-01T21:38:19.529001: step 1686, loss 0.0076435, acc 1\n",
      "2018-05-01T21:38:19.542532: step 1687, loss 0.000745755, acc 1\n",
      "2018-05-01T21:38:19.555571: step 1688, loss 0.00213728, acc 1\n",
      "2018-05-01T21:38:19.570718: step 1689, loss 0.0105521, acc 1\n",
      "2018-05-01T21:38:19.584423: step 1690, loss 0.000933343, acc 1\n",
      "2018-05-01T21:38:19.601480: step 1691, loss 0.00203674, acc 1\n",
      "2018-05-01T21:38:19.616006: step 1692, loss 0.0247651, acc 0.96875\n",
      "2018-05-01T21:38:19.630291: step 1693, loss 0.048054, acc 0.96875\n",
      "2018-05-01T21:38:19.644275: step 1694, loss 0.0013422, acc 1\n",
      "2018-05-01T21:38:19.659962: step 1695, loss 0.00494489, acc 1\n",
      "2018-05-01T21:38:19.673247: step 1696, loss 0.0122206, acc 1\n",
      "2018-05-01T21:38:19.688149: step 1697, loss 0.00682897, acc 1\n",
      "2018-05-01T21:38:19.702145: step 1698, loss 0.0019249, acc 1\n",
      "2018-05-01T21:38:19.718708: step 1699, loss 0.00211653, acc 1\n",
      "2018-05-01T21:38:19.733751: step 1700, loss 0.00756749, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:19.737937: step 1700, loss 0.657085, acc 0.804598\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-1700\n",
      "\n",
      "2018-05-01T21:38:19.802682: step 1701, loss 0.00799423, acc 1\n",
      "2018-05-01T21:38:19.816604: step 1702, loss 0.0242618, acc 1\n",
      "2018-05-01T21:38:19.831303: step 1703, loss 0.034908, acc 0.96875\n",
      "2018-05-01T21:38:19.845308: step 1704, loss 0.0084767, acc 1\n",
      "2018-05-01T21:38:19.858187: step 1705, loss 0.00348109, acc 1\n",
      "2018-05-01T21:38:19.875472: step 1706, loss 0.0536976, acc 0.96875\n",
      "2018-05-01T21:38:19.891873: step 1707, loss 0.104394, acc 0.96875\n",
      "2018-05-01T21:38:19.913626: step 1708, loss 0.00238522, acc 1\n",
      "2018-05-01T21:38:19.929009: step 1709, loss 0.00122701, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:19.944726: step 1710, loss 0.00233868, acc 1\n",
      "2018-05-01T21:38:19.957734: step 1711, loss 0.00205032, acc 1\n",
      "2018-05-01T21:38:19.973096: step 1712, loss 0.000977443, acc 1\n",
      "2018-05-01T21:38:20.006652: step 1713, loss 0.00283933, acc 1\n",
      "2018-05-01T21:38:20.033149: step 1714, loss 0.0059167, acc 1\n",
      "2018-05-01T21:38:20.049984: step 1715, loss 0.143043, acc 0.96875\n",
      "2018-05-01T21:38:20.066838: step 1716, loss 0.00348745, acc 1\n",
      "2018-05-01T21:38:20.085476: step 1717, loss 0.0262542, acc 1\n",
      "2018-05-01T21:38:20.122481: step 1718, loss 0.000547907, acc 1\n",
      "2018-05-01T21:38:20.145447: step 1719, loss 0.0049042, acc 1\n",
      "2018-05-01T21:38:20.165935: step 1720, loss 0.0010278, acc 1\n",
      "2018-05-01T21:38:20.189623: step 1721, loss 0.00227295, acc 1\n",
      "2018-05-01T21:38:20.212814: step 1722, loss 0.00820911, acc 1\n",
      "2018-05-01T21:38:20.233172: step 1723, loss 0.0268647, acc 1\n",
      "2018-05-01T21:38:20.254017: step 1724, loss 0.0078082, acc 1\n",
      "2018-05-01T21:38:20.270693: step 1725, loss 0.0312036, acc 1\n",
      "2018-05-01T21:38:20.288894: step 1726, loss 0.01381, acc 1\n",
      "2018-05-01T21:38:20.309187: step 1727, loss 0.00154051, acc 1\n",
      "2018-05-01T21:38:20.327175: step 1728, loss 0.00563178, acc 1\n",
      "2018-05-01T21:38:20.342372: step 1729, loss 0.0197245, acc 1\n",
      "2018-05-01T21:38:20.362019: step 1730, loss 0.158577, acc 0.96875\n",
      "2018-05-01T21:38:20.387036: step 1731, loss 0.00340769, acc 1\n",
      "2018-05-01T21:38:20.402326: step 1732, loss 0.112258, acc 0.96875\n",
      "2018-05-01T21:38:20.416589: step 1733, loss 0.00495085, acc 1\n",
      "2018-05-01T21:38:20.430395: step 1734, loss 0.000734402, acc 1\n",
      "2018-05-01T21:38:20.448845: step 1735, loss 0.00880547, acc 1\n",
      "2018-05-01T21:38:20.463272: step 1736, loss 0.0894564, acc 0.96875\n",
      "2018-05-01T21:38:20.479170: step 1737, loss 0.00663126, acc 1\n",
      "2018-05-01T21:38:20.494296: step 1738, loss 0.00460664, acc 1\n",
      "2018-05-01T21:38:20.509054: step 1739, loss 0.00422742, acc 1\n",
      "2018-05-01T21:38:20.525695: step 1740, loss 0.0034123, acc 1\n",
      "2018-05-01T21:38:20.541053: step 1741, loss 0.00922315, acc 1\n",
      "2018-05-01T21:38:20.557796: step 1742, loss 0.0541396, acc 0.96875\n",
      "2018-05-01T21:38:20.573333: step 1743, loss 0.00107019, acc 1\n",
      "2018-05-01T21:38:20.590052: step 1744, loss 0.0511892, acc 0.96875\n",
      "2018-05-01T21:38:20.606693: step 1745, loss 0.00142493, acc 1\n",
      "2018-05-01T21:38:20.623395: step 1746, loss 0.00258744, acc 1\n",
      "2018-05-01T21:38:20.638912: step 1747, loss 0.00394163, acc 1\n",
      "2018-05-01T21:38:20.655041: step 1748, loss 0.00232679, acc 1\n",
      "2018-05-01T21:38:20.671946: step 1749, loss 0.00416581, acc 1\n",
      "2018-05-01T21:38:20.686441: step 1750, loss 0.00145004, acc 1\n",
      "2018-05-01T21:38:20.704031: step 1751, loss 0.0721251, acc 0.96875\n",
      "2018-05-01T21:38:20.719334: step 1752, loss 0.00291528, acc 1\n",
      "2018-05-01T21:38:20.733310: step 1753, loss 0.00450241, acc 1\n",
      "2018-05-01T21:38:20.747432: step 1754, loss 0.0654062, acc 0.96875\n",
      "2018-05-01T21:38:20.763065: step 1755, loss 0.051903, acc 0.96875\n",
      "2018-05-01T21:38:20.778871: step 1756, loss 0.0132394, acc 1\n",
      "2018-05-01T21:38:20.794119: step 1757, loss 0.000775938, acc 1\n",
      "2018-05-01T21:38:20.808419: step 1758, loss 0.00189284, acc 1\n",
      "2018-05-01T21:38:20.823144: step 1759, loss 0.002879, acc 1\n",
      "2018-05-01T21:38:20.838614: step 1760, loss 0.00429206, acc 1\n",
      "2018-05-01T21:38:20.855412: step 1761, loss 0.122421, acc 0.96875\n",
      "2018-05-01T21:38:20.869915: step 1762, loss 0.0046538, acc 1\n",
      "2018-05-01T21:38:20.885922: step 1763, loss 0.00132623, acc 1\n",
      "2018-05-01T21:38:20.899889: step 1764, loss 0.060758, acc 0.96875\n",
      "2018-05-01T21:38:20.913320: step 1765, loss 0.00870071, acc 1\n",
      "2018-05-01T21:38:20.929052: step 1766, loss 0.000382705, acc 1\n",
      "2018-05-01T21:38:20.945501: step 1767, loss 0.00160971, acc 1\n",
      "2018-05-01T21:38:20.960402: step 1768, loss 0.0809496, acc 0.96875\n",
      "2018-05-01T21:38:20.974680: step 1769, loss 0.0148222, acc 1\n",
      "2018-05-01T21:38:20.990100: step 1770, loss 0.00274983, acc 1\n",
      "2018-05-01T21:38:21.006442: step 1771, loss 0.04331, acc 0.96875\n",
      "2018-05-01T21:38:21.022569: step 1772, loss 0.0990132, acc 0.96875\n",
      "2018-05-01T21:38:21.038187: step 1773, loss 0.0102818, acc 1\n",
      "2018-05-01T21:38:21.052858: step 1774, loss 0.0298177, acc 0.96875\n",
      "2018-05-01T21:38:21.065505: step 1775, loss 0.00159595, acc 1\n",
      "2018-05-01T21:38:21.105405: step 1776, loss 0.0807707, acc 0.96875\n",
      "2018-05-01T21:38:21.124817: step 1777, loss 0.021227, acc 1\n",
      "2018-05-01T21:38:21.142325: step 1778, loss 0.000148848, acc 1\n",
      "2018-05-01T21:38:21.162440: step 1779, loss 0.00590019, acc 1\n",
      "2018-05-01T21:38:21.177864: step 1780, loss 0.00177984, acc 1\n",
      "2018-05-01T21:38:21.195304: step 1781, loss 0.00636093, acc 1\n",
      "2018-05-01T21:38:21.292461: step 1782, loss 0.0491826, acc 0.96875\n",
      "2018-05-01T21:38:21.310097: step 1783, loss 0.00642487, acc 1\n",
      "2018-05-01T21:38:21.329687: step 1784, loss 0.00425599, acc 1\n",
      "2018-05-01T21:38:21.351344: step 1785, loss 0.00764045, acc 1\n",
      "2018-05-01T21:38:21.430973: step 1786, loss 0.000520021, acc 1\n",
      "2018-05-01T21:38:21.447816: step 1787, loss 0.0485073, acc 0.96875\n",
      "2018-05-01T21:38:21.469122: step 1788, loss 0.0469141, acc 0.96875\n",
      "2018-05-01T21:38:21.485549: step 1789, loss 0.0315602, acc 0.96875\n",
      "2018-05-01T21:38:21.504329: step 1790, loss 0.00321785, acc 1\n",
      "2018-05-01T21:38:21.524201: step 1791, loss 0.00243567, acc 1\n",
      "2018-05-01T21:38:21.540509: step 1792, loss 0.000884876, acc 1\n",
      "2018-05-01T21:38:21.563703: step 1793, loss 0.0228924, acc 1\n",
      "2018-05-01T21:38:21.582329: step 1794, loss 0.0899217, acc 0.96875\n",
      "2018-05-01T21:38:21.597221: step 1795, loss 0.00375969, acc 1\n",
      "2018-05-01T21:38:21.609950: step 1796, loss 0.183525, acc 0.96875\n",
      "2018-05-01T21:38:21.625340: step 1797, loss 0.000409016, acc 1\n",
      "2018-05-01T21:38:21.639531: step 1798, loss 0.00198981, acc 1\n",
      "2018-05-01T21:38:21.652892: step 1799, loss 0.00255375, acc 1\n",
      "2018-05-01T21:38:21.667340: step 1800, loss 0.0159206, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:21.671670: step 1800, loss 0.573427, acc 0.83908\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-1800\n",
      "\n",
      "2018-05-01T21:38:21.739606: step 1801, loss 0.00197603, acc 1\n",
      "2018-05-01T21:38:21.752878: step 1802, loss 0.0226248, acc 1\n",
      "2018-05-01T21:38:21.768769: step 1803, loss 0.00295105, acc 1\n",
      "2018-05-01T21:38:21.783125: step 1804, loss 0.00220427, acc 1\n",
      "2018-05-01T21:38:21.797761: step 1805, loss 0.00473943, acc 1\n",
      "2018-05-01T21:38:21.811447: step 1806, loss 0.00351334, acc 1\n",
      "2018-05-01T21:38:21.825887: step 1807, loss 0.0568811, acc 0.96875\n",
      "2018-05-01T21:38:21.840564: step 1808, loss 0.0174893, acc 1\n",
      "2018-05-01T21:38:21.858805: step 1809, loss 0.00070743, acc 1\n",
      "2018-05-01T21:38:21.874930: step 1810, loss 0.0104876, acc 1\n",
      "2018-05-01T21:38:21.889969: step 1811, loss 0.0819197, acc 0.96875\n",
      "2018-05-01T21:38:21.904035: step 1812, loss 0.0120796, acc 1\n",
      "2018-05-01T21:38:21.919436: step 1813, loss 0.000888183, acc 1\n",
      "2018-05-01T21:38:21.938779: step 1814, loss 0.0035918, acc 1\n",
      "2018-05-01T21:38:21.952905: step 1815, loss 0.000121644, acc 1\n",
      "2018-05-01T21:38:21.969628: step 1816, loss 0.00230504, acc 1\n",
      "2018-05-01T21:38:21.986298: step 1817, loss 0.0152814, acc 1\n",
      "2018-05-01T21:38:22.001685: step 1818, loss 0.00203344, acc 1\n",
      "2018-05-01T21:38:22.018820: step 1819, loss 0.112038, acc 0.96875\n",
      "2018-05-01T21:38:22.034302: step 1820, loss 0.00212076, acc 1\n",
      "2018-05-01T21:38:22.050159: step 1821, loss 0.00242353, acc 1\n",
      "2018-05-01T21:38:22.066200: step 1822, loss 0.183317, acc 0.96875\n",
      "2018-05-01T21:38:22.082331: step 1823, loss 0.0215887, acc 1\n",
      "2018-05-01T21:38:22.098226: step 1824, loss 0.00489237, acc 1\n",
      "2018-05-01T21:38:22.110971: step 1825, loss 0.000802338, acc 1\n",
      "2018-05-01T21:38:22.125239: step 1826, loss 0.00376168, acc 1\n",
      "2018-05-01T21:38:22.141663: step 1827, loss 0.00233844, acc 1\n",
      "2018-05-01T21:38:22.160155: step 1828, loss 0.0930253, acc 0.96875\n",
      "2018-05-01T21:38:22.174419: step 1829, loss 0.00496163, acc 1\n",
      "2018-05-01T21:38:22.188638: step 1830, loss 0.139489, acc 0.96875\n",
      "2018-05-01T21:38:22.202758: step 1831, loss 0.0022157, acc 1\n",
      "2018-05-01T21:38:22.219865: step 1832, loss 0.00280743, acc 1\n",
      "2018-05-01T21:38:22.234263: step 1833, loss 0.00585995, acc 1\n",
      "2018-05-01T21:38:22.248374: step 1834, loss 0.00182893, acc 1\n",
      "2018-05-01T21:38:22.263514: step 1835, loss 0.0132245, acc 1\n",
      "2018-05-01T21:38:22.277225: step 1836, loss 0.00214667, acc 1\n",
      "2018-05-01T21:38:22.290504: step 1837, loss 0.000677475, acc 1\n",
      "2018-05-01T21:38:22.304633: step 1838, loss 0.00202071, acc 1\n",
      "2018-05-01T21:38:22.320884: step 1839, loss 0.013611, acc 1\n",
      "2018-05-01T21:38:22.335120: step 1840, loss 0.00433339, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:22.351029: step 1841, loss 0.0241866, acc 1\n",
      "2018-05-01T21:38:22.367058: step 1842, loss 0.00278447, acc 1\n",
      "2018-05-01T21:38:22.380167: step 1843, loss 0.00766889, acc 1\n",
      "2018-05-01T21:38:22.393378: step 1844, loss 0.0366231, acc 0.96875\n",
      "2018-05-01T21:38:22.408373: step 1845, loss 0.036197, acc 0.96875\n",
      "2018-05-01T21:38:22.423224: step 1846, loss 0.1348, acc 0.9375\n",
      "2018-05-01T21:38:22.438412: step 1847, loss 0.0445404, acc 0.96875\n",
      "2018-05-01T21:38:22.455411: step 1848, loss 0.0243338, acc 1\n",
      "2018-05-01T21:38:22.470081: step 1849, loss 0.000938393, acc 1\n",
      "2018-05-01T21:38:22.483433: step 1850, loss 0.000175523, acc 1\n",
      "2018-05-01T21:38:22.496261: step 1851, loss 0.00628276, acc 1\n",
      "2018-05-01T21:38:22.514096: step 1852, loss 0.00392906, acc 1\n",
      "2018-05-01T21:38:22.554670: step 1853, loss 0.00159877, acc 1\n",
      "2018-05-01T21:38:22.579661: step 1854, loss 0.00692264, acc 1\n",
      "2018-05-01T21:38:22.600693: step 1855, loss 0.00314186, acc 1\n",
      "2018-05-01T21:38:22.621984: step 1856, loss 0.00735282, acc 1\n",
      "2018-05-01T21:38:22.642400: step 1857, loss 0.0180609, acc 1\n",
      "2018-05-01T21:38:22.661727: step 1858, loss 0.195385, acc 0.9375\n",
      "2018-05-01T21:38:22.681729: step 1859, loss 0.138103, acc 0.96875\n",
      "2018-05-01T21:38:22.700363: step 1860, loss 0.00184989, acc 1\n",
      "2018-05-01T21:38:22.718555: step 1861, loss 0.00106231, acc 1\n",
      "2018-05-01T21:38:22.732361: step 1862, loss 0.0035945, acc 1\n",
      "2018-05-01T21:38:22.748775: step 1863, loss 0.00693981, acc 1\n",
      "2018-05-01T21:38:22.768743: step 1864, loss 0.00314703, acc 1\n",
      "2018-05-01T21:38:22.784284: step 1865, loss 0.116306, acc 0.96875\n",
      "2018-05-01T21:38:22.804499: step 1866, loss 0.00282723, acc 1\n",
      "2018-05-01T21:38:22.825841: step 1867, loss 0.022063, acc 1\n",
      "2018-05-01T21:38:22.844581: step 1868, loss 0.0358968, acc 0.96875\n",
      "2018-05-01T21:38:22.933928: step 1869, loss 0.00653547, acc 1\n",
      "2018-05-01T21:38:22.948026: step 1870, loss 0.220507, acc 0.96875\n",
      "2018-05-01T21:38:22.964300: step 1871, loss 0.038744, acc 0.96875\n",
      "2018-05-01T21:38:22.980583: step 1872, loss 0.0405528, acc 0.96875\n",
      "2018-05-01T21:38:22.993322: step 1873, loss 0.0147971, acc 1\n",
      "2018-05-01T21:38:23.009409: step 1874, loss 0.00198271, acc 1\n",
      "2018-05-01T21:38:23.022728: step 1875, loss 0.00575741, acc 1\n",
      "2018-05-01T21:38:23.035805: step 1876, loss 0.00308387, acc 1\n",
      "2018-05-01T21:38:23.050996: step 1877, loss 0.0085987, acc 1\n",
      "2018-05-01T21:38:23.066308: step 1878, loss 0.008311, acc 1\n",
      "2018-05-01T21:38:23.083338: step 1879, loss 0.12613, acc 0.96875\n",
      "2018-05-01T21:38:23.097260: step 1880, loss 0.126132, acc 0.96875\n",
      "2018-05-01T21:38:23.112663: step 1881, loss 0.00199002, acc 1\n",
      "2018-05-01T21:38:23.129040: step 1882, loss 0.0364666, acc 0.96875\n",
      "2018-05-01T21:38:23.145231: step 1883, loss 0.120775, acc 0.96875\n",
      "2018-05-01T21:38:23.160790: step 1884, loss 0.0041733, acc 1\n",
      "2018-05-01T21:38:23.175281: step 1885, loss 0.000658352, acc 1\n",
      "2018-05-01T21:38:23.190677: step 1886, loss 0.054512, acc 0.96875\n",
      "2018-05-01T21:38:23.207037: step 1887, loss 0.0154307, acc 1\n",
      "2018-05-01T21:38:23.221636: step 1888, loss 0.00242933, acc 1\n",
      "2018-05-01T21:38:23.236030: step 1889, loss 0.033262, acc 0.96875\n",
      "2018-05-01T21:38:23.249523: step 1890, loss 0.000807629, acc 1\n",
      "2018-05-01T21:38:23.265623: step 1891, loss 0.00320827, acc 1\n",
      "2018-05-01T21:38:23.279396: step 1892, loss 0.00338159, acc 1\n",
      "2018-05-01T21:38:23.294423: step 1893, loss 0.0520183, acc 0.96875\n",
      "2018-05-01T21:38:23.309150: step 1894, loss 0.000565286, acc 1\n",
      "2018-05-01T21:38:23.323720: step 1895, loss 0.00509786, acc 1\n",
      "2018-05-01T21:38:23.338021: step 1896, loss 0.00390708, acc 1\n",
      "2018-05-01T21:38:23.351997: step 1897, loss 0.0105005, acc 1\n",
      "2018-05-01T21:38:23.366206: step 1898, loss 0.00838675, acc 1\n",
      "2018-05-01T21:38:23.382258: step 1899, loss 0.0438155, acc 0.96875\n",
      "2018-05-01T21:38:23.397277: step 1900, loss 0.000601563, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:23.401798: step 1900, loss 0.616467, acc 0.827586\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-1900\n",
      "\n",
      "2018-05-01T21:38:23.494329: step 1901, loss 0.00885799, acc 1\n",
      "2018-05-01T21:38:23.509839: step 1902, loss 0.110707, acc 0.96875\n",
      "2018-05-01T21:38:23.528240: step 1903, loss 0.00094612, acc 1\n",
      "2018-05-01T21:38:23.544408: step 1904, loss 0.00386479, acc 1\n",
      "2018-05-01T21:38:23.560129: step 1905, loss 0.00432873, acc 1\n",
      "2018-05-01T21:38:23.575315: step 1906, loss 0.0574194, acc 0.96875\n",
      "2018-05-01T21:38:23.592496: step 1907, loss 0.00402112, acc 1\n",
      "2018-05-01T21:38:23.609138: step 1908, loss 0.0395032, acc 0.96875\n",
      "2018-05-01T21:38:23.627780: step 1909, loss 0.00531512, acc 1\n",
      "2018-05-01T21:38:23.645779: step 1910, loss 0.00378759, acc 1\n",
      "2018-05-01T21:38:23.665248: step 1911, loss 0.0540175, acc 0.96875\n",
      "2018-05-01T21:38:23.685357: step 1912, loss 0.00670868, acc 1\n",
      "2018-05-01T21:38:23.703212: step 1913, loss 0.000360366, acc 1\n",
      "2018-05-01T21:38:23.723353: step 1914, loss 0.0108816, acc 1\n",
      "2018-05-01T21:38:23.743141: step 1915, loss 0.00703051, acc 1\n",
      "2018-05-01T21:38:23.768159: step 1916, loss 0.0556085, acc 0.96875\n",
      "2018-05-01T21:38:23.789662: step 1917, loss 0.00364666, acc 1\n",
      "2018-05-01T21:38:23.807209: step 1918, loss 0.00207468, acc 1\n",
      "2018-05-01T21:38:23.827891: step 1919, loss 0.00584685, acc 1\n",
      "2018-05-01T21:38:23.848424: step 1920, loss 0.000912538, acc 1\n",
      "2018-05-01T21:38:23.867347: step 1921, loss 0.0391634, acc 0.96875\n",
      "2018-05-01T21:38:23.886622: step 1922, loss 0.00345075, acc 1\n",
      "2018-05-01T21:38:23.907304: step 1923, loss 0.00217215, acc 1\n",
      "2018-05-01T21:38:23.925643: step 1924, loss 0.126491, acc 0.96875\n",
      "2018-05-01T21:38:23.939207: step 1925, loss 0.00173997, acc 1\n",
      "2018-05-01T21:38:23.953325: step 1926, loss 0.0195775, acc 1\n",
      "2018-05-01T21:38:23.968962: step 1927, loss 0.00388247, acc 1\n",
      "2018-05-01T21:38:23.984927: step 1928, loss 0.055282, acc 0.96875\n",
      "2018-05-01T21:38:24.001094: step 1929, loss 0.00257547, acc 1\n",
      "2018-05-01T21:38:24.014664: step 1930, loss 0.0115326, acc 1\n",
      "2018-05-01T21:38:24.029666: step 1931, loss 0.0869972, acc 0.96875\n",
      "2018-05-01T21:38:24.048795: step 1932, loss 0.0014328, acc 1\n",
      "2018-05-01T21:38:24.063102: step 1933, loss 0.000889088, acc 1\n",
      "2018-05-01T21:38:24.077180: step 1934, loss 0.0212604, acc 1\n",
      "2018-05-01T21:38:24.090466: step 1935, loss 0.00318124, acc 1\n",
      "2018-05-01T21:38:24.105370: step 1936, loss 0.0021749, acc 1\n",
      "2018-05-01T21:38:24.119290: step 1937, loss 0.019093, acc 1\n",
      "2018-05-01T21:38:24.134112: step 1938, loss 0.00317415, acc 1\n",
      "2018-05-01T21:38:24.148899: step 1939, loss 0.000811116, acc 1\n",
      "2018-05-01T21:38:24.165532: step 1940, loss 0.00907215, acc 1\n",
      "2018-05-01T21:38:24.183772: step 1941, loss 0.00322263, acc 1\n",
      "2018-05-01T21:38:24.198730: step 1942, loss 0.181994, acc 0.96875\n",
      "2018-05-01T21:38:24.212950: step 1943, loss 0.00157437, acc 1\n",
      "2018-05-01T21:38:24.225657: step 1944, loss 0.000768757, acc 1\n",
      "2018-05-01T21:38:24.238627: step 1945, loss 0.00225679, acc 1\n",
      "2018-05-01T21:38:24.253130: step 1946, loss 0.00349429, acc 1\n",
      "2018-05-01T21:38:24.271331: step 1947, loss 0.00737008, acc 1\n",
      "2018-05-01T21:38:24.284328: step 1948, loss 0.00246452, acc 1\n",
      "2018-05-01T21:38:24.297343: step 1949, loss 0.189729, acc 0.9375\n",
      "2018-05-01T21:38:24.310056: step 1950, loss 0.0063084, acc 1\n",
      "2018-05-01T21:38:24.324323: step 1951, loss 0.000840442, acc 1\n",
      "2018-05-01T21:38:24.350785: step 1952, loss 0.0845102, acc 0.96875\n",
      "2018-05-01T21:38:24.378251: step 1953, loss 0.0032261, acc 1\n",
      "2018-05-01T21:38:24.404709: step 1954, loss 0.00123079, acc 1\n",
      "2018-05-01T21:38:24.507141: step 1955, loss 0.000983299, acc 1\n",
      "2018-05-01T21:38:24.543044: step 1956, loss 0.00758955, acc 1\n",
      "2018-05-01T21:38:24.557246: step 1957, loss 0.00321414, acc 1\n",
      "2018-05-01T21:38:24.578455: step 1958, loss 0.00101681, acc 1\n",
      "2018-05-01T21:38:24.594907: step 1959, loss 0.0448374, acc 0.96875\n",
      "2018-05-01T21:38:24.612349: step 1960, loss 0.0504973, acc 1\n",
      "2018-05-01T21:38:24.638647: step 1961, loss 0.0223965, acc 1\n",
      "2018-05-01T21:38:24.660756: step 1962, loss 0.00995748, acc 1\n",
      "2018-05-01T21:38:24.678160: step 1963, loss 0.000994863, acc 1\n",
      "2018-05-01T21:38:24.703203: step 1964, loss 0.00660402, acc 1\n",
      "2018-05-01T21:38:24.718729: step 1965, loss 0.00249952, acc 1\n",
      "2018-05-01T21:38:24.732909: step 1966, loss 0.00263844, acc 1\n",
      "2018-05-01T21:38:24.746316: step 1967, loss 0.00327566, acc 1\n",
      "2018-05-01T21:38:24.763178: step 1968, loss 0.0248906, acc 1\n",
      "2018-05-01T21:38:24.778603: step 1969, loss 0.0087694, acc 1\n",
      "2018-05-01T21:38:24.794733: step 1970, loss 0.231032, acc 0.96875\n",
      "2018-05-01T21:38:24.812186: step 1971, loss 0.00386802, acc 1\n",
      "2018-05-01T21:38:24.827917: step 1972, loss 0.00583198, acc 1\n",
      "2018-05-01T21:38:24.843701: step 1973, loss 0.023388, acc 1\n",
      "2018-05-01T21:38:24.859027: step 1974, loss 0.010892, acc 1\n",
      "2018-05-01T21:38:24.875206: step 1975, loss 0.00589989, acc 1\n",
      "2018-05-01T21:38:24.891177: step 1976, loss 0.0254928, acc 1\n",
      "2018-05-01T21:38:24.905439: step 1977, loss 0.00212451, acc 1\n",
      "2018-05-01T21:38:24.918543: step 1978, loss 0.0190132, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:24.933104: step 1979, loss 0.00249054, acc 1\n",
      "2018-05-01T21:38:24.947630: step 1980, loss 0.00805743, acc 1\n",
      "2018-05-01T21:38:24.963044: step 1981, loss 0.0835854, acc 0.96875\n",
      "2018-05-01T21:38:24.978073: step 1982, loss 0.00439778, acc 1\n",
      "2018-05-01T21:38:24.991852: step 1983, loss 0.179074, acc 0.9375\n",
      "2018-05-01T21:38:25.007362: step 1984, loss 0.129061, acc 0.96875\n",
      "2018-05-01T21:38:25.021269: step 1985, loss 0.0181123, acc 1\n",
      "2018-05-01T21:38:25.034238: step 1986, loss 0.00226636, acc 1\n",
      "2018-05-01T21:38:25.047769: step 1987, loss 0.00229788, acc 1\n",
      "2018-05-01T21:38:25.061737: step 1988, loss 0.0389576, acc 0.96875\n",
      "2018-05-01T21:38:25.076669: step 1989, loss 0.0021836, acc 1\n",
      "2018-05-01T21:38:25.090127: step 1990, loss 0.000865852, acc 1\n",
      "2018-05-01T21:38:25.103720: step 1991, loss 0.00327532, acc 1\n",
      "2018-05-01T21:38:25.116847: step 1992, loss 0.00118616, acc 1\n",
      "2018-05-01T21:38:25.131468: step 1993, loss 0.0733469, acc 0.96875\n",
      "2018-05-01T21:38:25.146759: step 1994, loss 0.00049593, acc 1\n",
      "2018-05-01T21:38:25.161474: step 1995, loss 0.00740969, acc 1\n",
      "2018-05-01T21:38:25.177402: step 1996, loss 0.00624485, acc 1\n",
      "2018-05-01T21:38:25.191171: step 1997, loss 0.0374539, acc 1\n",
      "2018-05-01T21:38:25.206439: step 1998, loss 0.00371295, acc 1\n",
      "2018-05-01T21:38:25.220572: step 1999, loss 0.00294358, acc 1\n",
      "2018-05-01T21:38:25.234216: step 2000, loss 0.00222118, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:25.238663: step 2000, loss 0.661764, acc 0.804598\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-2000\n",
      "\n",
      "2018-05-01T21:38:25.314492: step 2001, loss 0.00295956, acc 1\n",
      "2018-05-01T21:38:25.327226: step 2002, loss 0.0378671, acc 0.96875\n",
      "2018-05-01T21:38:25.341550: step 2003, loss 0.0255507, acc 0.96875\n",
      "2018-05-01T21:38:25.361159: step 2004, loss 0.0442949, acc 0.96875\n",
      "2018-05-01T21:38:25.387877: step 2005, loss 0.00507336, acc 1\n",
      "2018-05-01T21:38:25.405533: step 2006, loss 0.00368403, acc 1\n",
      "2018-05-01T21:38:25.423049: step 2007, loss 0.00760139, acc 1\n",
      "2018-05-01T21:38:25.438031: step 2008, loss 0.000708113, acc 1\n",
      "2018-05-01T21:38:25.457917: step 2009, loss 0.00347941, acc 1\n",
      "2018-05-01T21:38:25.478957: step 2010, loss 0.0295063, acc 0.96875\n",
      "2018-05-01T21:38:25.493460: step 2011, loss 0.00183547, acc 1\n",
      "2018-05-01T21:38:25.506620: step 2012, loss 0.00970936, acc 1\n",
      "2018-05-01T21:38:25.522451: step 2013, loss 0.000998639, acc 1\n",
      "2018-05-01T21:38:25.538226: step 2014, loss 0.00481838, acc 1\n",
      "2018-05-01T21:38:25.554960: step 2015, loss 0.000862446, acc 1\n",
      "2018-05-01T21:38:25.571410: step 2016, loss 0.00959207, acc 1\n",
      "2018-05-01T21:38:25.589583: step 2017, loss 0.138763, acc 0.96875\n",
      "2018-05-01T21:38:25.612303: step 2018, loss 0.0106695, acc 1\n",
      "2018-05-01T21:38:25.632980: step 2019, loss 0.00640295, acc 1\n",
      "2018-05-01T21:38:25.651154: step 2020, loss 0.00108927, acc 1\n",
      "2018-05-01T21:38:25.672968: step 2021, loss 0.00550718, acc 1\n",
      "2018-05-01T21:38:25.696125: step 2022, loss 0.174295, acc 0.96875\n",
      "2018-05-01T21:38:25.720759: step 2023, loss 0.235395, acc 0.9375\n",
      "2018-05-01T21:38:25.741082: step 2024, loss 0.000527643, acc 1\n",
      "2018-05-01T21:38:25.757903: step 2025, loss 0.00062937, acc 1\n",
      "2018-05-01T21:38:25.776542: step 2026, loss 0.0498223, acc 0.96875\n",
      "2018-05-01T21:38:25.792362: step 2027, loss 0.000808717, acc 1\n",
      "2018-05-01T21:38:25.811079: step 2028, loss 0.000504328, acc 1\n",
      "2018-05-01T21:38:25.831424: step 2029, loss 0.005508, acc 1\n",
      "2018-05-01T21:38:25.849303: step 2030, loss 0.00155954, acc 1\n",
      "2018-05-01T21:38:25.865938: step 2031, loss 0.00284675, acc 1\n",
      "2018-05-01T21:38:25.880855: step 2032, loss 0.00567232, acc 1\n",
      "2018-05-01T21:38:25.894352: step 2033, loss 0.00210593, acc 1\n",
      "2018-05-01T21:38:25.909759: step 2034, loss 0.00396127, acc 1\n",
      "2018-05-01T21:38:25.923037: step 2035, loss 0.0105687, acc 1\n",
      "2018-05-01T21:38:25.937690: step 2036, loss 0.0111017, acc 1\n",
      "2018-05-01T21:38:25.951203: step 2037, loss 0.173657, acc 0.9375\n",
      "2018-05-01T21:38:25.965987: step 2038, loss 0.125879, acc 0.96875\n",
      "2018-05-01T21:38:25.981147: step 2039, loss 0.00716988, acc 1\n",
      "2018-05-01T21:38:25.996338: step 2040, loss 0.00138871, acc 1\n",
      "2018-05-01T21:38:26.011100: step 2041, loss 0.0024148, acc 1\n",
      "2018-05-01T21:38:26.023737: step 2042, loss 0.0261433, acc 0.96875\n",
      "2018-05-01T21:38:26.039223: step 2043, loss 0.00164286, acc 1\n",
      "2018-05-01T21:38:26.053557: step 2044, loss 0.00422511, acc 1\n",
      "2018-05-01T21:38:26.066792: step 2045, loss 0.000789603, acc 1\n",
      "2018-05-01T21:38:26.080400: step 2046, loss 0.0370966, acc 0.96875\n",
      "2018-05-01T21:38:26.093966: step 2047, loss 0.0136427, acc 1\n",
      "2018-05-01T21:38:26.109139: step 2048, loss 0.00666418, acc 1\n",
      "2018-05-01T21:38:26.122957: step 2049, loss 0.000447862, acc 1\n",
      "2018-05-01T21:38:26.134991: step 2050, loss 0.00259958, acc 1\n",
      "2018-05-01T21:38:26.149131: step 2051, loss 0.00193632, acc 1\n",
      "2018-05-01T21:38:26.162982: step 2052, loss 0.0110567, acc 1\n",
      "2018-05-01T21:38:26.176699: step 2053, loss 0.0292739, acc 0.96875\n",
      "2018-05-01T21:38:26.192599: step 2054, loss 0.000921982, acc 1\n",
      "2018-05-01T21:38:26.210223: step 2055, loss 0.00469203, acc 1\n",
      "2018-05-01T21:38:26.225027: step 2056, loss 0.00312206, acc 1\n",
      "2018-05-01T21:38:26.239339: step 2057, loss 0.0396712, acc 1\n",
      "2018-05-01T21:38:26.255685: step 2058, loss 0.034081, acc 0.96875\n",
      "2018-05-01T21:38:26.272073: step 2059, loss 0.00247704, acc 1\n",
      "2018-05-01T21:38:26.285931: step 2060, loss 0.000978297, acc 1\n",
      "2018-05-01T21:38:26.300484: step 2061, loss 0.0106847, acc 1\n",
      "2018-05-01T21:38:26.317111: step 2062, loss 0.00172365, acc 1\n",
      "2018-05-01T21:38:26.330610: step 2063, loss 0.0356653, acc 0.96875\n",
      "2018-05-01T21:38:26.343521: step 2064, loss 0.000448807, acc 1\n",
      "2018-05-01T21:38:26.359476: step 2065, loss 0.114554, acc 0.9375\n",
      "2018-05-01T21:38:26.376569: step 2066, loss 0.00217471, acc 1\n",
      "2018-05-01T21:38:26.394697: step 2067, loss 0.000694438, acc 1\n",
      "2018-05-01T21:38:26.410475: step 2068, loss 0.0222945, acc 1\n",
      "2018-05-01T21:38:26.426857: step 2069, loss 0.00151531, acc 1\n",
      "2018-05-01T21:38:26.440747: step 2070, loss 0.173857, acc 0.96875\n",
      "2018-05-01T21:38:26.473186: step 2071, loss 0.00169403, acc 1\n",
      "2018-05-01T21:38:26.488610: step 2072, loss 0.002226, acc 1\n",
      "2018-05-01T21:38:26.506945: step 2073, loss 0.00692678, acc 1\n",
      "2018-05-01T21:38:26.523765: step 2074, loss 0.00228923, acc 1\n",
      "2018-05-01T21:38:26.540825: step 2075, loss 0.00298217, acc 1\n",
      "2018-05-01T21:38:26.920742: step 2076, loss 0.00267628, acc 1\n",
      "2018-05-01T21:38:26.937330: step 2077, loss 0.00514994, acc 1\n",
      "2018-05-01T21:38:26.956056: step 2078, loss 0.00373945, acc 1\n",
      "2018-05-01T21:38:26.974046: step 2079, loss 0.100613, acc 0.96875\n",
      "2018-05-01T21:38:26.988202: step 2080, loss 0.0121613, acc 1\n",
      "2018-05-01T21:38:27.004496: step 2081, loss 0.00102496, acc 1\n",
      "2018-05-01T21:38:27.022228: step 2082, loss 0.00271632, acc 1\n",
      "2018-05-01T21:38:27.037834: step 2083, loss 0.0151773, acc 1\n",
      "2018-05-01T21:38:27.052994: step 2084, loss 0.00094739, acc 1\n",
      "2018-05-01T21:38:27.071019: step 2085, loss 0.00288569, acc 1\n",
      "2018-05-01T21:38:27.086984: step 2086, loss 0.113827, acc 0.9375\n",
      "2018-05-01T21:38:27.100632: step 2087, loss 0.00496705, acc 1\n",
      "2018-05-01T21:38:27.115817: step 2088, loss 0.0012349, acc 1\n",
      "2018-05-01T21:38:27.132466: step 2089, loss 0.0125618, acc 1\n",
      "2018-05-01T21:38:27.148811: step 2090, loss 0.0141585, acc 1\n",
      "2018-05-01T21:38:27.165207: step 2091, loss 0.0370285, acc 0.96875\n",
      "2018-05-01T21:38:27.185545: step 2092, loss 0.000385811, acc 1\n",
      "2018-05-01T21:38:27.288487: step 2093, loss 0.00093773, acc 1\n",
      "2018-05-01T21:38:27.303737: step 2094, loss 0.00204388, acc 1\n",
      "2018-05-01T21:38:27.321746: step 2095, loss 0.048963, acc 0.96875\n",
      "2018-05-01T21:38:27.334378: step 2096, loss 0.000851006, acc 1\n",
      "2018-05-01T21:38:27.359474: step 2097, loss 0.00710043, acc 1\n",
      "2018-05-01T21:38:27.374712: step 2098, loss 0.0239906, acc 0.96875\n",
      "2018-05-01T21:38:27.391537: step 2099, loss 0.000971849, acc 1\n",
      "2018-05-01T21:38:27.418736: step 2100, loss 0.0785671, acc 0.944444\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:27.424871: step 2100, loss 0.611373, acc 0.827586\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-2100\n",
      "\n",
      "2018-05-01T21:38:27.513357: step 2101, loss 0.0323688, acc 0.96875\n",
      "2018-05-01T21:38:27.533342: step 2102, loss 0.0566635, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:27.551980: step 2103, loss 0.00151619, acc 1\n",
      "2018-05-01T21:38:27.570534: step 2104, loss 0.00390571, acc 1\n",
      "2018-05-01T21:38:27.593718: step 2105, loss 0.00194278, acc 1\n",
      "2018-05-01T21:38:27.610692: step 2106, loss 0.0414631, acc 0.96875\n",
      "2018-05-01T21:38:27.627090: step 2107, loss 0.0398495, acc 0.96875\n",
      "2018-05-01T21:38:27.640695: step 2108, loss 0.0123929, acc 1\n",
      "2018-05-01T21:38:27.655370: step 2109, loss 0.00173688, acc 1\n",
      "2018-05-01T21:38:27.669482: step 2110, loss 0.00674439, acc 1\n",
      "2018-05-01T21:38:27.685067: step 2111, loss 0.00164392, acc 1\n",
      "2018-05-01T21:38:27.698894: step 2112, loss 0.00132189, acc 1\n",
      "2018-05-01T21:38:27.714628: step 2113, loss 0.221402, acc 0.96875\n",
      "2018-05-01T21:38:27.729379: step 2114, loss 0.0248123, acc 1\n",
      "2018-05-01T21:38:27.743504: step 2115, loss 0.00141275, acc 1\n",
      "2018-05-01T21:38:27.757704: step 2116, loss 0.0021509, acc 1\n",
      "2018-05-01T21:38:27.771282: step 2117, loss 0.0725258, acc 0.96875\n",
      "2018-05-01T21:38:27.784958: step 2118, loss 0.0134621, acc 1\n",
      "2018-05-01T21:38:27.800165: step 2119, loss 0.0452154, acc 0.96875\n",
      "2018-05-01T21:38:27.815182: step 2120, loss 0.0307624, acc 0.96875\n",
      "2018-05-01T21:38:27.829897: step 2121, loss 0.0330714, acc 0.96875\n",
      "2018-05-01T21:38:27.842670: step 2122, loss 0.00366843, acc 1\n",
      "2018-05-01T21:38:27.857020: step 2123, loss 0.0691208, acc 0.96875\n",
      "2018-05-01T21:38:27.876293: step 2124, loss 0.010882, acc 1\n",
      "2018-05-01T21:38:27.889111: step 2125, loss 0.000734977, acc 1\n",
      "2018-05-01T21:38:27.903643: step 2126, loss 0.0122468, acc 1\n",
      "2018-05-01T21:38:27.919867: step 2127, loss 0.00130623, acc 1\n",
      "2018-05-01T21:38:27.940061: step 2128, loss 0.024018, acc 0.96875\n",
      "2018-05-01T21:38:27.974486: step 2129, loss 0.00649573, acc 1\n",
      "2018-05-01T21:38:27.994119: step 2130, loss 0.00575799, acc 1\n",
      "2018-05-01T21:38:28.015882: step 2131, loss 0.0151655, acc 1\n",
      "2018-05-01T21:38:28.031893: step 2132, loss 0.0173224, acc 1\n",
      "2018-05-01T21:38:28.046727: step 2133, loss 0.00746955, acc 1\n",
      "2018-05-01T21:38:28.065674: step 2134, loss 0.259784, acc 0.96875\n",
      "2018-05-01T21:38:28.086592: step 2135, loss 0.00513654, acc 1\n",
      "2018-05-01T21:38:28.116908: step 2136, loss 0.00892093, acc 1\n",
      "2018-05-01T21:38:28.137879: step 2137, loss 0.00495376, acc 1\n",
      "2018-05-01T21:38:28.161294: step 2138, loss 0.00113999, acc 1\n",
      "2018-05-01T21:38:28.180217: step 2139, loss 0.00104399, acc 1\n",
      "2018-05-01T21:38:28.204369: step 2140, loss 0.00065642, acc 1\n",
      "2018-05-01T21:38:28.222886: step 2141, loss 0.0565659, acc 0.96875\n",
      "2018-05-01T21:38:28.237955: step 2142, loss 0.000602915, acc 1\n",
      "2018-05-01T21:38:28.254887: step 2143, loss 0.00817205, acc 1\n",
      "2018-05-01T21:38:28.272105: step 2144, loss 0.00568642, acc 1\n",
      "2018-05-01T21:38:28.291340: step 2145, loss 0.00279161, acc 1\n",
      "2018-05-01T21:38:28.311046: step 2146, loss 0.0773229, acc 0.96875\n",
      "2018-05-01T21:38:28.330020: step 2147, loss 0.0015318, acc 1\n",
      "2018-05-01T21:38:28.349976: step 2148, loss 0.0275715, acc 1\n",
      "2018-05-01T21:38:28.371112: step 2149, loss 0.0202241, acc 1\n",
      "2018-05-01T21:38:28.392889: step 2150, loss 0.00361408, acc 1\n",
      "2018-05-01T21:38:28.405729: step 2151, loss 0.0519367, acc 0.96875\n",
      "2018-05-01T21:38:28.420674: step 2152, loss 0.0659334, acc 0.96875\n",
      "2018-05-01T21:38:28.434062: step 2153, loss 0.196753, acc 0.96875\n",
      "2018-05-01T21:38:28.450049: step 2154, loss 0.0028792, acc 1\n",
      "2018-05-01T21:38:28.463987: step 2155, loss 0.000513609, acc 1\n",
      "2018-05-01T21:38:28.479319: step 2156, loss 0.0028257, acc 1\n",
      "2018-05-01T21:38:28.495176: step 2157, loss 0.0101559, acc 1\n",
      "2018-05-01T21:38:28.511726: step 2158, loss 0.00100346, acc 1\n",
      "2018-05-01T21:38:28.531701: step 2159, loss 0.00102391, acc 1\n",
      "2018-05-01T21:38:28.545561: step 2160, loss 0.00111108, acc 1\n",
      "2018-05-01T21:38:28.562673: step 2161, loss 0.000830862, acc 1\n",
      "2018-05-01T21:38:28.578377: step 2162, loss 0.000777304, acc 1\n",
      "2018-05-01T21:38:28.593651: step 2163, loss 0.00212429, acc 1\n",
      "2018-05-01T21:38:28.610724: step 2164, loss 0.00140247, acc 1\n",
      "2018-05-01T21:38:28.629271: step 2165, loss 0.0218736, acc 1\n",
      "2018-05-01T21:38:28.642765: step 2166, loss 0.169928, acc 0.96875\n",
      "2018-05-01T21:38:28.661532: step 2167, loss 0.107826, acc 0.96875\n",
      "2018-05-01T21:38:28.677725: step 2168, loss 0.00384087, acc 1\n",
      "2018-05-01T21:38:28.696824: step 2169, loss 0.000665401, acc 1\n",
      "2018-05-01T21:38:28.716962: step 2170, loss 0.0261458, acc 0.96875\n",
      "2018-05-01T21:38:28.733803: step 2171, loss 0.00197322, acc 1\n",
      "2018-05-01T21:38:28.754842: step 2172, loss 0.00806312, acc 1\n",
      "2018-05-01T21:38:28.774014: step 2173, loss 0.00404275, acc 1\n",
      "2018-05-01T21:38:28.793452: step 2174, loss 0.000342242, acc 1\n",
      "2018-05-01T21:38:28.815362: step 2175, loss 0.00373608, acc 1\n",
      "2018-05-01T21:38:28.845064: step 2176, loss 0.0313236, acc 0.96875\n",
      "2018-05-01T21:38:28.867554: step 2177, loss 0.000472294, acc 1\n",
      "2018-05-01T21:38:28.889138: step 2178, loss 0.00922668, acc 1\n",
      "2018-05-01T21:38:28.909084: step 2179, loss 0.0949989, acc 0.96875\n",
      "2018-05-01T21:38:28.931021: step 2180, loss 0.133601, acc 0.9375\n",
      "2018-05-01T21:38:28.945773: step 2181, loss 0.00254462, acc 1\n",
      "2018-05-01T21:38:28.959871: step 2182, loss 0.00851231, acc 1\n",
      "2018-05-01T21:38:28.974801: step 2183, loss 0.00177182, acc 1\n",
      "2018-05-01T21:38:28.990437: step 2184, loss 0.00221366, acc 1\n",
      "2018-05-01T21:38:29.012479: step 2185, loss 0.0432569, acc 0.96875\n",
      "2018-05-01T21:38:29.027265: step 2186, loss 0.220961, acc 0.9375\n",
      "2018-05-01T21:38:29.041407: step 2187, loss 0.0026843, acc 1\n",
      "2018-05-01T21:38:29.054279: step 2188, loss 0.000179156, acc 1\n",
      "2018-05-01T21:38:29.084354: step 2189, loss 0.00497326, acc 1\n",
      "2018-05-01T21:38:29.113398: step 2190, loss 0.000272891, acc 1\n",
      "2018-05-01T21:38:29.143534: step 2191, loss 0.0310867, acc 0.96875\n",
      "2018-05-01T21:38:29.168199: step 2192, loss 0.00185366, acc 1\n",
      "2018-05-01T21:38:29.197320: step 2193, loss 0.000328066, acc 1\n",
      "2018-05-01T21:38:29.215677: step 2194, loss 0.00382218, acc 1\n",
      "2018-05-01T21:38:29.233538: step 2195, loss 0.000525918, acc 1\n",
      "2018-05-01T21:38:29.249582: step 2196, loss 0.00152647, acc 1\n",
      "2018-05-01T21:38:29.275400: step 2197, loss 0.00233629, acc 1\n",
      "2018-05-01T21:38:29.293891: step 2198, loss 0.00372542, acc 1\n",
      "2018-05-01T21:38:29.309932: step 2199, loss 0.00344134, acc 1\n",
      "2018-05-01T21:38:29.325538: step 2200, loss 0.00175734, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:29.333281: step 2200, loss 0.61546, acc 0.827586\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-2200\n",
      "\n",
      "2018-05-01T21:38:29.492341: step 2201, loss 0.00163708, acc 1\n",
      "2018-05-01T21:38:29.507616: step 2202, loss 0.000872137, acc 1\n",
      "2018-05-01T21:38:29.525546: step 2203, loss 0.0148056, acc 1\n",
      "2018-05-01T21:38:29.542207: step 2204, loss 0.000416637, acc 1\n",
      "2018-05-01T21:38:29.559069: step 2205, loss 0.00300177, acc 1\n",
      "2018-05-01T21:38:29.572933: step 2206, loss 0.0110438, acc 1\n",
      "2018-05-01T21:38:29.586018: step 2207, loss 0.00055667, acc 1\n",
      "2018-05-01T21:38:29.600090: step 2208, loss 0.0572879, acc 0.96875\n",
      "2018-05-01T21:38:29.616503: step 2209, loss 0.00462185, acc 1\n",
      "2018-05-01T21:38:29.631990: step 2210, loss 0.0280669, acc 0.96875\n",
      "2018-05-01T21:38:29.644081: step 2211, loss 0.000310525, acc 1\n",
      "2018-05-01T21:38:29.659640: step 2212, loss 0.00100767, acc 1\n",
      "2018-05-01T21:38:29.674191: step 2213, loss 0.00425625, acc 1\n",
      "2018-05-01T21:38:29.688773: step 2214, loss 0.00473613, acc 1\n",
      "2018-05-01T21:38:29.705717: step 2215, loss 0.0888867, acc 0.96875\n",
      "2018-05-01T21:38:29.720288: step 2216, loss 0.00228272, acc 1\n",
      "2018-05-01T21:38:29.735557: step 2217, loss 0.00320896, acc 1\n",
      "2018-05-01T21:38:29.749513: step 2218, loss 0.00487579, acc 1\n",
      "2018-05-01T21:38:29.764009: step 2219, loss 0.00151417, acc 1\n",
      "2018-05-01T21:38:29.779664: step 2220, loss 0.0456278, acc 0.96875\n",
      "2018-05-01T21:38:29.793712: step 2221, loss 0.0659207, acc 0.96875\n",
      "2018-05-01T21:38:29.816987: step 2222, loss 0.0249208, acc 1\n",
      "2018-05-01T21:38:29.832143: step 2223, loss 0.13241, acc 0.9375\n",
      "2018-05-01T21:38:29.845492: step 2224, loss 0.00461679, acc 1\n",
      "2018-05-01T21:38:29.858326: step 2225, loss 0.00459062, acc 1\n",
      "2018-05-01T21:38:29.873761: step 2226, loss 0.0286177, acc 0.96875\n",
      "2018-05-01T21:38:29.888701: step 2227, loss 0.00223322, acc 1\n",
      "2018-05-01T21:38:29.903257: step 2228, loss 0.00470655, acc 1\n",
      "2018-05-01T21:38:29.920658: step 2229, loss 0.0028434, acc 1\n",
      "2018-05-01T21:38:29.934561: step 2230, loss 0.00295136, acc 1\n",
      "2018-05-01T21:38:29.949661: step 2231, loss 0.0176306, acc 1\n",
      "2018-05-01T21:38:29.964632: step 2232, loss 0.0013524, acc 1\n",
      "2018-05-01T21:38:29.977832: step 2233, loss 0.0230833, acc 1\n",
      "2018-05-01T21:38:29.991150: step 2234, loss 0.000298073, acc 1\n",
      "2018-05-01T21:38:30.005404: step 2235, loss 0.00563189, acc 1\n",
      "2018-05-01T21:38:30.018851: step 2236, loss 0.00455009, acc 1\n",
      "2018-05-01T21:38:30.032707: step 2237, loss 0.00189504, acc 1\n",
      "2018-05-01T21:38:30.045910: step 2238, loss 0.00149691, acc 1\n",
      "2018-05-01T21:38:30.059227: step 2239, loss 0.000636158, acc 1\n",
      "2018-05-01T21:38:30.074565: step 2240, loss 0.0764714, acc 0.96875\n",
      "2018-05-01T21:38:30.087928: step 2241, loss 0.133848, acc 0.96875\n",
      "2018-05-01T21:38:30.101909: step 2242, loss 0.00322537, acc 1\n",
      "2018-05-01T21:38:30.117756: step 2243, loss 0.00416837, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:30.133271: step 2244, loss 0.0811986, acc 0.96875\n",
      "2018-05-01T21:38:30.148845: step 2245, loss 0.000845425, acc 1\n",
      "2018-05-01T21:38:30.161813: step 2246, loss 0.0021834, acc 1\n",
      "2018-05-01T21:38:30.176963: step 2247, loss 0.0921326, acc 0.96875\n",
      "2018-05-01T21:38:30.191866: step 2248, loss 0.00536481, acc 1\n",
      "2018-05-01T21:38:30.205575: step 2249, loss 0.0121508, acc 1\n",
      "2018-05-01T21:38:30.220933: step 2250, loss 0.00304299, acc 1\n",
      "2018-05-01T21:38:30.236325: step 2251, loss 0.0010403, acc 1\n",
      "2018-05-01T21:38:30.252776: step 2252, loss 0.00131453, acc 1\n",
      "2018-05-01T21:38:30.268824: step 2253, loss 0.0602259, acc 0.96875\n",
      "2018-05-01T21:38:30.285851: step 2254, loss 0.00561967, acc 1\n",
      "2018-05-01T21:38:30.300026: step 2255, loss 0.00752909, acc 1\n",
      "2018-05-01T21:38:30.315233: step 2256, loss 0.0875275, acc 0.96875\n",
      "2018-05-01T21:38:30.328241: step 2257, loss 0.0804169, acc 0.96875\n",
      "2018-05-01T21:38:30.342063: step 2258, loss 0.000247868, acc 1\n",
      "2018-05-01T21:38:30.357550: step 2259, loss 0.109384, acc 0.96875\n",
      "2018-05-01T21:38:30.372093: step 2260, loss 0.0207743, acc 1\n",
      "2018-05-01T21:38:30.387326: step 2261, loss 0.00245599, acc 1\n",
      "2018-05-01T21:38:30.402703: step 2262, loss 0.00159451, acc 1\n",
      "2018-05-01T21:38:30.417953: step 2263, loss 0.000353415, acc 1\n",
      "2018-05-01T21:38:30.432145: step 2264, loss 0.00328777, acc 1\n",
      "2018-05-01T21:38:30.445520: step 2265, loss 0.00226804, acc 1\n",
      "2018-05-01T21:38:30.459383: step 2266, loss 0.000581091, acc 1\n",
      "2018-05-01T21:38:30.473131: step 2267, loss 0.000456696, acc 1\n",
      "2018-05-01T21:38:30.487745: step 2268, loss 0.0634598, acc 0.96875\n",
      "2018-05-01T21:38:30.503693: step 2269, loss 0.00602845, acc 1\n",
      "2018-05-01T21:38:30.518755: step 2270, loss 0.00719272, acc 1\n",
      "2018-05-01T21:38:30.535067: step 2271, loss 0.00892085, acc 1\n",
      "2018-05-01T21:38:30.552289: step 2272, loss 0.00249083, acc 1\n",
      "2018-05-01T21:38:30.567441: step 2273, loss 0.0168391, acc 1\n",
      "2018-05-01T21:38:30.581540: step 2274, loss 0.00101851, acc 1\n",
      "2018-05-01T21:38:30.594031: step 2275, loss 0.000196788, acc 1\n",
      "2018-05-01T21:38:30.607860: step 2276, loss 0.0760015, acc 0.96875\n",
      "2018-05-01T21:38:30.621107: step 2277, loss 0.0874023, acc 0.96875\n",
      "2018-05-01T21:38:30.637519: step 2278, loss 0.00336191, acc 1\n",
      "2018-05-01T21:38:30.651791: step 2279, loss 0.0243199, acc 1\n",
      "2018-05-01T21:38:30.667952: step 2280, loss 0.00453688, acc 1\n",
      "2018-05-01T21:38:30.683003: step 2281, loss 0.00489734, acc 1\n",
      "2018-05-01T21:38:30.698186: step 2282, loss 0.000298679, acc 1\n",
      "2018-05-01T21:38:30.710879: step 2283, loss 0.00231428, acc 1\n",
      "2018-05-01T21:38:30.724248: step 2284, loss 0.0100637, acc 1\n",
      "2018-05-01T21:38:30.738300: step 2285, loss 0.0547125, acc 0.96875\n",
      "2018-05-01T21:38:30.752682: step 2286, loss 0.00185126, acc 1\n",
      "2018-05-01T21:38:30.768358: step 2287, loss 0.00677925, acc 1\n",
      "2018-05-01T21:38:30.782989: step 2288, loss 0.00351918, acc 1\n",
      "2018-05-01T21:38:30.798152: step 2289, loss 0.00130606, acc 1\n",
      "2018-05-01T21:38:30.812279: step 2290, loss 0.0298174, acc 0.96875\n",
      "2018-05-01T21:38:30.825070: step 2291, loss 0.00309743, acc 1\n",
      "2018-05-01T21:38:30.840950: step 2292, loss 0.000711737, acc 1\n",
      "2018-05-01T21:38:30.856299: step 2293, loss 0.176199, acc 0.96875\n",
      "2018-05-01T21:38:30.872469: step 2294, loss 0.00113453, acc 1\n",
      "2018-05-01T21:38:30.888756: step 2295, loss 0.055492, acc 0.96875\n",
      "2018-05-01T21:38:30.904176: step 2296, loss 0.00141964, acc 1\n",
      "2018-05-01T21:38:30.919169: step 2297, loss 0.110849, acc 0.96875\n",
      "2018-05-01T21:38:30.932616: step 2298, loss 0.00676204, acc 1\n",
      "2018-05-01T21:38:30.947552: step 2299, loss 0.000811164, acc 1\n",
      "2018-05-01T21:38:30.962080: step 2300, loss 0.0063479, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:30.966589: step 2300, loss 0.591056, acc 0.83908\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-2300\n",
      "\n",
      "2018-05-01T21:38:31.037508: step 2301, loss 0.00421179, acc 1\n",
      "2018-05-01T21:38:31.054427: step 2302, loss 0.0549329, acc 0.96875\n",
      "2018-05-01T21:38:31.077705: step 2303, loss 0.0122075, acc 1\n",
      "2018-05-01T21:38:31.095895: step 2304, loss 0.00774929, acc 1\n",
      "2018-05-01T21:38:31.109500: step 2305, loss 0.0255753, acc 1\n",
      "2018-05-01T21:38:31.127194: step 2306, loss 0.010373, acc 1\n",
      "2018-05-01T21:38:31.144967: step 2307, loss 0.00758157, acc 1\n",
      "2018-05-01T21:38:31.160645: step 2308, loss 0.0247222, acc 1\n",
      "2018-05-01T21:38:31.176979: step 2309, loss 0.00204609, acc 1\n",
      "2018-05-01T21:38:31.200023: step 2310, loss 0.00131673, acc 1\n",
      "2018-05-01T21:38:31.221799: step 2311, loss 0.00318262, acc 1\n",
      "2018-05-01T21:38:31.238756: step 2312, loss 0.013834, acc 1\n",
      "2018-05-01T21:38:31.313357: step 2313, loss 0.00944179, acc 1\n",
      "2018-05-01T21:38:31.338741: step 2314, loss 0.191718, acc 0.9375\n",
      "2018-05-01T21:38:31.356272: step 2315, loss 0.000675717, acc 1\n",
      "2018-05-01T21:38:31.377967: step 2316, loss 0.00164169, acc 1\n",
      "2018-05-01T21:38:31.406053: step 2317, loss 0.000919632, acc 1\n",
      "2018-05-01T21:38:31.425182: step 2318, loss 0.0263761, acc 0.96875\n",
      "2018-05-01T21:38:31.449087: step 2319, loss 0.00222395, acc 1\n",
      "2018-05-01T21:38:31.469295: step 2320, loss 0.00533837, acc 1\n",
      "2018-05-01T21:38:31.485383: step 2321, loss 0.0640638, acc 0.96875\n",
      "2018-05-01T21:38:31.498909: step 2322, loss 0.00425068, acc 1\n",
      "2018-05-01T21:38:31.512930: step 2323, loss 0.00027802, acc 1\n",
      "2018-05-01T21:38:31.526817: step 2324, loss 0.0641205, acc 0.96875\n",
      "2018-05-01T21:38:31.544236: step 2325, loss 0.00686852, acc 1\n",
      "2018-05-01T21:38:31.560741: step 2326, loss 0.011374, acc 1\n",
      "2018-05-01T21:38:31.575185: step 2327, loss 0.00230962, acc 1\n",
      "2018-05-01T21:38:31.589294: step 2328, loss 0.00725695, acc 1\n",
      "2018-05-01T21:38:31.605813: step 2329, loss 0.0249204, acc 0.96875\n",
      "2018-05-01T21:38:31.633425: step 2330, loss 0.00386403, acc 1\n",
      "2018-05-01T21:38:31.654500: step 2331, loss 0.000657278, acc 1\n",
      "2018-05-01T21:38:31.685631: step 2332, loss 0.0429735, acc 0.96875\n",
      "2018-05-01T21:38:31.709879: step 2333, loss 0.0039019, acc 1\n",
      "2018-05-01T21:38:31.737283: step 2334, loss 0.0199375, acc 1\n",
      "2018-05-01T21:38:31.753988: step 2335, loss 0.00371226, acc 1\n",
      "2018-05-01T21:38:31.774576: step 2336, loss 0.00515608, acc 1\n",
      "2018-05-01T21:38:31.792098: step 2337, loss 0.17984, acc 0.96875\n",
      "2018-05-01T21:38:31.811490: step 2338, loss 0.017932, acc 1\n",
      "2018-05-01T21:38:31.838517: step 2339, loss 0.0188265, acc 1\n",
      "2018-05-01T21:38:31.855707: step 2340, loss 0.00486928, acc 1\n",
      "2018-05-01T21:38:31.876239: step 2341, loss 0.00980476, acc 1\n",
      "2018-05-01T21:38:31.889889: step 2342, loss 0.0218472, acc 1\n",
      "2018-05-01T21:38:31.907184: step 2343, loss 0.0157129, acc 1\n",
      "2018-05-01T21:38:31.924903: step 2344, loss 0.0180095, acc 1\n",
      "2018-05-01T21:38:31.940129: step 2345, loss 0.00241726, acc 1\n",
      "2018-05-01T21:38:31.956742: step 2346, loss 0.245294, acc 0.96875\n",
      "2018-05-01T21:38:31.983210: step 2347, loss 0.0439567, acc 0.96875\n",
      "2018-05-01T21:38:31.997685: step 2348, loss 0.0948357, acc 0.96875\n",
      "2018-05-01T21:38:32.012416: step 2349, loss 0.00056457, acc 1\n",
      "2018-05-01T21:38:32.026466: step 2350, loss 0.278946, acc 0.944444\n",
      "2018-05-01T21:38:32.042093: step 2351, loss 0.0734029, acc 0.96875\n",
      "2018-05-01T21:38:32.057012: step 2352, loss 0.00198327, acc 1\n",
      "2018-05-01T21:38:32.072754: step 2353, loss 0.0230898, acc 0.96875\n",
      "2018-05-01T21:38:32.087316: step 2354, loss 0.00160156, acc 1\n",
      "2018-05-01T21:38:32.100383: step 2355, loss 0.00224057, acc 1\n",
      "2018-05-01T21:38:32.116548: step 2356, loss 0.0130665, acc 1\n",
      "2018-05-01T21:38:32.130661: step 2357, loss 0.0027776, acc 1\n",
      "2018-05-01T21:38:32.145859: step 2358, loss 0.0019921, acc 1\n",
      "2018-05-01T21:38:32.158121: step 2359, loss 0.00045158, acc 1\n",
      "2018-05-01T21:38:32.175329: step 2360, loss 0.0252259, acc 0.96875\n",
      "2018-05-01T21:38:32.191935: step 2361, loss 0.0120168, acc 1\n",
      "2018-05-01T21:38:32.206156: step 2362, loss 0.00124258, acc 1\n",
      "2018-05-01T21:38:32.221801: step 2363, loss 0.013031, acc 1\n",
      "2018-05-01T21:38:32.238779: step 2364, loss 0.11061, acc 0.9375\n",
      "2018-05-01T21:38:32.255192: step 2365, loss 0.0407308, acc 0.96875\n",
      "2018-05-01T21:38:32.271114: step 2366, loss 0.00809853, acc 1\n",
      "2018-05-01T21:38:32.293707: step 2367, loss 0.0112195, acc 1\n",
      "2018-05-01T21:38:32.308750: step 2368, loss 0.000927485, acc 1\n",
      "2018-05-01T21:38:32.323038: step 2369, loss 0.0207848, acc 1\n",
      "2018-05-01T21:38:32.338497: step 2370, loss 0.0387146, acc 0.96875\n",
      "2018-05-01T21:38:32.354906: step 2371, loss 0.00651674, acc 1\n",
      "2018-05-01T21:38:32.371101: step 2372, loss 0.00867833, acc 1\n",
      "2018-05-01T21:38:32.386168: step 2373, loss 0.000819355, acc 1\n",
      "2018-05-01T21:38:32.400094: step 2374, loss 0.0025395, acc 1\n",
      "2018-05-01T21:38:32.412817: step 2375, loss 0.00181806, acc 1\n",
      "2018-05-01T21:38:32.426426: step 2376, loss 0.00152261, acc 1\n",
      "2018-05-01T21:38:32.442056: step 2377, loss 0.0212346, acc 1\n",
      "2018-05-01T21:38:32.458984: step 2378, loss 0.0171176, acc 1\n",
      "2018-05-01T21:38:32.475232: step 2379, loss 0.00204581, acc 1\n",
      "2018-05-01T21:38:32.494504: step 2380, loss 0.0105451, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:32.517314: step 2381, loss 0.000597383, acc 1\n",
      "2018-05-01T21:38:32.536150: step 2382, loss 0.000415711, acc 1\n",
      "2018-05-01T21:38:32.552468: step 2383, loss 0.0413683, acc 0.96875\n",
      "2018-05-01T21:38:32.578884: step 2384, loss 0.00974544, acc 1\n",
      "2018-05-01T21:38:32.597808: step 2385, loss 0.00646124, acc 1\n",
      "2018-05-01T21:38:32.619102: step 2386, loss 0.00133214, acc 1\n",
      "2018-05-01T21:38:32.638136: step 2387, loss 0.001686, acc 1\n",
      "2018-05-01T21:38:32.655627: step 2388, loss 0.0118298, acc 1\n",
      "2018-05-01T21:38:32.673237: step 2389, loss 0.00428144, acc 1\n",
      "2018-05-01T21:38:32.692606: step 2390, loss 0.00696087, acc 1\n",
      "2018-05-01T21:38:32.707766: step 2391, loss 0.0415494, acc 0.96875\n",
      "2018-05-01T21:38:32.728929: step 2392, loss 0.00868015, acc 1\n",
      "2018-05-01T21:38:32.754847: step 2393, loss 0.0106472, acc 1\n",
      "2018-05-01T21:38:32.777019: step 2394, loss 0.00275269, acc 1\n",
      "2018-05-01T21:38:32.795781: step 2395, loss 0.00532999, acc 1\n",
      "2018-05-01T21:38:32.813455: step 2396, loss 0.00206787, acc 1\n",
      "2018-05-01T21:38:32.829486: step 2397, loss 0.132626, acc 0.96875\n",
      "2018-05-01T21:38:32.847894: step 2398, loss 0.00566397, acc 1\n",
      "2018-05-01T21:38:32.866941: step 2399, loss 0.00927732, acc 1\n",
      "2018-05-01T21:38:32.882243: step 2400, loss 0.121095, acc 0.944444\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:32.887917: step 2400, loss 0.610086, acc 0.804598\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-2400\n",
      "\n",
      "2018-05-01T21:38:32.973823: step 2401, loss 0.00173082, acc 1\n",
      "2018-05-01T21:38:32.990314: step 2402, loss 0.00385886, acc 1\n",
      "2018-05-01T21:38:33.009231: step 2403, loss 0.0134703, acc 1\n",
      "2018-05-01T21:38:33.026215: step 2404, loss 0.00158942, acc 1\n",
      "2018-05-01T21:38:33.043733: step 2405, loss 0.136144, acc 0.9375\n",
      "2018-05-01T21:38:33.060509: step 2406, loss 0.0851751, acc 0.96875\n",
      "2018-05-01T21:38:33.074833: step 2407, loss 0.00429252, acc 1\n",
      "2018-05-01T21:38:33.090036: step 2408, loss 0.0193754, acc 1\n",
      "2018-05-01T21:38:33.105277: step 2409, loss 0.00994743, acc 1\n",
      "2018-05-01T21:38:33.119331: step 2410, loss 0.130124, acc 0.96875\n",
      "2018-05-01T21:38:33.132720: step 2411, loss 0.00981537, acc 1\n",
      "2018-05-01T21:38:33.147019: step 2412, loss 0.00132784, acc 1\n",
      "2018-05-01T21:38:33.161961: step 2413, loss 0.00173781, acc 1\n",
      "2018-05-01T21:38:33.178003: step 2414, loss 0.0101892, acc 1\n",
      "2018-05-01T21:38:33.197094: step 2415, loss 0.0711772, acc 0.96875\n",
      "2018-05-01T21:38:33.212197: step 2416, loss 0.000525633, acc 1\n",
      "2018-05-01T21:38:33.226227: step 2417, loss 3.45668e-05, acc 1\n",
      "2018-05-01T21:38:33.242363: step 2418, loss 0.00218201, acc 1\n",
      "2018-05-01T21:38:33.257315: step 2419, loss 0.00196794, acc 1\n",
      "2018-05-01T21:38:33.270751: step 2420, loss 0.00302323, acc 1\n",
      "2018-05-01T21:38:33.286484: step 2421, loss 0.00637355, acc 1\n",
      "2018-05-01T21:38:33.300825: step 2422, loss 0.00384119, acc 1\n",
      "2018-05-01T21:38:33.314618: step 2423, loss 0.000481959, acc 1\n",
      "2018-05-01T21:38:33.329061: step 2424, loss 0.0422316, acc 0.96875\n",
      "2018-05-01T21:38:33.343483: step 2425, loss 0.000467037, acc 1\n",
      "2018-05-01T21:38:33.358406: step 2426, loss 0.00188025, acc 1\n",
      "2018-05-01T21:38:33.374123: step 2427, loss 0.00278399, acc 1\n",
      "2018-05-01T21:38:33.387473: step 2428, loss 0.0101354, acc 1\n",
      "2018-05-01T21:38:33.399958: step 2429, loss 0.00643925, acc 1\n",
      "2018-05-01T21:38:33.414757: step 2430, loss 0.00249977, acc 1\n",
      "2018-05-01T21:38:33.429149: step 2431, loss 0.00155211, acc 1\n",
      "2018-05-01T21:38:33.444261: step 2432, loss 0.00433029, acc 1\n",
      "2018-05-01T21:38:33.459631: step 2433, loss 0.00121228, acc 1\n",
      "2018-05-01T21:38:33.475422: step 2434, loss 0.0100569, acc 1\n",
      "2018-05-01T21:38:33.492386: step 2435, loss 0.00578557, acc 1\n",
      "2018-05-01T21:38:33.510701: step 2436, loss 0.11188, acc 0.96875\n",
      "2018-05-01T21:38:33.528014: step 2437, loss 3.10171e-05, acc 1\n",
      "2018-05-01T21:38:33.542483: step 2438, loss 0.0136198, acc 1\n",
      "2018-05-01T21:38:33.558357: step 2439, loss 0.0127066, acc 1\n",
      "2018-05-01T21:38:33.574711: step 2440, loss 0.134718, acc 0.96875\n",
      "2018-05-01T21:38:33.589050: step 2441, loss 0.00557964, acc 1\n",
      "2018-05-01T21:38:33.604071: step 2442, loss 0.00241027, acc 1\n",
      "2018-05-01T21:38:33.617659: step 2443, loss 0.00290941, acc 1\n",
      "2018-05-01T21:38:33.630957: step 2444, loss 0.0259202, acc 1\n",
      "2018-05-01T21:38:33.647449: step 2445, loss 0.00222678, acc 1\n",
      "2018-05-01T21:38:33.664883: step 2446, loss 0.25698, acc 0.9375\n",
      "2018-05-01T21:38:33.684671: step 2447, loss 0.00217124, acc 1\n",
      "2018-05-01T21:38:33.704072: step 2448, loss 0.000961484, acc 1\n",
      "2018-05-01T21:38:33.724377: step 2449, loss 0.0155822, acc 1\n",
      "2018-05-01T21:38:33.741607: step 2450, loss 0.00438008, acc 1\n",
      "2018-05-01T21:38:33.764240: step 2451, loss 0.0747385, acc 0.96875\n",
      "2018-05-01T21:38:33.783436: step 2452, loss 0.00371204, acc 1\n",
      "2018-05-01T21:38:33.840540: step 2453, loss 0.00682138, acc 1\n",
      "2018-05-01T21:38:33.855595: step 2454, loss 0.000806172, acc 1\n",
      "2018-05-01T21:38:33.873740: step 2455, loss 0.00176516, acc 1\n",
      "2018-05-01T21:38:33.890800: step 2456, loss 0.000445298, acc 1\n",
      "2018-05-01T21:38:33.914841: step 2457, loss 0.0516681, acc 0.96875\n",
      "2018-05-01T21:38:33.930074: step 2458, loss 0.00094069, acc 1\n",
      "2018-05-01T21:38:33.950353: step 2459, loss 0.00127797, acc 1\n",
      "2018-05-01T21:38:33.968138: step 2460, loss 0.00301055, acc 1\n",
      "2018-05-01T21:38:33.985921: step 2461, loss 0.0108761, acc 1\n",
      "2018-05-01T21:38:34.003642: step 2462, loss 0.0959705, acc 0.96875\n",
      "2018-05-01T21:38:34.018600: step 2463, loss 0.00567573, acc 1\n",
      "2018-05-01T21:38:34.033744: step 2464, loss 0.023474, acc 0.96875\n",
      "2018-05-01T21:38:34.048448: step 2465, loss 0.00557946, acc 1\n",
      "2018-05-01T21:38:34.065515: step 2466, loss 0.0341443, acc 0.96875\n",
      "2018-05-01T21:38:34.084112: step 2467, loss 0.000609465, acc 1\n",
      "2018-05-01T21:38:34.102440: step 2468, loss 0.00104516, acc 1\n",
      "2018-05-01T21:38:34.119919: step 2469, loss 0.177572, acc 0.96875\n",
      "2018-05-01T21:38:34.135823: step 2470, loss 0.00396487, acc 1\n",
      "2018-05-01T21:38:34.150357: step 2471, loss 0.163678, acc 0.96875\n",
      "2018-05-01T21:38:34.166067: step 2472, loss 0.000593714, acc 1\n",
      "2018-05-01T21:38:34.180460: step 2473, loss 0.000362957, acc 1\n",
      "2018-05-01T21:38:34.196059: step 2474, loss 0.0261, acc 0.96875\n",
      "2018-05-01T21:38:34.209239: step 2475, loss 0.0232975, acc 1\n",
      "2018-05-01T21:38:34.224530: step 2476, loss 0.0014255, acc 1\n",
      "2018-05-01T21:38:34.241533: step 2477, loss 0.0169641, acc 1\n",
      "2018-05-01T21:38:34.259864: step 2478, loss 0.000197341, acc 1\n",
      "2018-05-01T21:38:34.273364: step 2479, loss 0.000727933, acc 1\n",
      "2018-05-01T21:38:34.289812: step 2480, loss 0.0129947, acc 1\n",
      "2018-05-01T21:38:34.305907: step 2481, loss 0.00781085, acc 1\n",
      "2018-05-01T21:38:34.320205: step 2482, loss 0.0041841, acc 1\n",
      "2018-05-01T21:38:34.335922: step 2483, loss 0.00233771, acc 1\n",
      "2018-05-01T21:38:34.351647: step 2484, loss 0.00649991, acc 1\n",
      "2018-05-01T21:38:34.367229: step 2485, loss 0.0241242, acc 1\n",
      "2018-05-01T21:38:34.381307: step 2486, loss 0.0110017, acc 1\n",
      "2018-05-01T21:38:34.395668: step 2487, loss 0.00185115, acc 1\n",
      "2018-05-01T21:38:34.411048: step 2488, loss 0.00105012, acc 1\n",
      "2018-05-01T21:38:34.426956: step 2489, loss 0.0018745, acc 1\n",
      "2018-05-01T21:38:34.440477: step 2490, loss 0.111117, acc 0.96875\n",
      "2018-05-01T21:38:34.457234: step 2491, loss 0.00127882, acc 1\n",
      "2018-05-01T21:38:34.474924: step 2492, loss 0.000591002, acc 1\n",
      "2018-05-01T21:38:34.517647: step 2493, loss 0.10804, acc 0.9375\n",
      "2018-05-01T21:38:34.541752: step 2494, loss 0.00736377, acc 1\n",
      "2018-05-01T21:38:34.557893: step 2495, loss 0.00921189, acc 1\n",
      "2018-05-01T21:38:34.576238: step 2496, loss 0.0258824, acc 1\n",
      "2018-05-01T21:38:34.590017: step 2497, loss 0.00418889, acc 1\n",
      "2018-05-01T21:38:34.605211: step 2498, loss 0.00108953, acc 1\n",
      "2018-05-01T21:38:34.621739: step 2499, loss 0.202043, acc 0.96875\n",
      "2018-05-01T21:38:34.636507: step 2500, loss 0.00661647, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:34.640579: step 2500, loss 0.624215, acc 0.827586\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-2500\n",
      "\n",
      "2018-05-01T21:38:34.713547: step 2501, loss 0.00188132, acc 1\n",
      "2018-05-01T21:38:34.728854: step 2502, loss 0.00104768, acc 1\n",
      "2018-05-01T21:38:34.747473: step 2503, loss 0.00154363, acc 1\n",
      "2018-05-01T21:38:34.765639: step 2504, loss 0.00116403, acc 1\n",
      "2018-05-01T21:38:34.793684: step 2505, loss 0.00369774, acc 1\n",
      "2018-05-01T21:38:34.811071: step 2506, loss 0.00166679, acc 1\n",
      "2018-05-01T21:38:34.829972: step 2507, loss 0.150497, acc 0.96875\n",
      "2018-05-01T21:38:34.847276: step 2508, loss 0.0567457, acc 0.96875\n",
      "2018-05-01T21:38:34.867309: step 2509, loss 0.00375163, acc 1\n",
      "2018-05-01T21:38:34.896589: step 2510, loss 0.0598407, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:34.912518: step 2511, loss 0.00105663, acc 1\n",
      "2018-05-01T21:38:34.929805: step 2512, loss 0.0249122, acc 0.96875\n",
      "2018-05-01T21:38:34.947663: step 2513, loss 0.00693212, acc 1\n",
      "2018-05-01T21:38:34.971320: step 2514, loss 0.0146433, acc 1\n",
      "2018-05-01T21:38:34.985722: step 2515, loss 0.00255138, acc 1\n",
      "2018-05-01T21:38:35.015897: step 2516, loss 0.00340511, acc 1\n",
      "2018-05-01T21:38:35.184500: step 2517, loss 0.00599563, acc 1\n",
      "2018-05-01T21:38:35.206773: step 2518, loss 0.00565152, acc 1\n",
      "2018-05-01T21:38:35.226029: step 2519, loss 0.000852103, acc 1\n",
      "2018-05-01T21:38:35.247843: step 2520, loss 0.0119098, acc 1\n",
      "2018-05-01T21:38:35.268404: step 2521, loss 0.00115745, acc 1\n",
      "2018-05-01T21:38:35.283308: step 2522, loss 0.000404813, acc 1\n",
      "2018-05-01T21:38:35.299246: step 2523, loss 0.177516, acc 0.9375\n",
      "2018-05-01T21:38:35.314972: step 2524, loss 0.0120272, acc 1\n",
      "2018-05-01T21:38:35.329261: step 2525, loss 0.00597225, acc 1\n",
      "2018-05-01T21:38:35.343694: step 2526, loss 0.00780217, acc 1\n",
      "2018-05-01T21:38:35.358978: step 2527, loss 0.0384765, acc 1\n",
      "2018-05-01T21:38:35.375233: step 2528, loss 0.00061379, acc 1\n",
      "2018-05-01T21:38:35.391010: step 2529, loss 0.00126953, acc 1\n",
      "2018-05-01T21:38:35.407279: step 2530, loss 0.000737055, acc 1\n",
      "2018-05-01T21:38:35.422925: step 2531, loss 0.000778237, acc 1\n",
      "2018-05-01T21:38:35.440138: step 2532, loss 0.0485802, acc 0.96875\n",
      "2018-05-01T21:38:35.456180: step 2533, loss 0.00180894, acc 1\n",
      "2018-05-01T21:38:35.473780: step 2534, loss 0.0387155, acc 0.96875\n",
      "2018-05-01T21:38:35.490675: step 2535, loss 0.0383143, acc 0.96875\n",
      "2018-05-01T21:38:35.508672: step 2536, loss 0.160729, acc 0.96875\n",
      "2018-05-01T21:38:35.524258: step 2537, loss 0.0940469, acc 0.96875\n",
      "2018-05-01T21:38:35.540480: step 2538, loss 0.0117471, acc 1\n",
      "2018-05-01T21:38:35.557454: step 2539, loss 0.00214464, acc 1\n",
      "2018-05-01T21:38:35.573189: step 2540, loss 0.00419019, acc 1\n",
      "2018-05-01T21:38:35.589229: step 2541, loss 0.000455486, acc 1\n",
      "2018-05-01T21:38:35.606051: step 2542, loss 0.001169, acc 1\n",
      "2018-05-01T21:38:35.623113: step 2543, loss 0.00203767, acc 1\n",
      "2018-05-01T21:38:35.637613: step 2544, loss 0.00201196, acc 1\n",
      "2018-05-01T21:38:35.651184: step 2545, loss 0.0808524, acc 0.96875\n",
      "2018-05-01T21:38:35.667736: step 2546, loss 0.0036141, acc 1\n",
      "2018-05-01T21:38:35.681317: step 2547, loss 0.00673872, acc 1\n",
      "2018-05-01T21:38:35.695147: step 2548, loss 0.247847, acc 0.96875\n",
      "2018-05-01T21:38:35.710380: step 2549, loss 0.000754023, acc 1\n",
      "2018-05-01T21:38:35.728041: step 2550, loss 0.00227411, acc 1\n",
      "2018-05-01T21:38:35.747383: step 2551, loss 0.00105195, acc 1\n",
      "2018-05-01T21:38:35.761428: step 2552, loss 0.0195062, acc 1\n",
      "2018-05-01T21:38:35.775287: step 2553, loss 0.00470283, acc 1\n",
      "2018-05-01T21:38:35.791187: step 2554, loss 0.000913115, acc 1\n",
      "2018-05-01T21:38:35.816025: step 2555, loss 0.141696, acc 0.96875\n",
      "2018-05-01T21:38:35.844742: step 2556, loss 0.0034598, acc 1\n",
      "2018-05-01T21:38:35.879349: step 2557, loss 0.000116432, acc 1\n",
      "2018-05-01T21:38:35.906134: step 2558, loss 0.00422417, acc 1\n",
      "2018-05-01T21:38:35.920422: step 2559, loss 0.0191702, acc 1\n",
      "2018-05-01T21:38:35.939524: step 2560, loss 0.00896338, acc 1\n",
      "2018-05-01T21:38:35.959920: step 2561, loss 0.0550589, acc 0.96875\n",
      "2018-05-01T21:38:35.981847: step 2562, loss 0.00932523, acc 1\n",
      "2018-05-01T21:38:36.001489: step 2563, loss 0.0343251, acc 0.96875\n",
      "2018-05-01T21:38:36.023285: step 2564, loss 0.00321648, acc 1\n",
      "2018-05-01T21:38:36.041778: step 2565, loss 0.054727, acc 0.96875\n",
      "2018-05-01T21:38:36.059687: step 2566, loss 0.0207225, acc 1\n",
      "2018-05-01T21:38:36.076850: step 2567, loss 0.00189142, acc 1\n",
      "2018-05-01T21:38:36.094369: step 2568, loss 0.000614117, acc 1\n",
      "2018-05-01T21:38:36.111366: step 2569, loss 0.00317245, acc 1\n",
      "2018-05-01T21:38:36.133406: step 2570, loss 0.000863498, acc 1\n",
      "2018-05-01T21:38:36.252117: step 2571, loss 0.0037214, acc 1\n",
      "2018-05-01T21:38:36.267704: step 2572, loss 0.000689566, acc 1\n",
      "2018-05-01T21:38:36.284896: step 2573, loss 0.080581, acc 0.96875\n",
      "2018-05-01T21:38:36.306671: step 2574, loss 0.310252, acc 0.9375\n",
      "2018-05-01T21:38:36.336329: step 2575, loss 0.00585522, acc 1\n",
      "2018-05-01T21:38:36.356367: step 2576, loss 0.00105034, acc 1\n",
      "2018-05-01T21:38:36.381241: step 2577, loss 0.10003, acc 0.96875\n",
      "2018-05-01T21:38:36.400030: step 2578, loss 0.0108959, acc 1\n",
      "2018-05-01T21:38:36.426315: step 2579, loss 0.0118415, acc 1\n",
      "2018-05-01T21:38:36.445712: step 2580, loss 0.0011656, acc 1\n",
      "2018-05-01T21:38:36.461158: step 2581, loss 0.00200031, acc 1\n",
      "2018-05-01T21:38:36.475713: step 2582, loss 0.11567, acc 0.96875\n",
      "2018-05-01T21:38:36.492630: step 2583, loss 0.00027178, acc 1\n",
      "2018-05-01T21:38:36.509612: step 2584, loss 0.00678638, acc 1\n",
      "2018-05-01T21:38:36.523254: step 2585, loss 0.00516411, acc 1\n",
      "2018-05-01T21:38:36.539925: step 2586, loss 0.0151233, acc 1\n",
      "2018-05-01T21:38:36.558342: step 2587, loss 0.0103432, acc 1\n",
      "2018-05-01T21:38:36.579023: step 2588, loss 0.00938722, acc 1\n",
      "2018-05-01T21:38:36.679361: step 2589, loss 0.00446277, acc 1\n",
      "2018-05-01T21:38:36.702623: step 2590, loss 0.0243355, acc 1\n",
      "2018-05-01T21:38:36.725490: step 2591, loss 0.000856775, acc 1\n",
      "2018-05-01T21:38:36.747597: step 2592, loss 0.000434384, acc 1\n",
      "2018-05-01T21:38:36.775649: step 2593, loss 0.000553947, acc 1\n",
      "2018-05-01T21:38:36.807397: step 2594, loss 0.184295, acc 0.96875\n",
      "2018-05-01T21:38:36.827432: step 2595, loss 0.005693, acc 1\n",
      "2018-05-01T21:38:36.843564: step 2596, loss 0.00308965, acc 1\n",
      "2018-05-01T21:38:36.857008: step 2597, loss 0.0178626, acc 1\n",
      "2018-05-01T21:38:36.874294: step 2598, loss 0.00283894, acc 1\n",
      "2018-05-01T21:38:36.892347: step 2599, loss 0.00621072, acc 1\n",
      "2018-05-01T21:38:36.914881: step 2600, loss 0.00796333, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:36.926773: step 2600, loss 0.673336, acc 0.827586\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-2600\n",
      "\n",
      "2018-05-01T21:38:37.037947: step 2601, loss 0.00326578, acc 1\n",
      "2018-05-01T21:38:37.058586: step 2602, loss 0.00953281, acc 1\n",
      "2018-05-01T21:38:37.080001: step 2603, loss 0.0145186, acc 1\n",
      "2018-05-01T21:38:37.092960: step 2604, loss 0.0809058, acc 0.96875\n",
      "2018-05-01T21:38:37.109763: step 2605, loss 0.00725946, acc 1\n",
      "2018-05-01T21:38:37.127893: step 2606, loss 0.00616418, acc 1\n",
      "2018-05-01T21:38:37.142834: step 2607, loss 0.00123814, acc 1\n",
      "2018-05-01T21:38:37.158459: step 2608, loss 0.00107357, acc 1\n",
      "2018-05-01T21:38:37.188110: step 2609, loss 0.0141674, acc 1\n",
      "2018-05-01T21:38:37.205799: step 2610, loss 0.00136635, acc 1\n",
      "2018-05-01T21:38:37.221720: step 2611, loss 0.00239855, acc 1\n",
      "2018-05-01T21:38:37.236996: step 2612, loss 0.0164473, acc 1\n",
      "2018-05-01T21:38:37.253003: step 2613, loss 0.0646412, acc 0.96875\n",
      "2018-05-01T21:38:37.268873: step 2614, loss 0.00918465, acc 1\n",
      "2018-05-01T21:38:37.284046: step 2615, loss 0.058576, acc 0.96875\n",
      "2018-05-01T21:38:37.297934: step 2616, loss 0.034836, acc 0.96875\n",
      "2018-05-01T21:38:37.311554: step 2617, loss 0.000336935, acc 1\n",
      "2018-05-01T21:38:37.328465: step 2618, loss 0.0503195, acc 0.96875\n",
      "2018-05-01T21:38:37.343836: step 2619, loss 0.000914953, acc 1\n",
      "2018-05-01T21:38:37.358396: step 2620, loss 0.0378204, acc 0.96875\n",
      "2018-05-01T21:38:37.371450: step 2621, loss 0.00506304, acc 1\n",
      "2018-05-01T21:38:37.388507: step 2622, loss 0.000828312, acc 1\n",
      "2018-05-01T21:38:37.403640: step 2623, loss 0.0170392, acc 1\n",
      "2018-05-01T21:38:37.419823: step 2624, loss 0.0171321, acc 1\n",
      "2018-05-01T21:38:37.434467: step 2625, loss 0.000526735, acc 1\n",
      "2018-05-01T21:38:37.448793: step 2626, loss 0.0802613, acc 0.96875\n",
      "2018-05-01T21:38:37.463364: step 2627, loss 0.0439191, acc 0.96875\n",
      "2018-05-01T21:38:37.477275: step 2628, loss 0.00294024, acc 1\n",
      "2018-05-01T21:38:37.490555: step 2629, loss 0.0130847, acc 1\n",
      "2018-05-01T21:38:37.505651: step 2630, loss 0.000110048, acc 1\n",
      "2018-05-01T21:38:37.522763: step 2631, loss 0.00134366, acc 1\n",
      "2018-05-01T21:38:37.538994: step 2632, loss 0.000546876, acc 1\n",
      "2018-05-01T21:38:37.556745: step 2633, loss 0.000675878, acc 1\n",
      "2018-05-01T21:38:37.572890: step 2634, loss 0.0036095, acc 1\n",
      "2018-05-01T21:38:37.588083: step 2635, loss 0.000658079, acc 1\n",
      "2018-05-01T21:38:37.604488: step 2636, loss 0.0127077, acc 1\n",
      "2018-05-01T21:38:37.621377: step 2637, loss 0.00452849, acc 1\n",
      "2018-05-01T21:38:37.640107: step 2638, loss 0.00613844, acc 1\n",
      "2018-05-01T21:38:37.655819: step 2639, loss 0.237112, acc 0.96875\n",
      "2018-05-01T21:38:37.673099: step 2640, loss 0.000878644, acc 1\n",
      "2018-05-01T21:38:37.689094: step 2641, loss 0.0219516, acc 1\n",
      "2018-05-01T21:38:37.704390: step 2642, loss 0.00162367, acc 1\n",
      "2018-05-01T21:38:37.719911: step 2643, loss 0.108511, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:37.734476: step 2644, loss 0.0292201, acc 0.96875\n",
      "2018-05-01T21:38:37.748517: step 2645, loss 0.00195635, acc 1\n",
      "2018-05-01T21:38:37.762345: step 2646, loss 0.00486328, acc 1\n",
      "2018-05-01T21:38:37.778270: step 2647, loss 0.180502, acc 0.96875\n",
      "2018-05-01T21:38:37.794982: step 2648, loss 0.00240749, acc 1\n",
      "2018-05-01T21:38:37.808732: step 2649, loss 0.00317601, acc 1\n",
      "2018-05-01T21:38:37.823678: step 2650, loss 0.000431032, acc 1\n",
      "2018-05-01T21:38:37.839513: step 2651, loss 0.0063822, acc 1\n",
      "2018-05-01T21:38:37.857508: step 2652, loss 0.0219014, acc 1\n",
      "2018-05-01T21:38:37.876679: step 2653, loss 0.0419682, acc 0.96875\n",
      "2018-05-01T21:38:37.906449: step 2654, loss 0.014559, acc 1\n",
      "2018-05-01T21:38:37.924636: step 2655, loss 0.0090911, acc 1\n",
      "2018-05-01T21:38:37.945086: step 2656, loss 0.165537, acc 0.96875\n",
      "2018-05-01T21:38:37.967126: step 2657, loss 0.114557, acc 0.96875\n",
      "2018-05-01T21:38:37.993116: step 2658, loss 0.0785703, acc 0.96875\n",
      "2018-05-01T21:38:38.008420: step 2659, loss 0.00128734, acc 1\n",
      "2018-05-01T21:38:38.023754: step 2660, loss 0.076503, acc 0.96875\n",
      "2018-05-01T21:38:38.039711: step 2661, loss 0.0339012, acc 0.96875\n",
      "2018-05-01T21:38:38.057151: step 2662, loss 0.116302, acc 0.96875\n",
      "2018-05-01T21:38:38.073924: step 2663, loss 0.00189764, acc 1\n",
      "2018-05-01T21:38:38.092258: step 2664, loss 0.0306921, acc 0.96875\n",
      "2018-05-01T21:38:38.115136: step 2665, loss 0.000331682, acc 1\n",
      "2018-05-01T21:38:38.134046: step 2666, loss 0.00682043, acc 1\n",
      "2018-05-01T21:38:38.215967: step 2667, loss 0.00610607, acc 1\n",
      "2018-05-01T21:38:38.236491: step 2668, loss 0.000766757, acc 1\n",
      "2018-05-01T21:38:38.254097: step 2669, loss 0.00324397, acc 1\n",
      "2018-05-01T21:38:38.270594: step 2670, loss 0.0215108, acc 1\n",
      "2018-05-01T21:38:38.290794: step 2671, loss 0.00302206, acc 1\n",
      "2018-05-01T21:38:38.308971: step 2672, loss 0.0059733, acc 1\n",
      "2018-05-01T21:38:38.330424: step 2673, loss 0.00243205, acc 1\n",
      "2018-05-01T21:38:38.348679: step 2674, loss 0.0022802, acc 1\n",
      "2018-05-01T21:38:38.366769: step 2675, loss 0.00189615, acc 1\n",
      "2018-05-01T21:38:38.385599: step 2676, loss 0.0157542, acc 1\n",
      "2018-05-01T21:38:38.398695: step 2677, loss 0.123769, acc 0.96875\n",
      "2018-05-01T21:38:38.413274: step 2678, loss 0.00201563, acc 1\n",
      "2018-05-01T21:38:38.428560: step 2679, loss 0.00138803, acc 1\n",
      "2018-05-01T21:38:38.443871: step 2680, loss 0.0372863, acc 0.96875\n",
      "2018-05-01T21:38:38.460778: step 2681, loss 0.00114766, acc 1\n",
      "2018-05-01T21:38:38.477605: step 2682, loss 0.0268057, acc 1\n",
      "2018-05-01T21:38:38.494683: step 2683, loss 0.00251796, acc 1\n",
      "2018-05-01T21:38:38.511288: step 2684, loss 0.011444, acc 1\n",
      "2018-05-01T21:38:38.527548: step 2685, loss 0.000485861, acc 1\n",
      "2018-05-01T21:38:38.542311: step 2686, loss 0.0310694, acc 0.96875\n",
      "2018-05-01T21:38:38.559330: step 2687, loss 0.0385821, acc 0.96875\n",
      "2018-05-01T21:38:38.574354: step 2688, loss 0.00222581, acc 1\n",
      "2018-05-01T21:38:38.589645: step 2689, loss 0.0110751, acc 1\n",
      "2018-05-01T21:38:38.602671: step 2690, loss 0.000531308, acc 1\n",
      "2018-05-01T21:38:38.616573: step 2691, loss 0.00435734, acc 1\n",
      "2018-05-01T21:38:38.633739: step 2692, loss 0.0928051, acc 0.96875\n",
      "2018-05-01T21:38:38.649060: step 2693, loss 0.000891694, acc 1\n",
      "2018-05-01T21:38:38.665753: step 2694, loss 0.0011749, acc 1\n",
      "2018-05-01T21:38:38.686507: step 2695, loss 0.00256807, acc 1\n",
      "2018-05-01T21:38:38.700252: step 2696, loss 0.00158306, acc 1\n",
      "2018-05-01T21:38:38.713707: step 2697, loss 0.0011955, acc 1\n",
      "2018-05-01T21:38:38.730510: step 2698, loss 0.00197384, acc 1\n",
      "2018-05-01T21:38:38.746827: step 2699, loss 0.00219269, acc 1\n",
      "2018-05-01T21:38:38.781357: step 2700, loss 0.000245483, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:38.788067: step 2700, loss 0.65858, acc 0.816092\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-2700\n",
      "\n",
      "2018-05-01T21:38:38.894485: step 2701, loss 0.00098208, acc 1\n",
      "2018-05-01T21:38:38.912785: step 2702, loss 0.00211471, acc 1\n",
      "2018-05-01T21:38:38.930693: step 2703, loss 0.00257999, acc 1\n",
      "2018-05-01T21:38:38.949969: step 2704, loss 0.00172914, acc 1\n",
      "2018-05-01T21:38:38.969752: step 2705, loss 0.0415597, acc 0.96875\n",
      "2018-05-01T21:38:38.993549: step 2706, loss 0.0023326, acc 1\n",
      "2018-05-01T21:38:39.012581: step 2707, loss 0.00465721, acc 1\n",
      "2018-05-01T21:38:39.030765: step 2708, loss 0.000747987, acc 1\n",
      "2018-05-01T21:38:39.046970: step 2709, loss 0.0695553, acc 0.96875\n",
      "2018-05-01T21:38:39.063725: step 2710, loss 0.0223832, acc 1\n",
      "2018-05-01T21:38:39.084322: step 2711, loss 0.219402, acc 0.9375\n",
      "2018-05-01T21:38:39.102624: step 2712, loss 0.000759069, acc 1\n",
      "2018-05-01T21:38:39.119012: step 2713, loss 0.0219223, acc 1\n",
      "2018-05-01T21:38:39.134090: step 2714, loss 0.000194708, acc 1\n",
      "2018-05-01T21:38:39.151639: step 2715, loss 0.00825479, acc 1\n",
      "2018-05-01T21:38:39.167156: step 2716, loss 0.0108449, acc 1\n",
      "2018-05-01T21:38:39.184343: step 2717, loss 0.00285897, acc 1\n",
      "2018-05-01T21:38:39.200517: step 2718, loss 0.0126802, acc 1\n",
      "2018-05-01T21:38:39.214436: step 2719, loss 0.000168964, acc 1\n",
      "2018-05-01T21:38:39.231767: step 2720, loss 0.219903, acc 0.9375\n",
      "2018-05-01T21:38:39.250056: step 2721, loss 0.123407, acc 0.96875\n",
      "2018-05-01T21:38:39.264562: step 2722, loss 0.000851295, acc 1\n",
      "2018-05-01T21:38:39.279404: step 2723, loss 0.000954236, acc 1\n",
      "2018-05-01T21:38:39.294181: step 2724, loss 0.00126694, acc 1\n",
      "2018-05-01T21:38:39.308530: step 2725, loss 0.00702177, acc 1\n",
      "2018-05-01T21:38:39.322757: step 2726, loss 0.0278199, acc 1\n",
      "2018-05-01T21:38:39.336510: step 2727, loss 0.0058694, acc 1\n",
      "2018-05-01T21:38:39.352011: step 2728, loss 0.00139191, acc 1\n",
      "2018-05-01T21:38:39.367844: step 2729, loss 0.000770316, acc 1\n",
      "2018-05-01T21:38:39.384887: step 2730, loss 0.00703564, acc 1\n",
      "2018-05-01T21:38:39.400155: step 2731, loss 0.0040438, acc 1\n",
      "2018-05-01T21:38:39.415533: step 2732, loss 0.000677071, acc 1\n",
      "2018-05-01T21:38:39.432531: step 2733, loss 0.00083581, acc 1\n",
      "2018-05-01T21:38:39.448118: step 2734, loss 0.0409702, acc 0.96875\n",
      "2018-05-01T21:38:39.464352: step 2735, loss 0.000853147, acc 1\n",
      "2018-05-01T21:38:39.477837: step 2736, loss 0.00115755, acc 1\n",
      "2018-05-01T21:38:39.491534: step 2737, loss 0.00144701, acc 1\n",
      "2018-05-01T21:38:39.504762: step 2738, loss 0.000342453, acc 1\n",
      "2018-05-01T21:38:39.520320: step 2739, loss 0.000923712, acc 1\n",
      "2018-05-01T21:38:39.534532: step 2740, loss 0.0424711, acc 0.96875\n",
      "2018-05-01T21:38:39.550055: step 2741, loss 0.00346262, acc 1\n",
      "2018-05-01T21:38:39.569898: step 2742, loss 0.00196429, acc 1\n",
      "2018-05-01T21:38:39.583958: step 2743, loss 0.00401691, acc 1\n",
      "2018-05-01T21:38:39.599733: step 2744, loss 0.0791946, acc 0.96875\n",
      "2018-05-01T21:38:39.614948: step 2745, loss 0.0797263, acc 0.96875\n",
      "2018-05-01T21:38:39.630166: step 2746, loss 0.00268017, acc 1\n",
      "2018-05-01T21:38:39.646531: step 2747, loss 0.0029932, acc 1\n",
      "2018-05-01T21:38:39.664152: step 2748, loss 0.000738153, acc 1\n",
      "2018-05-01T21:38:39.681893: step 2749, loss 0.00627042, acc 1\n",
      "2018-05-01T21:38:39.696616: step 2750, loss 0.198035, acc 0.944444\n",
      "2018-05-01T21:38:39.711735: step 2751, loss 0.000945483, acc 1\n",
      "2018-05-01T21:38:39.725506: step 2752, loss 0.0902065, acc 0.96875\n",
      "2018-05-01T21:38:39.741998: step 2753, loss 0.0252095, acc 1\n",
      "2018-05-01T21:38:39.757605: step 2754, loss 0.172937, acc 0.9375\n",
      "2018-05-01T21:38:39.771458: step 2755, loss 0.0138754, acc 1\n",
      "2018-05-01T21:38:39.787141: step 2756, loss 0.0172775, acc 1\n",
      "2018-05-01T21:38:39.802505: step 2757, loss 0.00148535, acc 1\n",
      "2018-05-01T21:38:39.817214: step 2758, loss 0.00616237, acc 1\n",
      "2018-05-01T21:38:39.832774: step 2759, loss 0.108544, acc 0.96875\n",
      "2018-05-01T21:38:39.848631: step 2760, loss 0.00040123, acc 1\n",
      "2018-05-01T21:38:39.866506: step 2761, loss 0.168689, acc 0.96875\n",
      "2018-05-01T21:38:39.881046: step 2762, loss 0.0124727, acc 1\n",
      "2018-05-01T21:38:39.897202: step 2763, loss 0.00267605, acc 1\n",
      "2018-05-01T21:38:39.912991: step 2764, loss 0.0012914, acc 1\n",
      "2018-05-01T21:38:39.929960: step 2765, loss 0.00508048, acc 1\n",
      "2018-05-01T21:38:39.947203: step 2766, loss 0.020591, acc 1\n",
      "2018-05-01T21:38:39.961677: step 2767, loss 0.00180132, acc 1\n",
      "2018-05-01T21:38:39.979804: step 2768, loss 0.125969, acc 0.96875\n",
      "2018-05-01T21:38:39.997755: step 2769, loss 0.00569266, acc 1\n",
      "2018-05-01T21:38:40.013522: step 2770, loss 0.00216381, acc 1\n",
      "2018-05-01T21:38:40.030634: step 2771, loss 0.00555758, acc 1\n",
      "2018-05-01T21:38:40.046650: step 2772, loss 0.00487052, acc 1\n",
      "2018-05-01T21:38:40.064417: step 2773, loss 0.00228203, acc 1\n",
      "2018-05-01T21:38:40.080103: step 2774, loss 0.0021513, acc 1\n",
      "2018-05-01T21:38:40.093406: step 2775, loss 0.00332019, acc 1\n",
      "2018-05-01T21:38:40.107033: step 2776, loss 0.00471832, acc 1\n",
      "2018-05-01T21:38:40.122046: step 2777, loss 0.0900536, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:40.138278: step 2778, loss 0.000674634, acc 1\n",
      "2018-05-01T21:38:40.153839: step 2779, loss 0.0107208, acc 1\n",
      "2018-05-01T21:38:40.168405: step 2780, loss 0.00223695, acc 1\n",
      "2018-05-01T21:38:40.183423: step 2781, loss 0.0284786, acc 0.96875\n",
      "2018-05-01T21:38:40.199032: step 2782, loss 0.0420686, acc 0.96875\n",
      "2018-05-01T21:38:40.216268: step 2783, loss 0.0107952, acc 1\n",
      "2018-05-01T21:38:40.231647: step 2784, loss 0.00666755, acc 1\n",
      "2018-05-01T21:38:40.248473: step 2785, loss 0.000580062, acc 1\n",
      "2018-05-01T21:38:40.265257: step 2786, loss 0.00207002, acc 1\n",
      "2018-05-01T21:38:40.279273: step 2787, loss 0.0100862, acc 1\n",
      "2018-05-01T21:38:40.294545: step 2788, loss 0.00182944, acc 1\n",
      "2018-05-01T21:38:40.309724: step 2789, loss 0.113057, acc 0.96875\n",
      "2018-05-01T21:38:40.328420: step 2790, loss 0.0194758, acc 1\n",
      "2018-05-01T21:38:40.345325: step 2791, loss 0.00687629, acc 1\n",
      "2018-05-01T21:38:40.361841: step 2792, loss 0.00435793, acc 1\n",
      "2018-05-01T21:38:40.378967: step 2793, loss 0.00672503, acc 1\n",
      "2018-05-01T21:38:40.393276: step 2794, loss 0.00173359, acc 1\n",
      "2018-05-01T21:38:40.410140: step 2795, loss 0.000676184, acc 1\n",
      "2018-05-01T21:38:40.427553: step 2796, loss 0.0158031, acc 1\n",
      "2018-05-01T21:38:40.443143: step 2797, loss 0.0904328, acc 0.96875\n",
      "2018-05-01T21:38:40.464470: step 2798, loss 0.0942327, acc 0.96875\n",
      "2018-05-01T21:38:40.480241: step 2799, loss 7.3721e-05, acc 1\n",
      "2018-05-01T21:38:40.492449: step 2800, loss 0.00205021, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:40.499797: step 2800, loss 0.618769, acc 0.850575\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-2800\n",
      "\n",
      "2018-05-01T21:38:40.564219: step 2801, loss 0.00277509, acc 1\n",
      "2018-05-01T21:38:40.582764: step 2802, loss 0.00195214, acc 1\n",
      "2018-05-01T21:38:40.600055: step 2803, loss 0.036248, acc 0.96875\n",
      "2018-05-01T21:38:40.621189: step 2804, loss 0.0470024, acc 0.96875\n",
      "2018-05-01T21:38:40.706100: step 2805, loss 0.000150382, acc 1\n",
      "2018-05-01T21:38:40.728933: step 2806, loss 0.000668107, acc 1\n",
      "2018-05-01T21:38:40.748125: step 2807, loss 0.00416096, acc 1\n",
      "2018-05-01T21:38:40.765280: step 2808, loss 0.0470171, acc 0.96875\n",
      "2018-05-01T21:38:40.789213: step 2809, loss 0.0111507, acc 1\n",
      "2018-05-01T21:38:40.807367: step 2810, loss 0.0451648, acc 0.96875\n",
      "2018-05-01T21:38:40.827177: step 2811, loss 0.00222525, acc 1\n",
      "2018-05-01T21:38:40.844632: step 2812, loss 0.0146398, acc 1\n",
      "2018-05-01T21:38:40.867260: step 2813, loss 0.00254718, acc 1\n",
      "2018-05-01T21:38:40.883083: step 2814, loss 0.00311126, acc 1\n",
      "2018-05-01T21:38:40.897148: step 2815, loss 0.00231531, acc 1\n",
      "2018-05-01T21:38:40.911419: step 2816, loss 0.0537406, acc 0.96875\n",
      "2018-05-01T21:38:40.925201: step 2817, loss 0.000947194, acc 1\n",
      "2018-05-01T21:38:40.940612: step 2818, loss 0.00461633, acc 1\n",
      "2018-05-01T21:38:40.953816: step 2819, loss 0.0108697, acc 1\n",
      "2018-05-01T21:38:40.971573: step 2820, loss 0.000167394, acc 1\n",
      "2018-05-01T21:38:40.988233: step 2821, loss 0.00146402, acc 1\n",
      "2018-05-01T21:38:41.004602: step 2822, loss 0.008745, acc 1\n",
      "2018-05-01T21:38:41.021047: step 2823, loss 0.00236199, acc 1\n",
      "2018-05-01T21:38:41.036637: step 2824, loss 0.00875582, acc 1\n",
      "2018-05-01T21:38:41.052874: step 2825, loss 0.000959436, acc 1\n",
      "2018-05-01T21:38:41.069459: step 2826, loss 0.00372922, acc 1\n",
      "2018-05-01T21:38:41.086350: step 2827, loss 0.00166695, acc 1\n",
      "2018-05-01T21:38:41.104407: step 2828, loss 0.00219294, acc 1\n",
      "2018-05-01T21:38:41.121033: step 2829, loss 0.000321927, acc 1\n",
      "2018-05-01T21:38:41.139358: step 2830, loss 0.0247866, acc 0.96875\n",
      "2018-05-01T21:38:41.153964: step 2831, loss 0.154688, acc 0.96875\n",
      "2018-05-01T21:38:41.171957: step 2832, loss 0.0159136, acc 1\n",
      "2018-05-01T21:38:41.189910: step 2833, loss 0.00131177, acc 1\n",
      "2018-05-01T21:38:41.205148: step 2834, loss 0.00881389, acc 1\n",
      "2018-05-01T21:38:41.222018: step 2835, loss 0.000844873, acc 1\n",
      "2018-05-01T21:38:41.238651: step 2836, loss 0.0042446, acc 1\n",
      "2018-05-01T21:38:41.255445: step 2837, loss 0.099947, acc 0.96875\n",
      "2018-05-01T21:38:41.270465: step 2838, loss 0.00487895, acc 1\n",
      "2018-05-01T21:38:41.287076: step 2839, loss 0.000785417, acc 1\n",
      "2018-05-01T21:38:41.304242: step 2840, loss 0.0443331, acc 0.96875\n",
      "2018-05-01T21:38:41.320625: step 2841, loss 0.0376871, acc 0.96875\n",
      "2018-05-01T21:38:41.337657: step 2842, loss 0.00519748, acc 1\n",
      "2018-05-01T21:38:41.354744: step 2843, loss 0.00127454, acc 1\n",
      "2018-05-01T21:38:41.371027: step 2844, loss 0.0010075, acc 1\n",
      "2018-05-01T21:38:41.387675: step 2845, loss 0.000683533, acc 1\n",
      "2018-05-01T21:38:41.405301: step 2846, loss 0.0142317, acc 1\n",
      "2018-05-01T21:38:41.420916: step 2847, loss 0.00736738, acc 1\n",
      "2018-05-01T21:38:41.437765: step 2848, loss 0.131621, acc 0.96875\n",
      "2018-05-01T21:38:41.454866: step 2849, loss 0.00098197, acc 1\n",
      "2018-05-01T21:38:41.470783: step 2850, loss 0.0872589, acc 0.944444\n",
      "2018-05-01T21:38:41.487885: step 2851, loss 0.0989933, acc 0.96875\n",
      "2018-05-01T21:38:41.504799: step 2852, loss 0.0159742, acc 1\n",
      "2018-05-01T21:38:41.522401: step 2853, loss 0.000454331, acc 1\n",
      "2018-05-01T21:38:41.536749: step 2854, loss 0.0302484, acc 0.96875\n",
      "2018-05-01T21:38:41.552030: step 2855, loss 0.00435676, acc 1\n",
      "2018-05-01T21:38:41.565623: step 2856, loss 0.163658, acc 0.96875\n",
      "2018-05-01T21:38:41.578798: step 2857, loss 0.0160743, acc 1\n",
      "2018-05-01T21:38:41.593191: step 2858, loss 0.000610098, acc 1\n",
      "2018-05-01T21:38:41.606871: step 2859, loss 0.00139503, acc 1\n",
      "2018-05-01T21:38:41.621084: step 2860, loss 0.000550283, acc 1\n",
      "2018-05-01T21:38:41.638776: step 2861, loss 0.000650909, acc 1\n",
      "2018-05-01T21:38:41.654262: step 2862, loss 0.000769072, acc 1\n",
      "2018-05-01T21:38:41.671068: step 2863, loss 0.00534973, acc 1\n",
      "2018-05-01T21:38:41.687881: step 2864, loss 0.00855264, acc 1\n",
      "2018-05-01T21:38:41.706263: step 2865, loss 0.0568471, acc 0.96875\n",
      "2018-05-01T21:38:41.723025: step 2866, loss 0.128363, acc 0.96875\n",
      "2018-05-01T21:38:41.740666: step 2867, loss 0.000802332, acc 1\n",
      "2018-05-01T21:38:41.755029: step 2868, loss 0.000833151, acc 1\n",
      "2018-05-01T21:38:41.770538: step 2869, loss 0.00120428, acc 1\n",
      "2018-05-01T21:38:41.785320: step 2870, loss 0.00335074, acc 1\n",
      "2018-05-01T21:38:41.805524: step 2871, loss 0.00217727, acc 1\n",
      "2018-05-01T21:38:41.845195: step 2872, loss 0.000953133, acc 1\n",
      "2018-05-01T21:38:41.865054: step 2873, loss 0.0933697, acc 0.96875\n",
      "2018-05-01T21:38:41.895999: step 2874, loss 0.0434648, acc 0.96875\n",
      "2018-05-01T21:38:41.920685: step 2875, loss 0.00496799, acc 1\n",
      "2018-05-01T21:38:41.940847: step 2876, loss 0.00317533, acc 1\n",
      "2018-05-01T21:38:41.961453: step 2877, loss 0.0276082, acc 0.96875\n",
      "2018-05-01T21:38:41.980711: step 2878, loss 0.00178231, acc 1\n",
      "2018-05-01T21:38:41.999679: step 2879, loss 0.00175628, acc 1\n",
      "2018-05-01T21:38:42.020809: step 2880, loss 0.00680323, acc 1\n",
      "2018-05-01T21:38:42.037270: step 2881, loss 0.000738803, acc 1\n",
      "2018-05-01T21:38:42.054697: step 2882, loss 0.0032069, acc 1\n",
      "2018-05-01T21:38:42.068161: step 2883, loss 0.00199273, acc 1\n",
      "2018-05-01T21:38:42.087546: step 2884, loss 0.000644301, acc 1\n",
      "2018-05-01T21:38:42.104314: step 2885, loss 0.0013825, acc 1\n",
      "2018-05-01T21:38:42.118751: step 2886, loss 0.000133415, acc 1\n",
      "2018-05-01T21:38:42.135131: step 2887, loss 0.00912947, acc 1\n",
      "2018-05-01T21:38:42.152671: step 2888, loss 0.00340315, acc 1\n",
      "2018-05-01T21:38:42.181428: step 2889, loss 0.0444299, acc 0.96875\n",
      "2018-05-01T21:38:42.197686: step 2890, loss 0.000688557, acc 1\n",
      "2018-05-01T21:38:42.211817: step 2891, loss 0.203865, acc 0.9375\n",
      "2018-05-01T21:38:42.225466: step 2892, loss 0.0077398, acc 1\n",
      "2018-05-01T21:38:42.239483: step 2893, loss 0.101732, acc 0.96875\n",
      "2018-05-01T21:38:42.255863: step 2894, loss 0.0128549, acc 1\n",
      "2018-05-01T21:38:42.277410: step 2895, loss 0.0553986, acc 0.96875\n",
      "2018-05-01T21:38:42.295339: step 2896, loss 0.00237733, acc 1\n",
      "2018-05-01T21:38:42.309037: step 2897, loss 0.000428249, acc 1\n",
      "2018-05-01T21:38:42.324292: step 2898, loss 0.0686448, acc 0.9375\n",
      "2018-05-01T21:38:42.338326: step 2899, loss 0.000824814, acc 1\n",
      "2018-05-01T21:38:42.353877: step 2900, loss 0.000141872, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:42.360827: step 2900, loss 0.616243, acc 0.827586\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-2900\n",
      "\n",
      "2018-05-01T21:38:42.440889: step 2901, loss 0.00578815, acc 1\n",
      "2018-05-01T21:38:42.457375: step 2902, loss 0.000190816, acc 1\n",
      "2018-05-01T21:38:42.474498: step 2903, loss 0.156371, acc 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:42.493078: step 2904, loss 0.0164777, acc 1\n",
      "2018-05-01T21:38:42.512613: step 2905, loss 0.00115888, acc 1\n",
      "2018-05-01T21:38:42.533412: step 2906, loss 0.0058853, acc 1\n",
      "2018-05-01T21:38:42.558154: step 2907, loss 0.00146898, acc 1\n",
      "2018-05-01T21:38:42.578829: step 2908, loss 0.00313628, acc 1\n",
      "2018-05-01T21:38:42.598708: step 2909, loss 0.0516933, acc 0.96875\n",
      "2018-05-01T21:38:42.618076: step 2910, loss 0.00203973, acc 1\n",
      "2018-05-01T21:38:42.638228: step 2911, loss 0.00640661, acc 1\n",
      "2018-05-01T21:38:42.660658: step 2912, loss 0.00281678, acc 1\n",
      "2018-05-01T21:38:42.679027: step 2913, loss 0.00495873, acc 1\n",
      "2018-05-01T21:38:42.697894: step 2914, loss 0.190069, acc 0.96875\n",
      "2018-05-01T21:38:42.720576: step 2915, loss 0.0131414, acc 1\n",
      "2018-05-01T21:38:42.741837: step 2916, loss 0.000150711, acc 1\n",
      "2018-05-01T21:38:42.762815: step 2917, loss 0.00423521, acc 1\n",
      "2018-05-01T21:38:42.779223: step 2918, loss 0.0154742, acc 1\n",
      "2018-05-01T21:38:42.801892: step 2919, loss 0.00880212, acc 1\n",
      "2018-05-01T21:38:42.822417: step 2920, loss 0.0238815, acc 0.96875\n",
      "2018-05-01T21:38:42.842189: step 2921, loss 0.000682486, acc 1\n",
      "2018-05-01T21:38:42.860627: step 2922, loss 0.0315737, acc 0.96875\n",
      "2018-05-01T21:38:42.880389: step 2923, loss 0.00446335, acc 1\n",
      "2018-05-01T21:38:42.899947: step 2924, loss 0.057903, acc 0.96875\n",
      "2018-05-01T21:38:42.917983: step 2925, loss 0.000318895, acc 1\n",
      "2018-05-01T21:38:42.939170: step 2926, loss 0.000685143, acc 1\n",
      "2018-05-01T21:38:42.960875: step 2927, loss 0.0146016, acc 1\n",
      "2018-05-01T21:38:42.981224: step 2928, loss 0.000370912, acc 1\n",
      "2018-05-01T21:38:42.998405: step 2929, loss 0.0447377, acc 0.96875\n",
      "2018-05-01T21:38:43.022868: step 2930, loss 0.00050063, acc 1\n",
      "2018-05-01T21:38:43.046020: step 2931, loss 0.000276727, acc 1\n",
      "2018-05-01T21:38:43.063693: step 2932, loss 0.00499956, acc 1\n",
      "2018-05-01T21:38:43.082908: step 2933, loss 0.0163509, acc 1\n",
      "2018-05-01T21:38:43.098835: step 2934, loss 0.0613507, acc 0.96875\n",
      "2018-05-01T21:38:43.115675: step 2935, loss 0.000755902, acc 1\n",
      "2018-05-01T21:38:43.131940: step 2936, loss 0.000204111, acc 1\n",
      "2018-05-01T21:38:43.149866: step 2937, loss 0.00205225, acc 1\n",
      "2018-05-01T21:38:43.168363: step 2938, loss 0.0322913, acc 0.96875\n",
      "2018-05-01T21:38:43.187032: step 2939, loss 0.0039571, acc 1\n",
      "2018-05-01T21:38:43.202247: step 2940, loss 0.000130707, acc 1\n",
      "2018-05-01T21:38:43.222745: step 2941, loss 0.00308638, acc 1\n",
      "2018-05-01T21:38:43.242217: step 2942, loss 0.00768327, acc 1\n",
      "2018-05-01T21:38:43.261727: step 2943, loss 0.0240956, acc 0.96875\n",
      "2018-05-01T21:38:43.282331: step 2944, loss 0.00174094, acc 1\n",
      "2018-05-01T21:38:43.300702: step 2945, loss 0.0151594, acc 1\n",
      "2018-05-01T21:38:43.318521: step 2946, loss 0.00835939, acc 1\n",
      "2018-05-01T21:38:43.332974: step 2947, loss 0.00123864, acc 1\n",
      "2018-05-01T21:38:43.349474: step 2948, loss 0.0998838, acc 0.96875\n",
      "2018-05-01T21:38:43.364593: step 2949, loss 0.000734113, acc 1\n",
      "2018-05-01T21:38:43.377786: step 2950, loss 0.0165596, acc 1\n",
      "2018-05-01T21:38:43.394734: step 2951, loss 0.00126214, acc 1\n",
      "2018-05-01T21:38:43.411825: step 2952, loss 0.0395917, acc 0.96875\n",
      "2018-05-01T21:38:43.425706: step 2953, loss 0.102762, acc 0.96875\n",
      "2018-05-01T21:38:43.440301: step 2954, loss 0.00279368, acc 1\n",
      "2018-05-01T21:38:43.454986: step 2955, loss 0.00261723, acc 1\n",
      "2018-05-01T21:38:43.470345: step 2956, loss 0.000496491, acc 1\n",
      "2018-05-01T21:38:43.487107: step 2957, loss 0.0319183, acc 0.96875\n",
      "2018-05-01T21:38:43.502943: step 2958, loss 0.115677, acc 0.96875\n",
      "2018-05-01T21:38:43.516133: step 2959, loss 0.000158424, acc 1\n",
      "2018-05-01T21:38:43.529810: step 2960, loss 0.000483194, acc 1\n",
      "2018-05-01T21:38:43.544224: step 2961, loss 0.0125137, acc 1\n",
      "2018-05-01T21:38:43.561311: step 2962, loss 0.000229076, acc 1\n",
      "2018-05-01T21:38:43.576795: step 2963, loss 0.00228458, acc 1\n",
      "2018-05-01T21:38:43.591517: step 2964, loss 0.0219548, acc 1\n",
      "2018-05-01T21:38:43.609724: step 2965, loss 0.0252086, acc 1\n",
      "2018-05-01T21:38:43.627081: step 2966, loss 0.00619893, acc 1\n",
      "2018-05-01T21:38:43.641342: step 2967, loss 0.00125252, acc 1\n",
      "2018-05-01T21:38:43.659205: step 2968, loss 0.00495194, acc 1\n",
      "2018-05-01T21:38:43.673923: step 2969, loss 0.0226679, acc 1\n",
      "2018-05-01T21:38:43.692251: step 2970, loss 0.00237755, acc 1\n",
      "2018-05-01T21:38:43.708073: step 2971, loss 0.000372583, acc 1\n",
      "2018-05-01T21:38:43.725009: step 2972, loss 0.00102112, acc 1\n",
      "2018-05-01T21:38:43.740467: step 2973, loss 0.00125663, acc 1\n",
      "2018-05-01T21:38:43.758602: step 2974, loss 0.00558745, acc 1\n",
      "2018-05-01T21:38:43.774577: step 2975, loss 0.00520226, acc 1\n",
      "2018-05-01T21:38:43.790691: step 2976, loss 0.0570283, acc 0.96875\n",
      "2018-05-01T21:38:43.806817: step 2977, loss 0.000398217, acc 1\n",
      "2018-05-01T21:38:43.823309: step 2978, loss 0.00411431, acc 1\n",
      "2018-05-01T21:38:43.837824: step 2979, loss 0.00235633, acc 1\n",
      "2018-05-01T21:38:43.855603: step 2980, loss 0.0581629, acc 0.96875\n",
      "2018-05-01T21:38:43.875354: step 2981, loss 0.00126683, acc 1\n",
      "2018-05-01T21:38:43.890958: step 2982, loss 0.00284238, acc 1\n",
      "2018-05-01T21:38:43.907307: step 2983, loss 0.00544659, acc 1\n",
      "2018-05-01T21:38:43.922899: step 2984, loss 0.00954098, acc 1\n",
      "2018-05-01T21:38:43.938599: step 2985, loss 0.00770587, acc 1\n",
      "2018-05-01T21:38:43.954991: step 2986, loss 0.00670294, acc 1\n",
      "2018-05-01T21:38:43.971845: step 2987, loss 0.00659586, acc 1\n",
      "2018-05-01T21:38:43.987787: step 2988, loss 0.0377781, acc 0.96875\n",
      "2018-05-01T21:38:44.005972: step 2989, loss 0.0316737, acc 0.96875\n",
      "2018-05-01T21:38:44.026378: step 2990, loss 0.0419481, acc 0.96875\n",
      "2018-05-01T21:38:44.043636: step 2991, loss 0.0245497, acc 0.96875\n",
      "2018-05-01T21:38:44.062867: step 2992, loss 0.00343616, acc 1\n",
      "2018-05-01T21:38:44.085667: step 2993, loss 0.00203385, acc 1\n",
      "2018-05-01T21:38:44.104881: step 2994, loss 0.0670094, acc 0.96875\n",
      "2018-05-01T21:38:44.123708: step 2995, loss 0.000161245, acc 1\n",
      "2018-05-01T21:38:44.140460: step 2996, loss 0.000921153, acc 1\n",
      "2018-05-01T21:38:44.161231: step 2997, loss 0.00652191, acc 1\n",
      "2018-05-01T21:38:44.184259: step 2998, loss 0.00993852, acc 1\n",
      "2018-05-01T21:38:44.205202: step 2999, loss 0.0515472, acc 0.96875\n",
      "2018-05-01T21:38:44.221298: step 3000, loss 0.00185505, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:44.226481: step 3000, loss 0.597835, acc 0.850575\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-3000\n",
      "\n",
      "2018-05-01T21:38:44.320992: step 3001, loss 0.0111943, acc 1\n",
      "2018-05-01T21:38:44.339011: step 3002, loss 0.0346415, acc 1\n",
      "2018-05-01T21:38:44.359498: step 3003, loss 0.00698652, acc 1\n",
      "2018-05-01T21:38:44.378425: step 3004, loss 0.00627188, acc 1\n",
      "2018-05-01T21:38:44.393172: step 3005, loss 0.232245, acc 0.96875\n",
      "2018-05-01T21:38:44.406716: step 3006, loss 0.00137004, acc 1\n",
      "2018-05-01T21:38:44.426772: step 3007, loss 0.00353447, acc 1\n",
      "2018-05-01T21:38:44.443010: step 3008, loss 0.00144285, acc 1\n",
      "2018-05-01T21:38:44.457030: step 3009, loss 0.00184832, acc 1\n",
      "2018-05-01T21:38:44.472521: step 3010, loss 0.000495454, acc 1\n",
      "2018-05-01T21:38:44.487128: step 3011, loss 0.000331369, acc 1\n",
      "2018-05-01T21:38:44.504923: step 3012, loss 0.0257151, acc 0.96875\n",
      "2018-05-01T21:38:44.520033: step 3013, loss 0.000855771, acc 1\n",
      "2018-05-01T21:38:44.536066: step 3014, loss 0.000705116, acc 1\n",
      "2018-05-01T21:38:44.551342: step 3015, loss 0.0344219, acc 0.96875\n",
      "2018-05-01T21:38:44.566927: step 3016, loss 0.00156622, acc 1\n",
      "2018-05-01T21:38:44.583798: step 3017, loss 0.000474511, acc 1\n",
      "2018-05-01T21:38:44.603624: step 3018, loss 0.00544009, acc 1\n",
      "2018-05-01T21:38:44.619840: step 3019, loss 0.00610102, acc 1\n",
      "2018-05-01T21:38:44.639838: step 3020, loss 0.000795211, acc 1\n",
      "2018-05-01T21:38:44.656193: step 3021, loss 0.00771276, acc 1\n",
      "2018-05-01T21:38:44.675063: step 3022, loss 0.00301147, acc 1\n",
      "2018-05-01T21:38:44.700592: step 3023, loss 0.00203411, acc 1\n",
      "2018-05-01T21:38:44.722861: step 3024, loss 0.0149725, acc 1\n",
      "2018-05-01T21:38:44.738552: step 3025, loss 0.0548787, acc 0.944444\n",
      "2018-05-01T21:38:44.754504: step 3026, loss 0.000408061, acc 1\n",
      "2018-05-01T21:38:44.769767: step 3027, loss 0.00447066, acc 1\n",
      "2018-05-01T21:38:44.794688: step 3028, loss 0.0104778, acc 1\n",
      "2018-05-01T21:38:44.822264: step 3029, loss 0.0204944, acc 1\n",
      "2018-05-01T21:38:44.847112: step 3030, loss 0.0352821, acc 0.96875\n",
      "2018-05-01T21:38:44.874142: step 3031, loss 0.00470481, acc 1\n",
      "2018-05-01T21:38:44.890906: step 3032, loss 0.000386704, acc 1\n",
      "2018-05-01T21:38:44.906972: step 3033, loss 0.00652855, acc 1\n",
      "2018-05-01T21:38:44.923459: step 3034, loss 0.000443523, acc 1\n",
      "2018-05-01T21:38:44.942804: step 3035, loss 0.000578614, acc 1\n",
      "2018-05-01T21:38:44.959815: step 3036, loss 0.0114692, acc 1\n",
      "2018-05-01T21:38:44.982080: step 3037, loss 0.0106011, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:46.324561: step 3038, loss 0.0724464, acc 0.96875\n",
      "2018-05-01T21:38:46.405265: step 3039, loss 0.000638144, acc 1\n",
      "2018-05-01T21:38:46.432833: step 3040, loss 0.00114911, acc 1\n",
      "2018-05-01T21:38:46.450880: step 3041, loss 0.000527794, acc 1\n",
      "2018-05-01T21:38:46.464559: step 3042, loss 0.00159151, acc 1\n",
      "2018-05-01T21:38:46.492785: step 3043, loss 0.00178178, acc 1\n",
      "2018-05-01T21:38:46.514071: step 3044, loss 0.00845454, acc 1\n",
      "2018-05-01T21:38:46.534629: step 3045, loss 0.00093141, acc 1\n",
      "2018-05-01T21:38:46.558547: step 3046, loss 0.00346039, acc 1\n",
      "2018-05-01T21:38:46.580204: step 3047, loss 0.200901, acc 0.9375\n",
      "2018-05-01T21:38:46.597662: step 3048, loss 0.00035624, acc 1\n",
      "2018-05-01T21:38:46.612578: step 3049, loss 0.00142149, acc 1\n",
      "2018-05-01T21:38:46.624609: step 3050, loss 0.0296896, acc 1\n",
      "2018-05-01T21:38:46.639141: step 3051, loss 0.00316707, acc 1\n",
      "2018-05-01T21:38:46.654978: step 3052, loss 0.172019, acc 0.96875\n",
      "2018-05-01T21:38:46.668465: step 3053, loss 0.0847744, acc 0.96875\n",
      "2018-05-01T21:38:46.687377: step 3054, loss 0.00342245, acc 1\n",
      "2018-05-01T21:38:46.701184: step 3055, loss 0.000120791, acc 1\n",
      "2018-05-01T21:38:46.720089: step 3056, loss 0.000918769, acc 1\n",
      "2018-05-01T21:38:46.735475: step 3057, loss 0.00294669, acc 1\n",
      "2018-05-01T21:38:46.756978: step 3058, loss 0.000785666, acc 1\n",
      "2018-05-01T21:38:46.776112: step 3059, loss 0.00131412, acc 1\n",
      "2018-05-01T21:38:46.794454: step 3060, loss 0.00303169, acc 1\n",
      "2018-05-01T21:38:46.810571: step 3061, loss 0.00417746, acc 1\n",
      "2018-05-01T21:38:46.829318: step 3062, loss 0.00730338, acc 1\n",
      "2018-05-01T21:38:46.844437: step 3063, loss 0.000833381, acc 1\n",
      "2018-05-01T21:38:46.863806: step 3064, loss 0.00511753, acc 1\n",
      "2018-05-01T21:38:46.878736: step 3065, loss 0.0163105, acc 1\n",
      "2018-05-01T21:38:46.897959: step 3066, loss 0.000969647, acc 1\n",
      "2018-05-01T21:38:46.913632: step 3067, loss 0.0018697, acc 1\n",
      "2018-05-01T21:38:46.930894: step 3068, loss 0.00317011, acc 1\n",
      "2018-05-01T21:38:46.945492: step 3069, loss 0.0741728, acc 0.96875\n",
      "2018-05-01T21:38:46.962965: step 3070, loss 0.00341756, acc 1\n",
      "2018-05-01T21:38:46.987404: step 3071, loss 0.0591962, acc 0.96875\n",
      "2018-05-01T21:38:47.003840: step 3072, loss 0.0225707, acc 1\n",
      "2018-05-01T21:38:47.019646: step 3073, loss 0.00117487, acc 1\n",
      "2018-05-01T21:38:47.037389: step 3074, loss 0.101213, acc 0.96875\n",
      "2018-05-01T21:38:47.051516: step 3075, loss 0.0331956, acc 1\n",
      "2018-05-01T21:38:47.068678: step 3076, loss 0.0033839, acc 1\n",
      "2018-05-01T21:38:47.088865: step 3077, loss 0.00618092, acc 1\n",
      "2018-05-01T21:38:47.105258: step 3078, loss 0.0413326, acc 0.96875\n",
      "2018-05-01T21:38:47.120266: step 3079, loss 0.000417748, acc 1\n",
      "2018-05-01T21:38:47.136508: step 3080, loss 0.00593384, acc 1\n",
      "2018-05-01T21:38:47.154801: step 3081, loss 0.0191597, acc 1\n",
      "2018-05-01T21:38:47.170141: step 3082, loss 0.0238239, acc 1\n",
      "2018-05-01T21:38:47.184698: step 3083, loss 0.00528719, acc 1\n",
      "2018-05-01T21:38:47.200930: step 3084, loss 0.000537787, acc 1\n",
      "2018-05-01T21:38:47.216953: step 3085, loss 0.0560567, acc 0.96875\n",
      "2018-05-01T21:38:47.231997: step 3086, loss 0.00218147, acc 1\n",
      "2018-05-01T21:38:47.246151: step 3087, loss 0.00255418, acc 1\n",
      "2018-05-01T21:38:47.260106: step 3088, loss 0.0321501, acc 1\n",
      "2018-05-01T21:38:47.275534: step 3089, loss 0.000504664, acc 1\n",
      "2018-05-01T21:38:47.290579: step 3090, loss 0.00012776, acc 1\n",
      "2018-05-01T21:38:47.305701: step 3091, loss 0.0099382, acc 1\n",
      "2018-05-01T21:38:47.322124: step 3092, loss 0.00114952, acc 1\n",
      "2018-05-01T21:38:47.336710: step 3093, loss 0.00161367, acc 1\n",
      "2018-05-01T21:38:47.350117: step 3094, loss 0.0634923, acc 0.9375\n",
      "2018-05-01T21:38:47.363331: step 3095, loss 0.00116063, acc 1\n",
      "2018-05-01T21:38:47.379128: step 3096, loss 0.0575307, acc 0.96875\n",
      "2018-05-01T21:38:47.398234: step 3097, loss 0.0176961, acc 1\n",
      "2018-05-01T21:38:47.412815: step 3098, loss 0.00639961, acc 1\n",
      "2018-05-01T21:38:47.427280: step 3099, loss 0.00366009, acc 1\n",
      "2018-05-01T21:38:47.441647: step 3100, loss 0.000491537, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:47.446188: step 3100, loss 0.634662, acc 0.827586\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-3100\n",
      "\n",
      "2018-05-01T21:38:47.515707: step 3101, loss 0.000754422, acc 1\n",
      "2018-05-01T21:38:47.533433: step 3102, loss 0.00167225, acc 1\n",
      "2018-05-01T21:38:47.547910: step 3103, loss 0.00761967, acc 1\n",
      "2018-05-01T21:38:47.563584: step 3104, loss 0.00101121, acc 1\n",
      "2018-05-01T21:38:47.577760: step 3105, loss 0.00513927, acc 1\n",
      "2018-05-01T21:38:47.591163: step 3106, loss 0.00750554, acc 1\n",
      "2018-05-01T21:38:47.608015: step 3107, loss 0.0372822, acc 0.96875\n",
      "2018-05-01T21:38:47.627860: step 3108, loss 0.0436387, acc 1\n",
      "2018-05-01T21:38:47.644786: step 3109, loss 0.00021368, acc 1\n",
      "2018-05-01T21:38:47.662122: step 3110, loss 0.00984842, acc 1\n",
      "2018-05-01T21:38:47.674702: step 3111, loss 0.000220477, acc 1\n",
      "2018-05-01T21:38:47.689262: step 3112, loss 0.000417963, acc 1\n",
      "2018-05-01T21:38:47.703722: step 3113, loss 0.00875988, acc 1\n",
      "2018-05-01T21:38:47.717590: step 3114, loss 0.00019938, acc 1\n",
      "2018-05-01T21:38:47.732811: step 3115, loss 0.00303683, acc 1\n",
      "2018-05-01T21:38:47.748082: step 3116, loss 0.183924, acc 0.96875\n",
      "2018-05-01T21:38:47.764819: step 3117, loss 0.00286678, acc 1\n",
      "2018-05-01T21:38:47.779004: step 3118, loss 0.0271865, acc 1\n",
      "2018-05-01T21:38:47.791746: step 3119, loss 0.00148868, acc 1\n",
      "2018-05-01T21:38:47.806228: step 3120, loss 0.00935788, acc 1\n",
      "2018-05-01T21:38:47.824694: step 3121, loss 0.000603079, acc 1\n",
      "2018-05-01T21:38:47.846259: step 3122, loss 0.0939324, acc 0.96875\n",
      "2018-05-01T21:38:47.859735: step 3123, loss 0.00119972, acc 1\n",
      "2018-05-01T21:38:47.873251: step 3124, loss 0.00603954, acc 1\n",
      "2018-05-01T21:38:47.886353: step 3125, loss 0.000222424, acc 1\n",
      "2018-05-01T21:38:47.900436: step 3126, loss 0.0321254, acc 0.96875\n",
      "2018-05-01T21:38:47.917866: step 3127, loss 0.00129385, acc 1\n",
      "2018-05-01T21:38:47.939559: step 3128, loss 0.00339728, acc 1\n",
      "2018-05-01T21:38:47.952191: step 3129, loss 0.000952492, acc 1\n",
      "2018-05-01T21:38:47.967785: step 3130, loss 0.0145568, acc 1\n",
      "2018-05-01T21:38:47.983236: step 3131, loss 0.0339971, acc 0.96875\n",
      "2018-05-01T21:38:47.999412: step 3132, loss 0.0145858, acc 1\n",
      "2018-05-01T21:38:48.016952: step 3133, loss 0.0888817, acc 0.96875\n",
      "2018-05-01T21:38:48.032305: step 3134, loss 0.000306013, acc 1\n",
      "2018-05-01T21:38:48.052153: step 3135, loss 0.000549234, acc 1\n",
      "2018-05-01T21:38:48.071163: step 3136, loss 0.000595832, acc 1\n",
      "2018-05-01T21:38:48.088683: step 3137, loss 0.00276972, acc 1\n",
      "2018-05-01T21:38:48.114767: step 3138, loss 0.000362412, acc 1\n",
      "2018-05-01T21:38:48.143505: step 3139, loss 0.00131796, acc 1\n",
      "2018-05-01T21:38:48.172935: step 3140, loss 0.00170678, acc 1\n",
      "2018-05-01T21:38:48.195124: step 3141, loss 0.000486473, acc 1\n",
      "2018-05-01T21:38:48.230042: step 3142, loss 0.000509353, acc 1\n",
      "2018-05-01T21:38:48.252169: step 3143, loss 0.00874957, acc 1\n",
      "2018-05-01T21:38:48.278898: step 3144, loss 0.108228, acc 0.96875\n",
      "2018-05-01T21:38:48.306947: step 3145, loss 0.00615552, acc 1\n",
      "2018-05-01T21:38:48.321005: step 3146, loss 0.000974965, acc 1\n",
      "2018-05-01T21:38:48.335190: step 3147, loss 0.00358851, acc 1\n",
      "2018-05-01T21:38:48.348398: step 3148, loss 0.00345732, acc 1\n",
      "2018-05-01T21:38:48.362118: step 3149, loss 0.0299083, acc 1\n",
      "2018-05-01T21:38:48.374839: step 3150, loss 0.0969742, acc 0.944444\n",
      "2018-05-01T21:38:48.388176: step 3151, loss 0.0290601, acc 1\n",
      "2018-05-01T21:38:48.401878: step 3152, loss 0.210035, acc 0.9375\n",
      "2018-05-01T21:38:48.416184: step 3153, loss 0.000315153, acc 1\n",
      "2018-05-01T21:38:48.429493: step 3154, loss 0.0511412, acc 0.96875\n",
      "2018-05-01T21:38:48.444643: step 3155, loss 0.0845535, acc 0.96875\n",
      "2018-05-01T21:38:48.461970: step 3156, loss 0.0913819, acc 0.96875\n",
      "2018-05-01T21:38:48.481286: step 3157, loss 0.0108538, acc 1\n",
      "2018-05-01T21:38:48.497332: step 3158, loss 0.00127686, acc 1\n",
      "2018-05-01T21:38:48.519812: step 3159, loss 0.0677221, acc 0.96875\n",
      "2018-05-01T21:38:48.547265: step 3160, loss 0.00386925, acc 1\n",
      "2018-05-01T21:38:48.576804: step 3161, loss 0.000328778, acc 1\n",
      "2018-05-01T21:38:48.596463: step 3162, loss 0.0986554, acc 0.96875\n",
      "2018-05-01T21:38:48.610565: step 3163, loss 0.000775028, acc 1\n",
      "2018-05-01T21:38:48.625192: step 3164, loss 0.00522361, acc 1\n",
      "2018-05-01T21:38:48.639612: step 3165, loss 0.027976, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:48.656645: step 3166, loss 0.0372477, acc 0.96875\n",
      "2018-05-01T21:38:48.674518: step 3167, loss 0.000247508, acc 1\n",
      "2018-05-01T21:38:48.695044: step 3168, loss 0.00610041, acc 1\n",
      "2018-05-01T21:38:48.709700: step 3169, loss 0.00107533, acc 1\n",
      "2018-05-01T21:38:48.724026: step 3170, loss 0.00145726, acc 1\n",
      "2018-05-01T21:38:48.738274: step 3171, loss 0.00144254, acc 1\n",
      "2018-05-01T21:38:48.752082: step 3172, loss 0.00116916, acc 1\n",
      "2018-05-01T21:38:48.768373: step 3173, loss 0.00406813, acc 1\n",
      "2018-05-01T21:38:48.782546: step 3174, loss 0.000262733, acc 1\n",
      "2018-05-01T21:38:48.796008: step 3175, loss 0.00203887, acc 1\n",
      "2018-05-01T21:38:48.809167: step 3176, loss 0.000983599, acc 1\n",
      "2018-05-01T21:38:48.822010: step 3177, loss 4.04934e-06, acc 1\n",
      "2018-05-01T21:38:48.837279: step 3178, loss 0.000140784, acc 1\n",
      "2018-05-01T21:38:48.852057: step 3179, loss 0.00620093, acc 1\n",
      "2018-05-01T21:38:48.869040: step 3180, loss 0.0440563, acc 0.96875\n",
      "2018-05-01T21:38:48.886267: step 3181, loss 0.0744196, acc 0.96875\n",
      "2018-05-01T21:38:48.903683: step 3182, loss 0.125644, acc 0.9375\n",
      "2018-05-01T21:38:48.917095: step 3183, loss 0.0934851, acc 0.96875\n",
      "2018-05-01T21:38:48.933835: step 3184, loss 0.00151965, acc 1\n",
      "2018-05-01T21:38:48.950253: step 3185, loss 0.00124368, acc 1\n",
      "2018-05-01T21:38:48.964322: step 3186, loss 0.0111938, acc 1\n",
      "2018-05-01T21:38:48.977396: step 3187, loss 0.000163535, acc 1\n",
      "2018-05-01T21:38:48.990676: step 3188, loss 0.0443828, acc 0.96875\n",
      "2018-05-01T21:38:49.004344: step 3189, loss 0.00147461, acc 1\n",
      "2018-05-01T21:38:49.018530: step 3190, loss 0.0317094, acc 0.96875\n",
      "2018-05-01T21:38:49.031405: step 3191, loss 0.00209398, acc 1\n",
      "2018-05-01T21:38:49.044332: step 3192, loss 0.00439522, acc 1\n",
      "2018-05-01T21:38:49.057607: step 3193, loss 0.000164349, acc 1\n",
      "2018-05-01T21:38:49.071648: step 3194, loss 0.0540272, acc 0.96875\n",
      "2018-05-01T21:38:49.090402: step 3195, loss 0.000502359, acc 1\n",
      "2018-05-01T21:38:49.105086: step 3196, loss 0.0120154, acc 1\n",
      "2018-05-01T21:38:49.118518: step 3197, loss 0.0021177, acc 1\n",
      "2018-05-01T21:38:49.131346: step 3198, loss 0.00352377, acc 1\n",
      "2018-05-01T21:38:49.144752: step 3199, loss 0.00223725, acc 1\n",
      "2018-05-01T21:38:49.156774: step 3200, loss 0.00713749, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:49.161050: step 3200, loss 0.646723, acc 0.83908\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-3200\n",
      "\n",
      "2018-05-01T21:38:49.228328: step 3201, loss 0.092322, acc 0.96875\n",
      "2018-05-01T21:38:49.241383: step 3202, loss 0.00962032, acc 1\n",
      "2018-05-01T21:38:49.256946: step 3203, loss 0.00885244, acc 1\n",
      "2018-05-01T21:38:49.274402: step 3204, loss 0.000324124, acc 1\n",
      "2018-05-01T21:38:49.288355: step 3205, loss 0.00780153, acc 1\n",
      "2018-05-01T21:38:49.304613: step 3206, loss 0.0454019, acc 0.96875\n",
      "2018-05-01T21:38:49.321473: step 3207, loss 0.00235453, acc 1\n",
      "2018-05-01T21:38:49.334372: step 3208, loss 0.00202052, acc 1\n",
      "2018-05-01T21:38:49.348620: step 3209, loss 0.00300916, acc 1\n",
      "2018-05-01T21:38:49.362101: step 3210, loss 0.00110097, acc 1\n",
      "2018-05-01T21:38:49.375520: step 3211, loss 0.149053, acc 0.96875\n",
      "2018-05-01T21:38:49.388897: step 3212, loss 0.00799765, acc 1\n",
      "2018-05-01T21:38:49.405164: step 3213, loss 0.0147162, acc 1\n",
      "2018-05-01T21:38:49.418709: step 3214, loss 0.142158, acc 0.96875\n",
      "2018-05-01T21:38:49.432202: step 3215, loss 0.000543918, acc 1\n",
      "2018-05-01T21:38:49.447617: step 3216, loss 0.0188039, acc 1\n",
      "2018-05-01T21:38:49.461615: step 3217, loss 0.00180803, acc 1\n",
      "2018-05-01T21:38:49.475538: step 3218, loss 0.0955601, acc 0.96875\n",
      "2018-05-01T21:38:49.490558: step 3219, loss 0.00118807, acc 1\n",
      "2018-05-01T21:38:49.505856: step 3220, loss 0.00200934, acc 1\n",
      "2018-05-01T21:38:49.520290: step 3221, loss 0.00426079, acc 1\n",
      "2018-05-01T21:38:49.535051: step 3222, loss 0.000410568, acc 1\n",
      "2018-05-01T21:38:49.548579: step 3223, loss 0.0148721, acc 1\n",
      "2018-05-01T21:38:49.563810: step 3224, loss 0.000890141, acc 1\n",
      "2018-05-01T21:38:49.576915: step 3225, loss 0.000641486, acc 1\n",
      "2018-05-01T21:38:49.591896: step 3226, loss 0.00513654, acc 1\n",
      "2018-05-01T21:38:49.608054: step 3227, loss 0.00896582, acc 1\n",
      "2018-05-01T21:38:49.621500: step 3228, loss 0.00152875, acc 1\n",
      "2018-05-01T21:38:49.636809: step 3229, loss 0.000918643, acc 1\n",
      "2018-05-01T21:38:49.650140: step 3230, loss 0.11611, acc 0.96875\n",
      "2018-05-01T21:38:49.664044: step 3231, loss 0.00959273, acc 1\n",
      "2018-05-01T21:38:49.677337: step 3232, loss 0.204516, acc 0.9375\n",
      "2018-05-01T21:38:49.690885: step 3233, loss 0.0256597, acc 1\n",
      "2018-05-01T21:38:49.711937: step 3234, loss 9.20006e-05, acc 1\n",
      "2018-05-01T21:38:49.730573: step 3235, loss 0.00338921, acc 1\n",
      "2018-05-01T21:38:49.743982: step 3236, loss 0.00344687, acc 1\n",
      "2018-05-01T21:38:49.757313: step 3237, loss 0.000755246, acc 1\n",
      "2018-05-01T21:38:49.770115: step 3238, loss 0.00220516, acc 1\n",
      "2018-05-01T21:38:49.783230: step 3239, loss 0.00339766, acc 1\n",
      "2018-05-01T21:38:49.796482: step 3240, loss 0.000422363, acc 1\n",
      "2018-05-01T21:38:49.811211: step 3241, loss 0.00951082, acc 1\n",
      "2018-05-01T21:38:49.824283: step 3242, loss 0.0103177, acc 1\n",
      "2018-05-01T21:38:49.838293: step 3243, loss 0.141886, acc 0.96875\n",
      "2018-05-01T21:38:49.852440: step 3244, loss 0.00882008, acc 1\n",
      "2018-05-01T21:38:49.866505: step 3245, loss 0.00586492, acc 1\n",
      "2018-05-01T21:38:49.880677: step 3246, loss 0.0378052, acc 0.96875\n",
      "2018-05-01T21:38:49.894493: step 3247, loss 0.00110658, acc 1\n",
      "2018-05-01T21:38:49.908096: step 3248, loss 0.000347902, acc 1\n",
      "2018-05-01T21:38:49.924969: step 3249, loss 0.000536737, acc 1\n",
      "2018-05-01T21:38:49.943614: step 3250, loss 0.0375358, acc 1\n",
      "2018-05-01T21:38:49.958191: step 3251, loss 0.00229092, acc 1\n",
      "2018-05-01T21:38:49.972930: step 3252, loss 0.0296822, acc 1\n",
      "2018-05-01T21:38:49.986838: step 3253, loss 0.00181467, acc 1\n",
      "2018-05-01T21:38:50.001333: step 3254, loss 0.000695377, acc 1\n",
      "2018-05-01T21:38:50.015998: step 3255, loss 0.00560553, acc 1\n",
      "2018-05-01T21:38:50.030225: step 3256, loss 0.00183911, acc 1\n",
      "2018-05-01T21:38:50.044045: step 3257, loss 0.00208288, acc 1\n",
      "2018-05-01T21:38:50.059086: step 3258, loss 0.0316622, acc 0.96875\n",
      "2018-05-01T21:38:50.074158: step 3259, loss 0.027236, acc 0.96875\n",
      "2018-05-01T21:38:50.091101: step 3260, loss 0.0124868, acc 1\n",
      "2018-05-01T21:38:50.106984: step 3261, loss 0.000461524, acc 1\n",
      "2018-05-01T21:38:50.120628: step 3262, loss 0.00160102, acc 1\n",
      "2018-05-01T21:38:50.136599: step 3263, loss 0.0871608, acc 0.96875\n",
      "2018-05-01T21:38:50.154333: step 3264, loss 0.00210117, acc 1\n",
      "2018-05-01T21:38:50.172378: step 3265, loss 0.0445419, acc 0.96875\n",
      "2018-05-01T21:38:50.186091: step 3266, loss 0.0925496, acc 0.96875\n",
      "2018-05-01T21:38:50.201446: step 3267, loss 0.000159043, acc 1\n",
      "2018-05-01T21:38:50.215376: step 3268, loss 0.0323054, acc 0.96875\n",
      "2018-05-01T21:38:50.230301: step 3269, loss 0.0736374, acc 0.96875\n",
      "2018-05-01T21:38:50.246435: step 3270, loss 0.0309617, acc 0.96875\n",
      "2018-05-01T21:38:50.260747: step 3271, loss 0.0472957, acc 0.96875\n",
      "2018-05-01T21:38:50.275003: step 3272, loss 0.000614556, acc 1\n",
      "2018-05-01T21:38:50.287903: step 3273, loss 0.000451517, acc 1\n",
      "2018-05-01T21:38:50.301826: step 3274, loss 0.00107306, acc 1\n",
      "2018-05-01T21:38:50.315629: step 3275, loss 0.000118585, acc 1\n",
      "2018-05-01T21:38:50.327886: step 3276, loss 0.000395755, acc 1\n",
      "2018-05-01T21:38:50.341366: step 3277, loss 0.0645905, acc 0.96875\n",
      "2018-05-01T21:38:50.357968: step 3278, loss 0.0116057, acc 1\n",
      "2018-05-01T21:38:50.376527: step 3279, loss 0.000763705, acc 1\n",
      "2018-05-01T21:38:50.392243: step 3280, loss 0.00162913, acc 1\n",
      "2018-05-01T21:38:50.406402: step 3281, loss 0.000318984, acc 1\n",
      "2018-05-01T21:38:50.422172: step 3282, loss 0.0021511, acc 1\n",
      "2018-05-01T21:38:50.436344: step 3283, loss 0.140619, acc 0.96875\n",
      "2018-05-01T21:38:50.452328: step 3284, loss 0.000670987, acc 1\n",
      "2018-05-01T21:38:50.466877: step 3285, loss 0.00488487, acc 1\n",
      "2018-05-01T21:38:50.480494: step 3286, loss 0.0238928, acc 1\n",
      "2018-05-01T21:38:50.496329: step 3287, loss 0.000479234, acc 1\n",
      "2018-05-01T21:38:50.510302: step 3288, loss 0.00440684, acc 1\n",
      "2018-05-01T21:38:50.524266: step 3289, loss 0.199655, acc 0.9375\n",
      "2018-05-01T21:38:50.537114: step 3290, loss 0.00115064, acc 1\n",
      "2018-05-01T21:38:50.554334: step 3291, loss 0.000403919, acc 1\n",
      "2018-05-01T21:38:50.569531: step 3292, loss 0.000254929, acc 1\n",
      "2018-05-01T21:38:50.587458: step 3293, loss 0.0069906, acc 1\n",
      "2018-05-01T21:38:50.601166: step 3294, loss 0.00245785, acc 1\n",
      "2018-05-01T21:38:50.615244: step 3295, loss 0.00273739, acc 1\n",
      "2018-05-01T21:38:50.628664: step 3296, loss 0.000662709, acc 1\n",
      "2018-05-01T21:38:50.642511: step 3297, loss 0.00415181, acc 1\n",
      "2018-05-01T21:38:50.655607: step 3298, loss 0.070306, acc 0.96875\n",
      "2018-05-01T21:38:50.669332: step 3299, loss 0.00247853, acc 1\n",
      "2018-05-01T21:38:50.682115: step 3300, loss 0.000434191, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:50.686285: step 3300, loss 0.651931, acc 0.827586\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-3300\n",
      "\n",
      "2018-05-01T21:38:50.749508: step 3301, loss 0.000851914, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:50.766634: step 3302, loss 0.0736085, acc 0.96875\n",
      "2018-05-01T21:38:50.784340: step 3303, loss 0.293298, acc 0.90625\n",
      "2018-05-01T21:38:50.798868: step 3304, loss 9.36065e-05, acc 1\n",
      "2018-05-01T21:38:50.813225: step 3305, loss 0.00215618, acc 1\n",
      "2018-05-01T21:38:50.826678: step 3306, loss 0.018179, acc 1\n",
      "2018-05-01T21:38:50.839852: step 3307, loss 0.00432604, acc 1\n",
      "2018-05-01T21:38:50.853941: step 3308, loss 0.000613762, acc 1\n",
      "2018-05-01T21:38:50.869118: step 3309, loss 0.00509029, acc 1\n",
      "2018-05-01T21:38:50.882334: step 3310, loss 0.00057066, acc 1\n",
      "2018-05-01T21:38:50.896414: step 3311, loss 0.005997, acc 1\n",
      "2018-05-01T21:38:50.910070: step 3312, loss 0.00209757, acc 1\n",
      "2018-05-01T21:38:50.924454: step 3313, loss 0.00420198, acc 1\n",
      "2018-05-01T21:38:50.938888: step 3314, loss 0.0067635, acc 1\n",
      "2018-05-01T21:38:50.951511: step 3315, loss 0.000548594, acc 1\n",
      "2018-05-01T21:38:50.964853: step 3316, loss 0.117942, acc 0.96875\n",
      "2018-05-01T21:38:50.980510: step 3317, loss 0.00130316, acc 1\n",
      "2018-05-01T21:38:51.001250: step 3318, loss 0.00118982, acc 1\n",
      "2018-05-01T21:38:51.015396: step 3319, loss 0.000523379, acc 1\n",
      "2018-05-01T21:38:51.029058: step 3320, loss 0.00143801, acc 1\n",
      "2018-05-01T21:38:51.042287: step 3321, loss 7.98721e-05, acc 1\n",
      "2018-05-01T21:38:51.055933: step 3322, loss 0.0226861, acc 0.96875\n",
      "2018-05-01T21:38:51.070422: step 3323, loss 0.00957143, acc 1\n",
      "2018-05-01T21:38:51.083800: step 3324, loss 0.000311979, acc 1\n",
      "2018-05-01T21:38:51.096284: step 3325, loss 0.000638045, acc 1\n",
      "2018-05-01T21:38:51.109997: step 3326, loss 0.0128914, acc 1\n",
      "2018-05-01T21:38:51.123719: step 3327, loss 0.00394961, acc 1\n",
      "2018-05-01T21:38:51.136859: step 3328, loss 0.000619472, acc 1\n",
      "2018-05-01T21:38:51.150054: step 3329, loss 0.00590237, acc 1\n",
      "2018-05-01T21:38:51.165049: step 3330, loss 0.00357306, acc 1\n",
      "2018-05-01T21:38:51.179157: step 3331, loss 0.000177744, acc 1\n",
      "2018-05-01T21:38:51.193898: step 3332, loss 0.0098225, acc 1\n",
      "2018-05-01T21:38:51.211926: step 3333, loss 0.00134168, acc 1\n",
      "2018-05-01T21:38:51.226087: step 3334, loss 0.000231423, acc 1\n",
      "2018-05-01T21:38:51.239459: step 3335, loss 0.000445302, acc 1\n",
      "2018-05-01T21:38:51.252804: step 3336, loss 0.0349293, acc 1\n",
      "2018-05-01T21:38:51.268125: step 3337, loss 0.000147381, acc 1\n",
      "2018-05-01T21:38:51.282519: step 3338, loss 0.0791941, acc 0.96875\n",
      "2018-05-01T21:38:51.295889: step 3339, loss 0.00107417, acc 1\n",
      "2018-05-01T21:38:51.308859: step 3340, loss 0.00159352, acc 1\n",
      "2018-05-01T21:38:51.322174: step 3341, loss 0.154668, acc 0.96875\n",
      "2018-05-01T21:38:51.335314: step 3342, loss 0.0104846, acc 1\n",
      "2018-05-01T21:38:51.349171: step 3343, loss 0.0115993, acc 1\n",
      "2018-05-01T21:38:51.361933: step 3344, loss 0.0017594, acc 1\n",
      "2018-05-01T21:38:51.376252: step 3345, loss 0.000649241, acc 1\n",
      "2018-05-01T21:38:51.389651: step 3346, loss 0.0038336, acc 1\n",
      "2018-05-01T21:38:51.409361: step 3347, loss 0.00299698, acc 1\n",
      "2018-05-01T21:38:51.425602: step 3348, loss 0.0150197, acc 1\n",
      "2018-05-01T21:38:51.439984: step 3349, loss 0.0244492, acc 1\n",
      "2018-05-01T21:38:51.454248: step 3350, loss 0.00229443, acc 1\n",
      "2018-05-01T21:38:51.468340: step 3351, loss 0.00481162, acc 1\n",
      "2018-05-01T21:38:51.481339: step 3352, loss 0.0240059, acc 1\n",
      "2018-05-01T21:38:51.494761: step 3353, loss 0.0231825, acc 0.96875\n",
      "2018-05-01T21:38:51.508172: step 3354, loss 0.00164145, acc 1\n",
      "2018-05-01T21:38:51.521647: step 3355, loss 0.01857, acc 1\n",
      "2018-05-01T21:38:51.536701: step 3356, loss 0.0016195, acc 1\n",
      "2018-05-01T21:38:51.550430: step 3357, loss 0.00179665, acc 1\n",
      "2018-05-01T21:38:51.563020: step 3358, loss 0.00171235, acc 1\n",
      "2018-05-01T21:38:51.576079: step 3359, loss 7.61743e-05, acc 1\n",
      "2018-05-01T21:38:51.590204: step 3360, loss 0.000116208, acc 1\n",
      "2018-05-01T21:38:51.604098: step 3361, loss 0.00328213, acc 1\n",
      "2018-05-01T21:38:51.622615: step 3362, loss 0.000537455, acc 1\n",
      "2018-05-01T21:38:51.640410: step 3363, loss 0.0291384, acc 0.96875\n",
      "2018-05-01T21:38:51.657098: step 3364, loss 0.000585802, acc 1\n",
      "2018-05-01T21:38:51.671034: step 3365, loss 0.00122208, acc 1\n",
      "2018-05-01T21:38:51.684711: step 3366, loss 0.00109, acc 1\n",
      "2018-05-01T21:38:51.698315: step 3367, loss 0.0393772, acc 0.96875\n",
      "2018-05-01T21:38:51.712649: step 3368, loss 0.028115, acc 0.96875\n",
      "2018-05-01T21:38:51.726531: step 3369, loss 0.000212531, acc 1\n",
      "2018-05-01T21:38:51.741951: step 3370, loss 0.00607728, acc 1\n",
      "2018-05-01T21:38:51.756262: step 3371, loss 0.0589342, acc 0.96875\n",
      "2018-05-01T21:38:51.769320: step 3372, loss 0.00344036, acc 1\n",
      "2018-05-01T21:38:51.781992: step 3373, loss 0.00112811, acc 1\n",
      "2018-05-01T21:38:51.796739: step 3374, loss 0.000196976, acc 1\n",
      "2018-05-01T21:38:51.809098: step 3375, loss 0.00990955, acc 1\n",
      "2018-05-01T21:38:51.822360: step 3376, loss 0.00065587, acc 1\n",
      "2018-05-01T21:38:51.838264: step 3377, loss 0.00506443, acc 1\n",
      "2018-05-01T21:38:51.854274: step 3378, loss 0.000534159, acc 1\n",
      "2018-05-01T21:38:51.868701: step 3379, loss 0.0010065, acc 1\n",
      "2018-05-01T21:38:51.881897: step 3380, loss 0.000513782, acc 1\n",
      "2018-05-01T21:38:51.894836: step 3381, loss 0.000528276, acc 1\n",
      "2018-05-01T21:38:51.908449: step 3382, loss 0.0211564, acc 1\n",
      "2018-05-01T21:38:51.921987: step 3383, loss 0.00328838, acc 1\n",
      "2018-05-01T21:38:51.937522: step 3384, loss 0.00413427, acc 1\n",
      "2018-05-01T21:38:51.951181: step 3385, loss 0.00212196, acc 1\n",
      "2018-05-01T21:38:51.964787: step 3386, loss 0.0707697, acc 0.96875\n",
      "2018-05-01T21:38:51.978561: step 3387, loss 0.0563313, acc 0.96875\n",
      "2018-05-01T21:38:51.995097: step 3388, loss 0.011391, acc 1\n",
      "2018-05-01T21:38:52.008071: step 3389, loss 0.00286088, acc 1\n",
      "2018-05-01T21:38:52.021277: step 3390, loss 0.00110619, acc 1\n",
      "2018-05-01T21:38:52.036813: step 3391, loss 0.000239057, acc 1\n",
      "2018-05-01T21:38:52.050694: step 3392, loss 0.00282827, acc 1\n",
      "2018-05-01T21:38:52.066705: step 3393, loss 0.0907699, acc 0.96875\n",
      "2018-05-01T21:38:52.082082: step 3394, loss 0.0114139, acc 1\n",
      "2018-05-01T21:38:52.096322: step 3395, loss 0.00718579, acc 1\n",
      "2018-05-01T21:38:52.109836: step 3396, loss 0.10111, acc 0.96875\n",
      "2018-05-01T21:38:52.123627: step 3397, loss 0.00755472, acc 1\n",
      "2018-05-01T21:38:52.137715: step 3398, loss 0.00107894, acc 1\n",
      "2018-05-01T21:38:52.150902: step 3399, loss 0.00256513, acc 1\n",
      "2018-05-01T21:38:52.165976: step 3400, loss 0.116584, acc 0.944444\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:52.170383: step 3400, loss 0.682789, acc 0.83908\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-3400\n",
      "\n",
      "2018-05-01T21:38:52.236055: step 3401, loss 0.00047876, acc 1\n",
      "2018-05-01T21:38:52.249028: step 3402, loss 0.000377452, acc 1\n",
      "2018-05-01T21:38:52.265627: step 3403, loss 0.00226928, acc 1\n",
      "2018-05-01T21:38:52.283728: step 3404, loss 0.0961944, acc 0.96875\n",
      "2018-05-01T21:38:52.298111: step 3405, loss 0.163711, acc 0.96875\n",
      "2018-05-01T21:38:52.312048: step 3406, loss 0.0111007, acc 1\n",
      "2018-05-01T21:38:52.324797: step 3407, loss 0.046733, acc 0.96875\n",
      "2018-05-01T21:38:52.337135: step 3408, loss 0.216642, acc 0.9375\n",
      "2018-05-01T21:38:52.351240: step 3409, loss 0.0011853, acc 1\n",
      "2018-05-01T21:38:52.364528: step 3410, loss 0.00466686, acc 1\n",
      "2018-05-01T21:38:52.378251: step 3411, loss 0.0666397, acc 0.96875\n",
      "2018-05-01T21:38:52.392943: step 3412, loss 0.00213892, acc 1\n",
      "2018-05-01T21:38:52.406507: step 3413, loss 0.0003616, acc 1\n",
      "2018-05-01T21:38:52.420376: step 3414, loss 0.00256006, acc 1\n",
      "2018-05-01T21:38:52.433974: step 3415, loss 8.58829e-05, acc 1\n",
      "2018-05-01T21:38:52.447040: step 3416, loss 0.000974966, acc 1\n",
      "2018-05-01T21:38:52.461157: step 3417, loss 0.0249639, acc 0.96875\n",
      "2018-05-01T21:38:52.480395: step 3418, loss 0.139048, acc 0.96875\n",
      "2018-05-01T21:38:52.496362: step 3419, loss 0.000183268, acc 1\n",
      "2018-05-01T21:38:52.512494: step 3420, loss 0.0159996, acc 1\n",
      "2018-05-01T21:38:52.528324: step 3421, loss 0.00839353, acc 1\n",
      "2018-05-01T21:38:52.543145: step 3422, loss 0.00113193, acc 1\n",
      "2018-05-01T21:38:52.557883: step 3423, loss 0.0751503, acc 0.9375\n",
      "2018-05-01T21:38:52.574542: step 3424, loss 0.0025883, acc 1\n",
      "2018-05-01T21:38:52.588668: step 3425, loss 0.0382048, acc 1\n",
      "2018-05-01T21:38:52.602293: step 3426, loss 0.00368757, acc 1\n",
      "2018-05-01T21:38:52.616521: step 3427, loss 0.0163182, acc 1\n",
      "2018-05-01T21:38:52.630110: step 3428, loss 5.29479e-05, acc 1\n",
      "2018-05-01T21:38:52.643756: step 3429, loss 0.064381, acc 0.96875\n",
      "2018-05-01T21:38:52.658201: step 3430, loss 0.000490834, acc 1\n",
      "2018-05-01T21:38:52.671115: step 3431, loss 0.000325206, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:52.685614: step 3432, loss 0.000763409, acc 1\n",
      "2018-05-01T21:38:52.699509: step 3433, loss 0.00103159, acc 1\n",
      "2018-05-01T21:38:52.715591: step 3434, loss 0.000639282, acc 1\n",
      "2018-05-01T21:38:52.732097: step 3435, loss 0.0937018, acc 0.96875\n",
      "2018-05-01T21:38:52.746126: step 3436, loss 0.000179847, acc 1\n",
      "2018-05-01T21:38:52.759505: step 3437, loss 0.00123907, acc 1\n",
      "2018-05-01T21:38:52.773410: step 3438, loss 0.00110239, acc 1\n",
      "2018-05-01T21:38:52.787269: step 3439, loss 0.0196386, acc 1\n",
      "2018-05-01T21:38:52.801419: step 3440, loss 0.0280369, acc 0.96875\n",
      "2018-05-01T21:38:52.815424: step 3441, loss 0.0222385, acc 0.96875\n",
      "2018-05-01T21:38:52.828064: step 3442, loss 0.00375847, acc 1\n",
      "2018-05-01T21:38:52.842980: step 3443, loss 0.0035664, acc 1\n",
      "2018-05-01T21:38:52.858254: step 3444, loss 0.0150222, acc 1\n",
      "2018-05-01T21:38:52.873595: step 3445, loss 0.000577814, acc 1\n",
      "2018-05-01T21:38:52.888317: step 3446, loss 0.000168195, acc 1\n",
      "2018-05-01T21:38:52.905230: step 3447, loss 0.000183935, acc 1\n",
      "2018-05-01T21:38:52.925674: step 3448, loss 0.00193994, acc 1\n",
      "2018-05-01T21:38:52.939171: step 3449, loss 0.0026795, acc 1\n",
      "2018-05-01T21:38:52.950903: step 3450, loss 0.361913, acc 0.888889\n",
      "2018-05-01T21:38:52.968688: step 3451, loss 0.00305936, acc 1\n",
      "2018-05-01T21:38:52.985725: step 3452, loss 0.0124042, acc 1\n",
      "2018-05-01T21:38:53.005572: step 3453, loss 0.00193179, acc 1\n",
      "2018-05-01T21:38:53.018965: step 3454, loss 0.0019515, acc 1\n",
      "2018-05-01T21:38:53.034163: step 3455, loss 0.0350094, acc 0.96875\n",
      "2018-05-01T21:38:53.048185: step 3456, loss 0.00661432, acc 1\n",
      "2018-05-01T21:38:53.061719: step 3457, loss 0.00279116, acc 1\n",
      "2018-05-01T21:38:53.075244: step 3458, loss 0.0880283, acc 0.96875\n",
      "2018-05-01T21:38:53.089457: step 3459, loss 0.000578039, acc 1\n",
      "2018-05-01T21:38:53.116839: step 3460, loss 0.000446004, acc 1\n",
      "2018-05-01T21:38:53.133778: step 3461, loss 0.0132561, acc 1\n",
      "2018-05-01T21:38:53.148795: step 3462, loss 0.00148421, acc 1\n",
      "2018-05-01T21:38:53.163326: step 3463, loss 0.0602154, acc 0.96875\n",
      "2018-05-01T21:38:53.178633: step 3464, loss 0.112029, acc 0.96875\n",
      "2018-05-01T21:38:53.193276: step 3465, loss 0.00221092, acc 1\n",
      "2018-05-01T21:38:53.207443: step 3466, loss 0.00707724, acc 1\n",
      "2018-05-01T21:38:53.222841: step 3467, loss 0.0660483, acc 0.96875\n",
      "2018-05-01T21:38:53.239511: step 3468, loss 0.00188779, acc 1\n",
      "2018-05-01T21:38:53.254926: step 3469, loss 0.0882952, acc 0.96875\n",
      "2018-05-01T21:38:53.268807: step 3470, loss 0.00132347, acc 1\n",
      "2018-05-01T21:38:53.282498: step 3471, loss 0.0778699, acc 0.96875\n",
      "2018-05-01T21:38:53.297440: step 3472, loss 0.0018848, acc 1\n",
      "2018-05-01T21:38:53.310937: step 3473, loss 0.00250061, acc 1\n",
      "2018-05-01T21:38:53.328477: step 3474, loss 0.0010481, acc 1\n",
      "2018-05-01T21:38:53.345557: step 3475, loss 0.00334278, acc 1\n",
      "2018-05-01T21:38:53.360878: step 3476, loss 0.000385574, acc 1\n",
      "2018-05-01T21:38:53.375368: step 3477, loss 0.00184892, acc 1\n",
      "2018-05-01T21:38:53.390560: step 3478, loss 0.0222415, acc 1\n",
      "2018-05-01T21:38:53.404379: step 3479, loss 0.0126226, acc 1\n",
      "2018-05-01T21:38:53.417641: step 3480, loss 0.00312113, acc 1\n",
      "2018-05-01T21:38:53.432689: step 3481, loss 0.00108651, acc 1\n",
      "2018-05-01T21:38:53.447340: step 3482, loss 0.014754, acc 1\n",
      "2018-05-01T21:38:53.462267: step 3483, loss 0.00482716, acc 1\n",
      "2018-05-01T21:38:53.476111: step 3484, loss 0.00892248, acc 1\n",
      "2018-05-01T21:38:53.491031: step 3485, loss 0.000244183, acc 1\n",
      "2018-05-01T21:38:53.504817: step 3486, loss 0.0290866, acc 0.96875\n",
      "2018-05-01T21:38:53.518574: step 3487, loss 0.0490242, acc 0.96875\n",
      "2018-05-01T21:38:53.533773: step 3488, loss 0.000464666, acc 1\n",
      "2018-05-01T21:38:53.556419: step 3489, loss 0.00166971, acc 1\n",
      "2018-05-01T21:38:53.570583: step 3490, loss 0.00184746, acc 1\n",
      "2018-05-01T21:38:53.583949: step 3491, loss 0.00220942, acc 1\n",
      "2018-05-01T21:38:53.597934: step 3492, loss 0.000347865, acc 1\n",
      "2018-05-01T21:38:53.611509: step 3493, loss 0.0033986, acc 1\n",
      "2018-05-01T21:38:53.625381: step 3494, loss 0.000679864, acc 1\n",
      "2018-05-01T21:38:53.640010: step 3495, loss 0.290391, acc 0.9375\n",
      "2018-05-01T21:38:53.654125: step 3496, loss 0.00132786, acc 1\n",
      "2018-05-01T21:38:53.668419: step 3497, loss 0.143143, acc 0.96875\n",
      "2018-05-01T21:38:53.681531: step 3498, loss 0.00129239, acc 1\n",
      "2018-05-01T21:38:53.695856: step 3499, loss 0.000114815, acc 1\n",
      "2018-05-01T21:38:53.709085: step 3500, loss 0.00301956, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:53.713251: step 3500, loss 0.69958, acc 0.816092\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-3500\n",
      "\n",
      "2018-05-01T21:38:53.785197: step 3501, loss 0.000423132, acc 1\n",
      "2018-05-01T21:38:53.798598: step 3502, loss 0.0138308, acc 1\n",
      "2018-05-01T21:38:53.811523: step 3503, loss 0.000919365, acc 1\n",
      "2018-05-01T21:38:53.824530: step 3504, loss 0.00237619, acc 1\n",
      "2018-05-01T21:38:53.838641: step 3505, loss 0.00133504, acc 1\n",
      "2018-05-01T21:38:53.851776: step 3506, loss 9.56484e-05, acc 1\n",
      "2018-05-01T21:38:53.865308: step 3507, loss 0.00321583, acc 1\n",
      "2018-05-01T21:38:53.878106: step 3508, loss 0.00243769, acc 1\n",
      "2018-05-01T21:38:53.892565: step 3509, loss 0.00664518, acc 1\n",
      "2018-05-01T21:38:53.905198: step 3510, loss 0.00124916, acc 1\n",
      "2018-05-01T21:38:53.920982: step 3511, loss 0.0591621, acc 0.96875\n",
      "2018-05-01T21:38:53.934412: step 3512, loss 0.160017, acc 0.96875\n",
      "2018-05-01T21:38:53.948177: step 3513, loss 0.00597215, acc 1\n",
      "2018-05-01T21:38:53.961711: step 3514, loss 0.00721974, acc 1\n",
      "2018-05-01T21:38:53.978819: step 3515, loss 0.0513886, acc 0.96875\n",
      "2018-05-01T21:38:53.995995: step 3516, loss 0.00604292, acc 1\n",
      "2018-05-01T21:38:54.016927: step 3517, loss 0.00580128, acc 1\n",
      "2018-05-01T21:38:54.044688: step 3518, loss 0.0586913, acc 0.96875\n",
      "2018-05-01T21:38:54.061888: step 3519, loss 0.00246189, acc 1\n",
      "2018-05-01T21:38:54.076498: step 3520, loss 0.0202434, acc 1\n",
      "2018-05-01T21:38:54.094706: step 3521, loss 0.000849234, acc 1\n",
      "2018-05-01T21:38:54.111726: step 3522, loss 0.000578031, acc 1\n",
      "2018-05-01T21:38:54.128666: step 3523, loss 0.000106875, acc 1\n",
      "2018-05-01T21:38:54.145079: step 3524, loss 0.00045699, acc 1\n",
      "2018-05-01T21:38:54.159311: step 3525, loss 0.000253335, acc 1\n",
      "2018-05-01T21:38:54.175388: step 3526, loss 0.0474117, acc 0.96875\n",
      "2018-05-01T21:38:54.194029: step 3527, loss 0.00363659, acc 1\n",
      "2018-05-01T21:38:54.216044: step 3528, loss 0.0254914, acc 0.96875\n",
      "2018-05-01T21:38:54.238653: step 3529, loss 0.0370553, acc 0.96875\n",
      "2018-05-01T21:38:54.256300: step 3530, loss 0.00070931, acc 1\n",
      "2018-05-01T21:38:54.275296: step 3531, loss 0.000165296, acc 1\n",
      "2018-05-01T21:38:54.292480: step 3532, loss 0.00125937, acc 1\n",
      "2018-05-01T21:38:54.306705: step 3533, loss 0.00216069, acc 1\n",
      "2018-05-01T21:38:54.325963: step 3534, loss 0.00164082, acc 1\n",
      "2018-05-01T21:38:54.343457: step 3535, loss 0.00155992, acc 1\n",
      "2018-05-01T21:38:54.361482: step 3536, loss 0.000128184, acc 1\n",
      "2018-05-01T21:38:54.379640: step 3537, loss 0.00228575, acc 1\n",
      "2018-05-01T21:38:54.396471: step 3538, loss 0.000300888, acc 1\n",
      "2018-05-01T21:38:54.423762: step 3539, loss 0.00794468, acc 1\n",
      "2018-05-01T21:38:54.440829: step 3540, loss 0.00146823, acc 1\n",
      "2018-05-01T21:38:54.455779: step 3541, loss 0.014394, acc 1\n",
      "2018-05-01T21:38:54.472277: step 3542, loss 0.00084733, acc 1\n",
      "2018-05-01T21:38:54.490807: step 3543, loss 0.177102, acc 0.96875\n",
      "2018-05-01T21:38:54.506052: step 3544, loss 0.000160943, acc 1\n",
      "2018-05-01T21:38:54.521958: step 3545, loss 0.0290281, acc 0.96875\n",
      "2018-05-01T21:38:54.538255: step 3546, loss 0.00148847, acc 1\n",
      "2018-05-01T21:38:54.555073: step 3547, loss 0.00487838, acc 1\n",
      "2018-05-01T21:38:54.571736: step 3548, loss 0.00065222, acc 1\n",
      "2018-05-01T21:38:54.592376: step 3549, loss 0.126006, acc 0.96875\n",
      "2018-05-01T21:38:54.606745: step 3550, loss 0.000383427, acc 1\n",
      "2018-05-01T21:38:54.628535: step 3551, loss 0.00171321, acc 1\n",
      "2018-05-01T21:38:54.650718: step 3552, loss 0.00636585, acc 1\n",
      "2018-05-01T21:38:54.668868: step 3553, loss 0.00154506, acc 1\n",
      "2018-05-01T21:38:54.688026: step 3554, loss 0.000292333, acc 1\n",
      "2018-05-01T21:38:54.703941: step 3555, loss 0.00281736, acc 1\n",
      "2018-05-01T21:38:54.719493: step 3556, loss 0.0724034, acc 0.96875\n",
      "2018-05-01T21:38:54.737072: step 3557, loss 0.113452, acc 0.9375\n",
      "2018-05-01T21:38:54.752663: step 3558, loss 0.0010742, acc 1\n",
      "2018-05-01T21:38:54.767187: step 3559, loss 0.000340181, acc 1\n",
      "2018-05-01T21:38:54.784409: step 3560, loss 0.00213801, acc 1\n",
      "2018-05-01T21:38:54.801717: step 3561, loss 0.000319403, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:54.822576: step 3562, loss 0.00076416, acc 1\n",
      "2018-05-01T21:38:54.845012: step 3563, loss 0.0140638, acc 1\n",
      "2018-05-01T21:38:54.860312: step 3564, loss 0.00597586, acc 1\n",
      "2018-05-01T21:38:54.877642: step 3565, loss 0.0921348, acc 0.96875\n",
      "2018-05-01T21:38:54.897991: step 3566, loss 0.0343938, acc 1\n",
      "2018-05-01T21:38:54.913041: step 3567, loss 0.0252674, acc 0.96875\n",
      "2018-05-01T21:38:54.928419: step 3568, loss 0.00171482, acc 1\n",
      "2018-05-01T21:38:54.944172: step 3569, loss 0.0150026, acc 1\n",
      "2018-05-01T21:38:54.960726: step 3570, loss 0.000304788, acc 1\n",
      "2018-05-01T21:38:54.981289: step 3571, loss 0.0207851, acc 1\n",
      "2018-05-01T21:38:55.021411: step 3572, loss 0.000180484, acc 1\n",
      "2018-05-01T21:38:55.046703: step 3573, loss 0.00129829, acc 1\n",
      "2018-05-01T21:38:55.065606: step 3574, loss 0.0851177, acc 0.96875\n",
      "2018-05-01T21:38:55.088227: step 3575, loss 0.000451363, acc 1\n",
      "2018-05-01T21:38:55.107727: step 3576, loss 0.122579, acc 0.96875\n",
      "2018-05-01T21:38:55.126801: step 3577, loss 0.00032421, acc 1\n",
      "2018-05-01T21:38:55.144538: step 3578, loss 0.0043883, acc 1\n",
      "2018-05-01T21:38:55.162395: step 3579, loss 0.0241106, acc 1\n",
      "2018-05-01T21:38:55.178156: step 3580, loss 0.0057846, acc 1\n",
      "2018-05-01T21:38:55.195456: step 3581, loss 0.160284, acc 0.96875\n",
      "2018-05-01T21:38:55.211321: step 3582, loss 0.000596384, acc 1\n",
      "2018-05-01T21:38:55.226861: step 3583, loss 0.038813, acc 0.96875\n",
      "2018-05-01T21:38:55.240643: step 3584, loss 0.000523151, acc 1\n",
      "2018-05-01T21:38:55.257427: step 3585, loss 0.000122875, acc 1\n",
      "2018-05-01T21:38:55.278841: step 3586, loss 0.070524, acc 0.9375\n",
      "2018-05-01T21:38:55.294320: step 3587, loss 0.00345844, acc 1\n",
      "2018-05-01T21:38:55.309113: step 3588, loss 0.000343228, acc 1\n",
      "2018-05-01T21:38:55.324303: step 3589, loss 0.000692411, acc 1\n",
      "2018-05-01T21:38:55.341555: step 3590, loss 0.00332838, acc 1\n",
      "2018-05-01T21:38:55.358575: step 3591, loss 0.000337026, acc 1\n",
      "2018-05-01T21:38:55.374423: step 3592, loss 0.0100678, acc 1\n",
      "2018-05-01T21:38:55.388388: step 3593, loss 0.000235815, acc 1\n",
      "2018-05-01T21:38:55.404141: step 3594, loss 0.00132348, acc 1\n",
      "2018-05-01T21:38:55.419690: step 3595, loss 0.00882116, acc 1\n",
      "2018-05-01T21:38:55.434101: step 3596, loss 0.00310648, acc 1\n",
      "2018-05-01T21:38:55.449677: step 3597, loss 0.00244685, acc 1\n",
      "2018-05-01T21:38:55.468214: step 3598, loss 0.0010501, acc 1\n",
      "2018-05-01T21:38:55.487233: step 3599, loss 0.0932496, acc 0.96875\n",
      "2018-05-01T21:38:55.499363: step 3600, loss 0.000749959, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:55.504204: step 3600, loss 0.66797, acc 0.83908\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-3600\n",
      "\n",
      "2018-05-01T21:38:55.571478: step 3601, loss 2.95013e-05, acc 1\n",
      "2018-05-01T21:38:55.586326: step 3602, loss 0.0186868, acc 1\n",
      "2018-05-01T21:38:55.600894: step 3603, loss 0.020237, acc 1\n",
      "2018-05-01T21:38:55.616859: step 3604, loss 0.00094403, acc 1\n",
      "2018-05-01T21:38:55.630821: step 3605, loss 0.0017567, acc 1\n",
      "2018-05-01T21:38:55.646024: step 3606, loss 0.0607579, acc 0.96875\n",
      "2018-05-01T21:38:55.659901: step 3607, loss 0.0579233, acc 0.96875\n",
      "2018-05-01T21:38:55.674961: step 3608, loss 0.00602103, acc 1\n",
      "2018-05-01T21:38:55.691613: step 3609, loss 0.000652905, acc 1\n",
      "2018-05-01T21:38:55.707151: step 3610, loss 0.000416438, acc 1\n",
      "2018-05-01T21:38:55.722142: step 3611, loss 0.0197201, acc 1\n",
      "2018-05-01T21:38:55.736472: step 3612, loss 0.000223657, acc 1\n",
      "2018-05-01T21:38:55.750246: step 3613, loss 0.002134, acc 1\n",
      "2018-05-01T21:38:55.763734: step 3614, loss 0.00645502, acc 1\n",
      "2018-05-01T21:38:55.777351: step 3615, loss 0.00556396, acc 1\n",
      "2018-05-01T21:38:55.791684: step 3616, loss 0.0159618, acc 1\n",
      "2018-05-01T21:38:55.805553: step 3617, loss 0.00322031, acc 1\n",
      "2018-05-01T21:38:55.820612: step 3618, loss 0.169684, acc 0.96875\n",
      "2018-05-01T21:38:55.834930: step 3619, loss 0.000322775, acc 1\n",
      "2018-05-01T21:38:55.850597: step 3620, loss 0.065948, acc 0.96875\n",
      "2018-05-01T21:38:55.865683: step 3621, loss 0.00685094, acc 1\n",
      "2018-05-01T21:38:55.879645: step 3622, loss 0.0547783, acc 0.96875\n",
      "2018-05-01T21:38:55.895909: step 3623, loss 0.0220494, acc 1\n",
      "2018-05-01T21:38:55.912252: step 3624, loss 0.00770413, acc 1\n",
      "2018-05-01T21:38:55.925060: step 3625, loss 0.000108901, acc 1\n",
      "2018-05-01T21:38:55.938856: step 3626, loss 0.000100979, acc 1\n",
      "2018-05-01T21:38:55.952316: step 3627, loss 0.0183587, acc 1\n",
      "2018-05-01T21:38:55.967199: step 3628, loss 0.0143061, acc 1\n",
      "2018-05-01T21:38:55.982464: step 3629, loss 0.0111825, acc 1\n",
      "2018-05-01T21:38:55.995998: step 3630, loss 0.000564502, acc 1\n",
      "2018-05-01T21:38:56.011936: step 3631, loss 0.00572398, acc 1\n",
      "2018-05-01T21:38:56.028435: step 3632, loss 0.000352761, acc 1\n",
      "2018-05-01T21:38:56.042543: step 3633, loss 0.0333142, acc 1\n",
      "2018-05-01T21:38:56.057438: step 3634, loss 0.0218845, acc 1\n",
      "2018-05-01T21:38:56.072315: step 3635, loss 0.00830522, acc 1\n",
      "2018-05-01T21:38:56.086849: step 3636, loss 0.000248639, acc 1\n",
      "2018-05-01T21:38:56.105578: step 3637, loss 0.000820969, acc 1\n",
      "2018-05-01T21:38:56.122510: step 3638, loss 0.000102333, acc 1\n",
      "2018-05-01T21:38:56.137972: step 3639, loss 0.00434632, acc 1\n",
      "2018-05-01T21:38:56.151003: step 3640, loss 0.000135025, acc 1\n",
      "2018-05-01T21:38:56.165581: step 3641, loss 0.120859, acc 0.96875\n",
      "2018-05-01T21:38:56.181264: step 3642, loss 0.0088036, acc 1\n",
      "2018-05-01T21:38:56.195307: step 3643, loss 0.00179212, acc 1\n",
      "2018-05-01T21:38:56.208525: step 3644, loss 0.136605, acc 0.96875\n",
      "2018-05-01T21:38:56.221956: step 3645, loss 0.000171664, acc 1\n",
      "2018-05-01T21:38:56.236290: step 3646, loss 0.000250228, acc 1\n",
      "2018-05-01T21:38:56.251364: step 3647, loss 0.0390145, acc 0.96875\n",
      "2018-05-01T21:38:56.265508: step 3648, loss 0.0125021, acc 1\n",
      "2018-05-01T21:38:56.280815: step 3649, loss 0.00435673, acc 1\n",
      "2018-05-01T21:38:56.297401: step 3650, loss 0.0122191, acc 1\n",
      "2018-05-01T21:38:56.315486: step 3651, loss 0.00804292, acc 1\n",
      "2018-05-01T21:38:56.328790: step 3652, loss 0.0380631, acc 0.96875\n",
      "2018-05-01T21:38:56.342530: step 3653, loss 0.000929172, acc 1\n",
      "2018-05-01T21:38:56.354720: step 3654, loss 0.000685175, acc 1\n",
      "2018-05-01T21:38:56.368284: step 3655, loss 0.120341, acc 0.96875\n",
      "2018-05-01T21:38:56.381590: step 3656, loss 0.00123319, acc 1\n",
      "2018-05-01T21:38:56.395710: step 3657, loss 0.0378385, acc 0.96875\n",
      "2018-05-01T21:38:56.409482: step 3658, loss 0.00039362, acc 1\n",
      "2018-05-01T21:38:56.423530: step 3659, loss 0.00214676, acc 1\n",
      "2018-05-01T21:38:56.437445: step 3660, loss 0.00944842, acc 1\n",
      "2018-05-01T21:38:56.452657: step 3661, loss 0.00304875, acc 1\n",
      "2018-05-01T21:38:56.467768: step 3662, loss 0.00154055, acc 1\n",
      "2018-05-01T21:38:56.481695: step 3663, loss 0.0378524, acc 0.96875\n",
      "2018-05-01T21:38:56.495508: step 3664, loss 0.000405522, acc 1\n",
      "2018-05-01T21:38:56.514608: step 3665, loss 0.0802798, acc 0.96875\n",
      "2018-05-01T21:38:56.533808: step 3666, loss 0.00393805, acc 1\n",
      "2018-05-01T21:38:56.547925: step 3667, loss 0.0113641, acc 1\n",
      "2018-05-01T21:38:56.560757: step 3668, loss 0.000646181, acc 1\n",
      "2018-05-01T21:38:56.573954: step 3669, loss 0.0147572, acc 1\n",
      "2018-05-01T21:38:56.587145: step 3670, loss 0.00346185, acc 1\n",
      "2018-05-01T21:38:56.600956: step 3671, loss 0.000372849, acc 1\n",
      "2018-05-01T21:38:56.613959: step 3672, loss 0.00206916, acc 1\n",
      "2018-05-01T21:38:56.628546: step 3673, loss 0.0890486, acc 0.96875\n",
      "2018-05-01T21:38:56.641914: step 3674, loss 0.000454538, acc 1\n",
      "2018-05-01T21:38:56.654905: step 3675, loss 0.00271625, acc 1\n",
      "2018-05-01T21:38:56.668472: step 3676, loss 0.00878171, acc 1\n",
      "2018-05-01T21:38:56.682423: step 3677, loss 0.00634032, acc 1\n",
      "2018-05-01T21:38:56.696533: step 3678, loss 0.000166772, acc 1\n",
      "2018-05-01T21:38:56.709750: step 3679, loss 0.0104099, acc 1\n",
      "2018-05-01T21:38:56.726934: step 3680, loss 0.00384703, acc 1\n",
      "2018-05-01T21:38:56.743548: step 3681, loss 0.000507607, acc 1\n",
      "2018-05-01T21:38:56.760076: step 3682, loss 0.0281466, acc 0.96875\n",
      "2018-05-01T21:38:56.773617: step 3683, loss 0.00155244, acc 1\n",
      "2018-05-01T21:38:56.787048: step 3684, loss 0.00438263, acc 1\n",
      "2018-05-01T21:38:56.800966: step 3685, loss 0.000777646, acc 1\n",
      "2018-05-01T21:38:56.814562: step 3686, loss 0.00196792, acc 1\n",
      "2018-05-01T21:38:56.827153: step 3687, loss 0.000900978, acc 1\n",
      "2018-05-01T21:38:56.841520: step 3688, loss 0.00330188, acc 1\n",
      "2018-05-01T21:38:56.854790: step 3689, loss 0.00366078, acc 1\n",
      "2018-05-01T21:38:56.868691: step 3690, loss 0.0429617, acc 0.96875\n",
      "2018-05-01T21:38:56.882836: step 3691, loss 0.150242, acc 0.96875\n",
      "2018-05-01T21:38:56.896140: step 3692, loss 0.000822257, acc 1\n",
      "2018-05-01T21:38:56.909601: step 3693, loss 0.00588524, acc 1\n",
      "2018-05-01T21:38:56.923091: step 3694, loss 0.000307758, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:56.940726: step 3695, loss 0.0617877, acc 0.96875\n",
      "2018-05-01T21:38:56.957047: step 3696, loss 0.11304, acc 0.9375\n",
      "2018-05-01T21:38:56.974349: step 3697, loss 0.0589658, acc 0.96875\n",
      "2018-05-01T21:38:56.988113: step 3698, loss 0.000744067, acc 1\n",
      "2018-05-01T21:38:57.000804: step 3699, loss 0.1621, acc 0.96875\n",
      "2018-05-01T21:38:57.012660: step 3700, loss 0.000123065, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:57.016735: step 3700, loss 0.659697, acc 0.827586\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-3700\n",
      "\n",
      "2018-05-01T21:38:57.079578: step 3701, loss 0.0116813, acc 1\n",
      "2018-05-01T21:38:57.094766: step 3702, loss 0.000570925, acc 1\n",
      "2018-05-01T21:38:57.108527: step 3703, loss 0.000873823, acc 1\n",
      "2018-05-01T21:38:57.122677: step 3704, loss 0.000285515, acc 1\n",
      "2018-05-01T21:38:57.137175: step 3705, loss 0.0034789, acc 1\n",
      "2018-05-01T21:38:57.152626: step 3706, loss 0.000211973, acc 1\n",
      "2018-05-01T21:38:57.172766: step 3707, loss 0.126876, acc 0.96875\n",
      "2018-05-01T21:38:57.189807: step 3708, loss 0.000943439, acc 1\n",
      "2018-05-01T21:38:57.203400: step 3709, loss 0.00682802, acc 1\n",
      "2018-05-01T21:38:57.216211: step 3710, loss 0.0640656, acc 0.96875\n",
      "2018-05-01T21:38:57.229097: step 3711, loss 0.000400951, acc 1\n",
      "2018-05-01T21:38:57.242717: step 3712, loss 0.04281, acc 0.96875\n",
      "2018-05-01T21:38:57.255429: step 3713, loss 0.00462847, acc 1\n",
      "2018-05-01T21:38:57.268783: step 3714, loss 5.52561e-05, acc 1\n",
      "2018-05-01T21:38:57.281995: step 3715, loss 0.00972727, acc 1\n",
      "2018-05-01T21:38:57.296002: step 3716, loss 0.00898115, acc 1\n",
      "2018-05-01T21:38:57.309642: step 3717, loss 0.0010329, acc 1\n",
      "2018-05-01T21:38:57.323752: step 3718, loss 0.00136805, acc 1\n",
      "2018-05-01T21:38:57.336793: step 3719, loss 0.000462185, acc 1\n",
      "2018-05-01T21:38:57.350105: step 3720, loss 0.00118548, acc 1\n",
      "2018-05-01T21:38:57.365729: step 3721, loss 0.137028, acc 0.96875\n",
      "2018-05-01T21:38:57.381766: step 3722, loss 0.000127214, acc 1\n",
      "2018-05-01T21:38:57.396010: step 3723, loss 0.000677236, acc 1\n",
      "2018-05-01T21:38:57.409972: step 3724, loss 0.00547467, acc 1\n",
      "2018-05-01T21:38:57.421758: step 3725, loss 0.000231964, acc 1\n",
      "2018-05-01T21:38:57.436458: step 3726, loss 0.000494444, acc 1\n",
      "2018-05-01T21:38:57.450506: step 3727, loss 0.142602, acc 0.9375\n",
      "2018-05-01T21:38:57.465221: step 3728, loss 0.00187315, acc 1\n",
      "2018-05-01T21:38:57.479512: step 3729, loss 0.0223584, acc 1\n",
      "2018-05-01T21:38:57.493984: step 3730, loss 0.00326225, acc 1\n",
      "2018-05-01T21:38:57.511772: step 3731, loss 0.00767498, acc 1\n",
      "2018-05-01T21:38:57.731578: step 3732, loss 0.000759588, acc 1\n",
      "2018-05-01T21:38:57.810424: step 3733, loss 0.00148685, acc 1\n",
      "2018-05-01T21:38:57.824662: step 3734, loss 0.00136145, acc 1\n",
      "2018-05-01T21:38:57.838254: step 3735, loss 0.059595, acc 0.96875\n",
      "2018-05-01T21:38:57.851678: step 3736, loss 0.00805215, acc 1\n",
      "2018-05-01T21:38:57.866029: step 3737, loss 0.000341595, acc 1\n",
      "2018-05-01T21:38:57.879747: step 3738, loss 0.00822582, acc 1\n",
      "2018-05-01T21:38:57.893043: step 3739, loss 0.0438869, acc 0.96875\n",
      "2018-05-01T21:38:57.905940: step 3740, loss 0.00137692, acc 1\n",
      "2018-05-01T21:38:57.918579: step 3741, loss 0.035698, acc 0.96875\n",
      "2018-05-01T21:38:57.934063: step 3742, loss 0.000628329, acc 1\n",
      "2018-05-01T21:38:57.949009: step 3743, loss 0.0214428, acc 1\n",
      "2018-05-01T21:38:57.961647: step 3744, loss 0.000107798, acc 1\n",
      "2018-05-01T21:38:57.974774: step 3745, loss 0.00076477, acc 1\n",
      "2018-05-01T21:38:57.988714: step 3746, loss 0.0166582, acc 1\n",
      "2018-05-01T21:38:58.006492: step 3747, loss 0.0261654, acc 1\n",
      "2018-05-01T21:38:58.022914: step 3748, loss 0.00160661, acc 1\n",
      "2018-05-01T21:38:58.036633: step 3749, loss 0.026619, acc 0.96875\n",
      "2018-05-01T21:38:58.049453: step 3750, loss 0.000594906, acc 1\n",
      "2018-05-01T21:38:58.063974: step 3751, loss 8.62425e-05, acc 1\n",
      "2018-05-01T21:38:58.077029: step 3752, loss 0.00165032, acc 1\n",
      "2018-05-01T21:38:58.090114: step 3753, loss 0.00141613, acc 1\n",
      "2018-05-01T21:38:58.103597: step 3754, loss 0.0289373, acc 0.96875\n",
      "2018-05-01T21:38:58.117600: step 3755, loss 0.0284601, acc 0.96875\n",
      "2018-05-01T21:38:58.131250: step 3756, loss 0.00108396, acc 1\n",
      "2018-05-01T21:38:58.144415: step 3757, loss 0.0386103, acc 0.96875\n",
      "2018-05-01T21:38:58.157801: step 3758, loss 0.00113184, acc 1\n",
      "2018-05-01T21:38:58.172265: step 3759, loss 0.0263371, acc 1\n",
      "2018-05-01T21:38:58.185820: step 3760, loss 0.00134031, acc 1\n",
      "2018-05-01T21:38:58.198732: step 3761, loss 0.00114818, acc 1\n",
      "2018-05-01T21:38:58.213286: step 3762, loss 0.000541503, acc 1\n",
      "2018-05-01T21:38:58.234352: step 3763, loss 0.00358853, acc 1\n",
      "2018-05-01T21:38:58.248652: step 3764, loss 0.0302352, acc 0.96875\n",
      "2018-05-01T21:38:58.262802: step 3765, loss 0.0132826, acc 1\n",
      "2018-05-01T21:38:58.276371: step 3766, loss 0.00477627, acc 1\n",
      "2018-05-01T21:38:58.289875: step 3767, loss 0.000734546, acc 1\n",
      "2018-05-01T21:38:58.303026: step 3768, loss 0.000103092, acc 1\n",
      "2018-05-01T21:38:58.316314: step 3769, loss 0.0668187, acc 0.96875\n",
      "2018-05-01T21:38:58.329888: step 3770, loss 0.00952608, acc 1\n",
      "2018-05-01T21:38:58.344842: step 3771, loss 0.00119794, acc 1\n",
      "2018-05-01T21:38:58.358090: step 3772, loss 0.018719, acc 1\n",
      "2018-05-01T21:38:58.371191: step 3773, loss 0.0186531, acc 1\n",
      "2018-05-01T21:38:58.384913: step 3774, loss 0.108194, acc 0.9375\n",
      "2018-05-01T21:38:58.397078: step 3775, loss 2.42967e-05, acc 1\n",
      "2018-05-01T21:38:58.410366: step 3776, loss 0.0102204, acc 1\n",
      "2018-05-01T21:38:58.428658: step 3777, loss 0.00676118, acc 1\n",
      "2018-05-01T21:38:58.445859: step 3778, loss 0.0115801, acc 1\n",
      "2018-05-01T21:38:58.459343: step 3779, loss 0.00037296, acc 1\n",
      "2018-05-01T21:38:58.472843: step 3780, loss 0.00135175, acc 1\n",
      "2018-05-01T21:38:58.486201: step 3781, loss 0.00643138, acc 1\n",
      "2018-05-01T21:38:58.499296: step 3782, loss 0.040455, acc 0.96875\n",
      "2018-05-01T21:38:58.513494: step 3783, loss 0.00556839, acc 1\n",
      "2018-05-01T21:38:58.526233: step 3784, loss 0.000827817, acc 1\n",
      "2018-05-01T21:38:58.541750: step 3785, loss 0.0842716, acc 0.96875\n",
      "2018-05-01T21:38:58.555704: step 3786, loss 0.132605, acc 0.96875\n",
      "2018-05-01T21:38:58.569517: step 3787, loss 0.000316053, acc 1\n",
      "2018-05-01T21:38:58.583187: step 3788, loss 0.000640622, acc 1\n",
      "2018-05-01T21:38:58.596346: step 3789, loss 0.00236978, acc 1\n",
      "2018-05-01T21:38:58.610302: step 3790, loss 0.0292075, acc 0.96875\n",
      "2018-05-01T21:38:58.624389: step 3791, loss 0.000822486, acc 1\n",
      "2018-05-01T21:38:58.643358: step 3792, loss 0.00177925, acc 1\n",
      "2018-05-01T21:38:58.660059: step 3793, loss 0.000494385, acc 1\n",
      "2018-05-01T21:38:58.673351: step 3794, loss 0.0211056, acc 1\n",
      "2018-05-01T21:38:58.687022: step 3795, loss 0.107928, acc 0.96875\n",
      "2018-05-01T21:38:58.700823: step 3796, loss 0.000779358, acc 1\n",
      "2018-05-01T21:38:58.714068: step 3797, loss 0.00761254, acc 1\n",
      "2018-05-01T21:38:58.726599: step 3798, loss 0.0663562, acc 0.96875\n",
      "2018-05-01T21:38:58.741038: step 3799, loss 0.00071636, acc 1\n",
      "2018-05-01T21:38:58.753080: step 3800, loss 0.00313592, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:38:58.757275: step 3800, loss 0.669183, acc 0.816092\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-3800\n",
      "\n",
      "2018-05-01T21:38:58.823506: step 3801, loss 0.0120725, acc 1\n",
      "2018-05-01T21:38:58.837587: step 3802, loss 0.00116437, acc 1\n",
      "2018-05-01T21:38:58.854859: step 3803, loss 0.0738867, acc 0.96875\n",
      "2018-05-01T21:38:58.871983: step 3804, loss 0.0013073, acc 1\n",
      "2018-05-01T21:38:58.886283: step 3805, loss 0.00915233, acc 1\n",
      "2018-05-01T21:38:58.899914: step 3806, loss 0.000711249, acc 1\n",
      "2018-05-01T21:38:58.913756: step 3807, loss 0.0540744, acc 0.96875\n",
      "2018-05-01T21:38:58.926536: step 3808, loss 0.00043491, acc 1\n",
      "2018-05-01T21:38:58.940659: step 3809, loss 0.0412537, acc 0.96875\n",
      "2018-05-01T21:38:58.954576: step 3810, loss 0.0164821, acc 1\n",
      "2018-05-01T21:38:58.967901: step 3811, loss 0.00133502, acc 1\n",
      "2018-05-01T21:38:58.981198: step 3812, loss 0.0247741, acc 0.96875\n",
      "2018-05-01T21:38:58.995280: step 3813, loss 0.0030308, acc 1\n",
      "2018-05-01T21:38:59.008065: step 3814, loss 0.00043928, acc 1\n",
      "2018-05-01T21:38:59.021462: step 3815, loss 0.00824462, acc 1\n",
      "2018-05-01T21:38:59.034204: step 3816, loss 0.000196124, acc 1\n",
      "2018-05-01T21:38:59.047603: step 3817, loss 0.0258413, acc 1\n",
      "2018-05-01T21:38:59.063418: step 3818, loss 0.000448138, acc 1\n",
      "2018-05-01T21:38:59.080807: step 3819, loss 0.0011676, acc 1\n",
      "2018-05-01T21:38:59.099375: step 3820, loss 0.0203004, acc 1\n",
      "2018-05-01T21:38:59.114267: step 3821, loss 0.000568905, acc 1\n",
      "2018-05-01T21:38:59.127702: step 3822, loss 0.000330661, acc 1\n",
      "2018-05-01T21:38:59.141421: step 3823, loss 0.0855956, acc 0.96875\n",
      "2018-05-01T21:38:59.155258: step 3824, loss 0.000489451, acc 1\n",
      "2018-05-01T21:38:59.167375: step 3825, loss 0.000175298, acc 1\n",
      "2018-05-01T21:38:59.181081: step 3826, loss 0.00103308, acc 1\n",
      "2018-05-01T21:38:59.198375: step 3827, loss 0.0866443, acc 0.96875\n",
      "2018-05-01T21:38:59.213341: step 3828, loss 0.0157876, acc 1\n",
      "2018-05-01T21:38:59.226957: step 3829, loss 0.0182384, acc 1\n",
      "2018-05-01T21:38:59.240135: step 3830, loss 0.000225022, acc 1\n",
      "2018-05-01T21:38:59.253587: step 3831, loss 0.000935809, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:38:59.267311: step 3832, loss 0.00942769, acc 1\n",
      "2018-05-01T21:38:59.284542: step 3833, loss 0.0874729, acc 0.96875\n",
      "2018-05-01T21:38:59.301595: step 3834, loss 0.00338791, acc 1\n",
      "2018-05-01T21:38:59.316160: step 3835, loss 0.00288736, acc 1\n",
      "2018-05-01T21:38:59.330290: step 3836, loss 0.0193059, acc 1\n",
      "2018-05-01T21:38:59.344382: step 3837, loss 0.000512644, acc 1\n",
      "2018-05-01T21:38:59.358781: step 3838, loss 0.0919622, acc 0.96875\n",
      "2018-05-01T21:38:59.372965: step 3839, loss 0.000507127, acc 1\n",
      "2018-05-01T21:38:59.387228: step 3840, loss 0.000282089, acc 1\n",
      "2018-05-01T21:38:59.402130: step 3841, loss 0.0402452, acc 0.96875\n",
      "2018-05-01T21:38:59.414924: step 3842, loss 0.0231641, acc 1\n",
      "2018-05-01T21:38:59.428333: step 3843, loss 0.000493075, acc 1\n",
      "2018-05-01T21:38:59.444006: step 3844, loss 0.00164558, acc 1\n",
      "2018-05-01T21:38:59.457996: step 3845, loss 0.000104898, acc 1\n",
      "2018-05-01T21:38:59.471160: step 3846, loss 0.00696518, acc 1\n",
      "2018-05-01T21:38:59.486921: step 3847, loss 0.00593428, acc 1\n",
      "2018-05-01T21:38:59.502566: step 3848, loss 0.00426486, acc 1\n",
      "2018-05-01T21:38:59.517444: step 3849, loss 0.000802796, acc 1\n",
      "2018-05-01T21:38:59.529501: step 3850, loss 0.000142227, acc 1\n",
      "2018-05-01T21:38:59.543060: step 3851, loss 0.000446797, acc 1\n",
      "2018-05-01T21:38:59.557142: step 3852, loss 0.0296958, acc 0.96875\n",
      "2018-05-01T21:38:59.570845: step 3853, loss 0.105886, acc 0.96875\n",
      "2018-05-01T21:38:59.584573: step 3854, loss 0.0413681, acc 0.96875\n",
      "2018-05-01T21:38:59.598826: step 3855, loss 0.00154395, acc 1\n",
      "2018-05-01T21:38:59.612926: step 3856, loss 0.032812, acc 0.96875\n",
      "2018-05-01T21:38:59.626776: step 3857, loss 0.000409493, acc 1\n",
      "2018-05-01T21:38:59.640273: step 3858, loss 0.00283981, acc 1\n",
      "2018-05-01T21:38:59.653701: step 3859, loss 0.00406664, acc 1\n",
      "2018-05-01T21:38:59.666931: step 3860, loss 0.000507105, acc 1\n",
      "2018-05-01T21:38:59.682562: step 3861, loss 0.000237662, acc 1\n",
      "2018-05-01T21:38:59.700151: step 3862, loss 0.0658643, acc 0.96875\n",
      "2018-05-01T21:38:59.714510: step 3863, loss 0.0154863, acc 1\n",
      "2018-05-01T21:38:59.727513: step 3864, loss 0.0130152, acc 1\n",
      "2018-05-01T21:38:59.740629: step 3865, loss 0.000274081, acc 1\n",
      "2018-05-01T21:38:59.753620: step 3866, loss 0.00097552, acc 1\n",
      "2018-05-01T21:38:59.767666: step 3867, loss 0.0023218, acc 1\n",
      "2018-05-01T21:38:59.780415: step 3868, loss 0.010696, acc 1\n",
      "2018-05-01T21:38:59.793396: step 3869, loss 0.000629902, acc 1\n",
      "2018-05-01T21:38:59.806754: step 3870, loss 0.0548718, acc 0.96875\n",
      "2018-05-01T21:38:59.824408: step 3871, loss 0.00108328, acc 1\n",
      "2018-05-01T21:38:59.838758: step 3872, loss 0.00173092, acc 1\n",
      "2018-05-01T21:38:59.851686: step 3873, loss 0.000889234, acc 1\n",
      "2018-05-01T21:38:59.865449: step 3874, loss 0.0540017, acc 0.96875\n",
      "2018-05-01T21:38:59.877695: step 3875, loss 0.000543178, acc 1\n",
      "2018-05-01T21:38:59.896369: step 3876, loss 0.00094914, acc 1\n",
      "2018-05-01T21:38:59.915851: step 3877, loss 0.00335181, acc 1\n",
      "2018-05-01T21:38:59.929164: step 3878, loss 0.00292253, acc 1\n",
      "2018-05-01T21:38:59.943721: step 3879, loss 0.000267414, acc 1\n",
      "2018-05-01T21:38:59.957272: step 3880, loss 0.0211742, acc 1\n",
      "2018-05-01T21:38:59.969801: step 3881, loss 0.0205029, acc 1\n",
      "2018-05-01T21:38:59.982996: step 3882, loss 0.0301425, acc 0.96875\n",
      "2018-05-01T21:38:59.997949: step 3883, loss 0.0216737, acc 1\n",
      "2018-05-01T21:39:00.012770: step 3884, loss 0.000315155, acc 1\n",
      "2018-05-01T21:39:00.026139: step 3885, loss 0.000203445, acc 1\n",
      "2018-05-01T21:39:00.039726: step 3886, loss 0.00120277, acc 1\n",
      "2018-05-01T21:39:00.053759: step 3887, loss 0.000471853, acc 1\n",
      "2018-05-01T21:39:00.066829: step 3888, loss 0.0635557, acc 0.9375\n",
      "2018-05-01T21:39:00.079868: step 3889, loss 0.0253347, acc 0.96875\n",
      "2018-05-01T21:39:00.094053: step 3890, loss 0.00531014, acc 1\n",
      "2018-05-01T21:39:00.112732: step 3891, loss 0.0271259, acc 0.96875\n",
      "2018-05-01T21:39:00.130927: step 3892, loss 0.000390996, acc 1\n",
      "2018-05-01T21:39:00.145405: step 3893, loss 0.000623281, acc 1\n",
      "2018-05-01T21:39:00.158890: step 3894, loss 0.00792927, acc 1\n",
      "2018-05-01T21:39:00.172903: step 3895, loss 0.000406925, acc 1\n",
      "2018-05-01T21:39:00.187611: step 3896, loss 0.0706904, acc 0.96875\n",
      "2018-05-01T21:39:00.200483: step 3897, loss 0.00608592, acc 1\n",
      "2018-05-01T21:39:00.213461: step 3898, loss 0.000793707, acc 1\n",
      "2018-05-01T21:39:00.226684: step 3899, loss 0.00113802, acc 1\n",
      "2018-05-01T21:39:00.238932: step 3900, loss 0.143877, acc 0.944444\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:39:00.243386: step 3900, loss 0.68499, acc 0.83908\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-3900\n",
      "\n",
      "2018-05-01T21:39:00.307327: step 3901, loss 0.00459612, acc 1\n",
      "2018-05-01T21:39:00.323685: step 3902, loss 0.00105243, acc 1\n",
      "2018-05-01T21:39:00.340463: step 3903, loss 0.00459849, acc 1\n",
      "2018-05-01T21:39:00.354424: step 3904, loss 0.00142223, acc 1\n",
      "2018-05-01T21:39:00.369559: step 3905, loss 0.0649418, acc 0.96875\n",
      "2018-05-01T21:39:00.383538: step 3906, loss 0.00270632, acc 1\n",
      "2018-05-01T21:39:00.396724: step 3907, loss 0.00120706, acc 1\n",
      "2018-05-01T21:39:00.410191: step 3908, loss 0.00691554, acc 1\n",
      "2018-05-01T21:39:00.424925: step 3909, loss 0.000473959, acc 1\n",
      "2018-05-01T21:39:00.438034: step 3910, loss 0.000623484, acc 1\n",
      "2018-05-01T21:39:00.451045: step 3911, loss 0.0835889, acc 0.96875\n",
      "2018-05-01T21:39:00.464408: step 3912, loss 0.000615472, acc 1\n",
      "2018-05-01T21:39:00.478049: step 3913, loss 0.00123858, acc 1\n",
      "2018-05-01T21:39:00.490771: step 3914, loss 0.000604956, acc 1\n",
      "2018-05-01T21:39:00.504162: step 3915, loss 0.00068981, acc 1\n",
      "2018-05-01T21:39:00.518424: step 3916, loss 6.58391e-05, acc 1\n",
      "2018-05-01T21:39:00.545549: step 3917, loss 0.000130542, acc 1\n",
      "2018-05-01T21:39:00.562338: step 3918, loss 0.0538394, acc 0.96875\n",
      "2018-05-01T21:39:00.576002: step 3919, loss 0.0236882, acc 0.96875\n",
      "2018-05-01T21:39:00.592245: step 3920, loss 0.00460269, acc 1\n",
      "2018-05-01T21:39:00.607125: step 3921, loss 0.137251, acc 0.96875\n",
      "2018-05-01T21:39:00.619811: step 3922, loss 0.0061034, acc 1\n",
      "2018-05-01T21:39:00.632740: step 3923, loss 0.00819528, acc 1\n",
      "2018-05-01T21:39:00.645403: step 3924, loss 0.016206, acc 1\n",
      "2018-05-01T21:39:00.658052: step 3925, loss 0.000349252, acc 1\n",
      "2018-05-01T21:39:00.672406: step 3926, loss 0.00181292, acc 1\n",
      "2018-05-01T21:39:00.686383: step 3927, loss 0.0035665, acc 1\n",
      "2018-05-01T21:39:00.699727: step 3928, loss 9.4496e-05, acc 1\n",
      "2018-05-01T21:39:00.712523: step 3929, loss 0.000120411, acc 1\n",
      "2018-05-01T21:39:00.726855: step 3930, loss 0.0241933, acc 0.96875\n",
      "2018-05-01T21:39:00.740904: step 3931, loss 0.0068217, acc 1\n",
      "2018-05-01T21:39:00.757190: step 3932, loss 0.0058685, acc 1\n",
      "2018-05-01T21:39:00.777996: step 3933, loss 0.00560504, acc 1\n",
      "2018-05-01T21:39:00.791413: step 3934, loss 0.00155917, acc 1\n",
      "2018-05-01T21:39:00.806249: step 3935, loss 0.0168255, acc 1\n",
      "2018-05-01T21:39:00.819551: step 3936, loss 0.000127954, acc 1\n",
      "2018-05-01T21:39:00.833485: step 3937, loss 0.00181848, acc 1\n",
      "2018-05-01T21:39:00.846637: step 3938, loss 0.00206223, acc 1\n",
      "2018-05-01T21:39:00.860267: step 3939, loss 0.00698081, acc 1\n",
      "2018-05-01T21:39:00.874102: step 3940, loss 0.000929115, acc 1\n",
      "2018-05-01T21:39:00.888730: step 3941, loss 0.165555, acc 0.9375\n",
      "2018-05-01T21:39:00.901949: step 3942, loss 0.0018549, acc 1\n",
      "2018-05-01T21:39:00.915613: step 3943, loss 0.0369773, acc 0.96875\n",
      "2018-05-01T21:39:00.929202: step 3944, loss 0.000299753, acc 1\n",
      "2018-05-01T21:39:00.942515: step 3945, loss 0.0254216, acc 0.96875\n",
      "2018-05-01T21:39:00.956275: step 3946, loss 0.0156262, acc 1\n",
      "2018-05-01T21:39:00.972299: step 3947, loss 0.00782999, acc 1\n",
      "2018-05-01T21:39:00.988476: step 3948, loss 0.000374199, acc 1\n",
      "2018-05-01T21:39:01.002619: step 3949, loss 0.00937359, acc 1\n",
      "2018-05-01T21:39:01.015431: step 3950, loss 0.0211645, acc 1\n",
      "2018-05-01T21:39:01.028822: step 3951, loss 0.0252112, acc 0.96875\n",
      "2018-05-01T21:39:01.042760: step 3952, loss 0.000928512, acc 1\n",
      "2018-05-01T21:39:01.056592: step 3953, loss 0.00885626, acc 1\n",
      "2018-05-01T21:39:01.073229: step 3954, loss 0.000567929, acc 1\n",
      "2018-05-01T21:39:01.087349: step 3955, loss 0.0113389, acc 1\n",
      "2018-05-01T21:39:01.101118: step 3956, loss 0.000447748, acc 1\n",
      "2018-05-01T21:39:01.116319: step 3957, loss 0.00317899, acc 1\n",
      "2018-05-01T21:39:01.129437: step 3958, loss 0.00789564, acc 1\n",
      "2018-05-01T21:39:01.143224: step 3959, loss 0.0526577, acc 0.96875\n",
      "2018-05-01T21:39:01.156578: step 3960, loss 0.0666101, acc 0.96875\n",
      "2018-05-01T21:39:01.170362: step 3961, loss 0.000197763, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:39:01.187806: step 3962, loss 0.00068548, acc 1\n",
      "2018-05-01T21:39:01.206042: step 3963, loss 0.000350948, acc 1\n",
      "2018-05-01T21:39:01.220534: step 3964, loss 0.000374238, acc 1\n",
      "2018-05-01T21:39:01.234813: step 3965, loss 0.00138615, acc 1\n",
      "2018-05-01T21:39:01.248980: step 3966, loss 0.00374792, acc 1\n",
      "2018-05-01T21:39:01.262662: step 3967, loss 0.0404355, acc 1\n",
      "2018-05-01T21:39:01.276962: step 3968, loss 0.0173138, acc 1\n",
      "2018-05-01T21:39:01.289994: step 3969, loss 0.000196932, acc 1\n",
      "2018-05-01T21:39:01.305389: step 3970, loss 0.000120927, acc 1\n",
      "2018-05-01T21:39:01.318704: step 3971, loss 0.029304, acc 0.96875\n",
      "2018-05-01T21:39:01.332948: step 3972, loss 0.0711707, acc 0.96875\n",
      "2018-05-01T21:39:01.347194: step 3973, loss 0.00171745, acc 1\n",
      "2018-05-01T21:39:01.360136: step 3974, loss 0.000205382, acc 1\n",
      "2018-05-01T21:39:01.372626: step 3975, loss 0.000969371, acc 1\n",
      "2018-05-01T21:39:01.385707: step 3976, loss 6.11961e-05, acc 1\n",
      "2018-05-01T21:39:01.399966: step 3977, loss 0.0750312, acc 0.96875\n",
      "2018-05-01T21:39:01.414649: step 3978, loss 0.0153044, acc 1\n",
      "2018-05-01T21:39:01.430607: step 3979, loss 0.00096991, acc 1\n",
      "2018-05-01T21:39:01.443923: step 3980, loss 0.002179, acc 1\n",
      "2018-05-01T21:39:01.457487: step 3981, loss 0.0131854, acc 1\n",
      "2018-05-01T21:39:01.471544: step 3982, loss 0.000319054, acc 1\n",
      "2018-05-01T21:39:01.485244: step 3983, loss 0.00192202, acc 1\n",
      "2018-05-01T21:39:01.499072: step 3984, loss 0.000164832, acc 1\n",
      "2018-05-01T21:39:01.511582: step 3985, loss 0.000400849, acc 1\n",
      "2018-05-01T21:39:01.524566: step 3986, loss 0.045211, acc 0.96875\n",
      "2018-05-01T21:39:01.537752: step 3987, loss 0.105631, acc 0.96875\n",
      "2018-05-01T21:39:01.551776: step 3988, loss 0.000272544, acc 1\n",
      "2018-05-01T21:39:01.568062: step 3989, loss 0.00129784, acc 1\n",
      "2018-05-01T21:39:01.580476: step 3990, loss 0.0363175, acc 0.96875\n",
      "2018-05-01T21:39:01.594624: step 3991, loss 0.0960703, acc 0.96875\n",
      "2018-05-01T21:39:01.611433: step 3992, loss 0.000255815, acc 1\n",
      "2018-05-01T21:39:01.628349: step 3993, loss 0.000386479, acc 1\n",
      "2018-05-01T21:39:01.641591: step 3994, loss 0.000899141, acc 1\n",
      "2018-05-01T21:39:01.654949: step 3995, loss 0.00263092, acc 1\n",
      "2018-05-01T21:39:01.670534: step 3996, loss 0.0182812, acc 1\n",
      "2018-05-01T21:39:01.684149: step 3997, loss 0.0198398, acc 1\n",
      "2018-05-01T21:39:01.697232: step 3998, loss 0.00407782, acc 1\n",
      "2018-05-01T21:39:01.710926: step 3999, loss 0.000160244, acc 1\n",
      "2018-05-01T21:39:01.723799: step 4000, loss 0.0213419, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:39:01.727802: step 4000, loss 0.728448, acc 0.850575\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-4000\n",
      "\n",
      "2018-05-01T21:39:01.789859: step 4001, loss 9.44921e-05, acc 1\n",
      "2018-05-01T21:39:01.803972: step 4002, loss 0.000159772, acc 1\n",
      "2018-05-01T21:39:01.819384: step 4003, loss 0.00189066, acc 1\n",
      "2018-05-01T21:39:01.838015: step 4004, loss 0.000636571, acc 1\n",
      "2018-05-01T21:39:01.852261: step 4005, loss 0.122673, acc 0.9375\n",
      "2018-05-01T21:39:01.866103: step 4006, loss 0.0402729, acc 0.96875\n",
      "2018-05-01T21:39:01.878741: step 4007, loss 0.0842306, acc 0.96875\n",
      "2018-05-01T21:39:01.891899: step 4008, loss 0.00428484, acc 1\n",
      "2018-05-01T21:39:01.905583: step 4009, loss 0.0175011, acc 1\n",
      "2018-05-01T21:39:01.918508: step 4010, loss 0.000268458, acc 1\n",
      "2018-05-01T21:39:01.932019: step 4011, loss 0.0176001, acc 1\n",
      "2018-05-01T21:39:01.945552: step 4012, loss 0.00141973, acc 1\n",
      "2018-05-01T21:39:01.958737: step 4013, loss 0.00553038, acc 1\n",
      "2018-05-01T21:39:01.974538: step 4014, loss 0.000149044, acc 1\n",
      "2018-05-01T21:39:01.988408: step 4015, loss 0.00026805, acc 1\n",
      "2018-05-01T21:39:02.001234: step 4016, loss 0.0225514, acc 1\n",
      "2018-05-01T21:39:02.015530: step 4017, loss 0.00167123, acc 1\n",
      "2018-05-01T21:39:02.031703: step 4018, loss 0.000510271, acc 1\n",
      "2018-05-01T21:39:02.049084: step 4019, loss 0.0646337, acc 0.96875\n",
      "2018-05-01T21:39:02.062746: step 4020, loss 0.00932869, acc 1\n",
      "2018-05-01T21:39:02.075816: step 4021, loss 0.000850198, acc 1\n",
      "2018-05-01T21:39:02.089122: step 4022, loss 0.000313646, acc 1\n",
      "2018-05-01T21:39:02.102389: step 4023, loss 0.00016505, acc 1\n",
      "2018-05-01T21:39:02.115861: step 4024, loss 0.00974753, acc 1\n",
      "2018-05-01T21:39:02.128577: step 4025, loss 0.0899198, acc 0.944444\n",
      "2018-05-01T21:39:02.141578: step 4026, loss 0.0040369, acc 1\n",
      "2018-05-01T21:39:02.154880: step 4027, loss 0.00142815, acc 1\n",
      "2018-05-01T21:39:02.168476: step 4028, loss 0.00175052, acc 1\n",
      "2018-05-01T21:39:02.182060: step 4029, loss 0.00172623, acc 1\n",
      "2018-05-01T21:39:02.196668: step 4030, loss 0.0314972, acc 0.96875\n",
      "2018-05-01T21:39:02.210292: step 4031, loss 0.0585227, acc 0.96875\n",
      "2018-05-01T21:39:02.223251: step 4032, loss 0.00263786, acc 1\n",
      "2018-05-01T21:39:02.237154: step 4033, loss 0.194823, acc 0.90625\n",
      "2018-05-01T21:39:02.257843: step 4034, loss 4.66588e-05, acc 1\n",
      "2018-05-01T21:39:02.272081: step 4035, loss 0.00226134, acc 1\n",
      "2018-05-01T21:39:02.286534: step 4036, loss 0.0119949, acc 1\n",
      "2018-05-01T21:39:02.300452: step 4037, loss 0.00133484, acc 1\n",
      "2018-05-01T21:39:02.313310: step 4038, loss 0.00164098, acc 1\n",
      "2018-05-01T21:39:02.326266: step 4039, loss 0.00225957, acc 1\n",
      "2018-05-01T21:39:02.341405: step 4040, loss 0.000270895, acc 1\n",
      "2018-05-01T21:39:02.355678: step 4041, loss 0.000267506, acc 1\n",
      "2018-05-01T21:39:02.369251: step 4042, loss 4.77964e-05, acc 1\n",
      "2018-05-01T21:39:02.382394: step 4043, loss 0.000897454, acc 1\n",
      "2018-05-01T21:39:02.396844: step 4044, loss 0.0184352, acc 1\n",
      "2018-05-01T21:39:02.410200: step 4045, loss 0.0240307, acc 0.96875\n",
      "2018-05-01T21:39:02.423831: step 4046, loss 0.0528549, acc 0.96875\n",
      "2018-05-01T21:39:02.437566: step 4047, loss 0.0295532, acc 0.96875\n",
      "2018-05-01T21:39:02.456720: step 4048, loss 0.025242, acc 0.96875\n",
      "2018-05-01T21:39:02.474133: step 4049, loss 0.0494463, acc 0.96875\n",
      "2018-05-01T21:39:02.486872: step 4050, loss 0.000258027, acc 1\n",
      "2018-05-01T21:39:02.500984: step 4051, loss 0.00211889, acc 1\n",
      "2018-05-01T21:39:02.515030: step 4052, loss 0.0211004, acc 1\n",
      "2018-05-01T21:39:02.529593: step 4053, loss 0.000492143, acc 1\n",
      "2018-05-01T21:39:02.545337: step 4054, loss 0.00039928, acc 1\n",
      "2018-05-01T21:39:02.559150: step 4055, loss 0.000596464, acc 1\n",
      "2018-05-01T21:39:02.572460: step 4056, loss 0.00388985, acc 1\n",
      "2018-05-01T21:39:02.585766: step 4057, loss 0.0314568, acc 0.96875\n",
      "2018-05-01T21:39:02.599934: step 4058, loss 0.00109804, acc 1\n",
      "2018-05-01T21:39:02.613887: step 4059, loss 0.00872941, acc 1\n",
      "2018-05-01T21:39:02.627623: step 4060, loss 0.0018053, acc 1\n",
      "2018-05-01T21:39:02.641107: step 4061, loss 0.00270441, acc 1\n",
      "2018-05-01T21:39:02.655294: step 4062, loss 0.0719688, acc 0.96875\n",
      "2018-05-01T21:39:02.672789: step 4063, loss 0.00225457, acc 1\n",
      "2018-05-01T21:39:02.689058: step 4064, loss 0.00815992, acc 1\n",
      "2018-05-01T21:39:02.702669: step 4065, loss 0.000289943, acc 1\n",
      "2018-05-01T21:39:02.718035: step 4066, loss 0.00470316, acc 1\n",
      "2018-05-01T21:39:02.733974: step 4067, loss 0.00149911, acc 1\n",
      "2018-05-01T21:39:02.746791: step 4068, loss 0.0045287, acc 1\n",
      "2018-05-01T21:39:02.762633: step 4069, loss 0.00242457, acc 1\n",
      "2018-05-01T21:39:02.776132: step 4070, loss 0.00519656, acc 1\n",
      "2018-05-01T21:39:02.791768: step 4071, loss 0.0321774, acc 0.96875\n",
      "2018-05-01T21:39:02.805120: step 4072, loss 0.0243068, acc 0.96875\n",
      "2018-05-01T21:39:02.820490: step 4073, loss 0.000751529, acc 1\n",
      "2018-05-01T21:39:02.834088: step 4074, loss 0.000608291, acc 1\n",
      "2018-05-01T21:39:02.846885: step 4075, loss 0.224282, acc 0.944444\n",
      "2018-05-01T21:39:02.860189: step 4076, loss 0.000241464, acc 1\n",
      "2018-05-01T21:39:02.873165: step 4077, loss 9.66288e-05, acc 1\n",
      "2018-05-01T21:39:02.891282: step 4078, loss 0.000234625, acc 1\n",
      "2018-05-01T21:39:02.911196: step 4079, loss 0.00416298, acc 1\n",
      "2018-05-01T21:39:02.926066: step 4080, loss 0.00316642, acc 1\n",
      "2018-05-01T21:39:02.940771: step 4081, loss 3.39434e-05, acc 1\n",
      "2018-05-01T21:39:02.953637: step 4082, loss 0.000358399, acc 1\n",
      "2018-05-01T21:39:02.966924: step 4083, loss 0.00451276, acc 1\n",
      "2018-05-01T21:39:02.983779: step 4084, loss 0.0453556, acc 0.96875\n",
      "2018-05-01T21:39:02.997997: step 4085, loss 0.000190192, acc 1\n",
      "2018-05-01T21:39:03.012163: step 4086, loss 0.000404073, acc 1\n",
      "2018-05-01T21:39:03.025669: step 4087, loss 0.0272783, acc 0.96875\n",
      "2018-05-01T21:39:03.039478: step 4088, loss 0.00102153, acc 1\n",
      "2018-05-01T21:39:03.053807: step 4089, loss 0.00304545, acc 1\n",
      "2018-05-01T21:39:03.067925: step 4090, loss 0.00876889, acc 1\n",
      "2018-05-01T21:39:03.082218: step 4091, loss 0.0119206, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:39:03.096198: step 4092, loss 0.0504859, acc 0.96875\n",
      "2018-05-01T21:39:03.113443: step 4093, loss 0.00390818, acc 1\n",
      "2018-05-01T21:39:03.132808: step 4094, loss 0.221167, acc 0.9375\n",
      "2018-05-01T21:39:03.146739: step 4095, loss 0.00109591, acc 1\n",
      "2018-05-01T21:39:03.160951: step 4096, loss 0.0392447, acc 0.96875\n",
      "2018-05-01T21:39:03.174596: step 4097, loss 0.000955713, acc 1\n",
      "2018-05-01T21:39:03.188401: step 4098, loss 0.00930068, acc 1\n",
      "2018-05-01T21:39:03.204784: step 4099, loss 0.0945556, acc 0.96875\n",
      "2018-05-01T21:39:03.217553: step 4100, loss 0.0171031, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:39:03.221868: step 4100, loss 0.72618, acc 0.827586\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-4100\n",
      "\n",
      "2018-05-01T21:39:03.286466: step 4101, loss 0.00032522, acc 1\n",
      "2018-05-01T21:39:03.303231: step 4102, loss 0.00765152, acc 1\n",
      "2018-05-01T21:39:03.324801: step 4103, loss 0.000142961, acc 1\n",
      "2018-05-01T21:39:03.338120: step 4104, loss 0.00789953, acc 1\n",
      "2018-05-01T21:39:03.352315: step 4105, loss 0.0140189, acc 1\n",
      "2018-05-01T21:39:03.366603: step 4106, loss 0.000388824, acc 1\n",
      "2018-05-01T21:39:03.380543: step 4107, loss 0.00049689, acc 1\n",
      "2018-05-01T21:39:03.394275: step 4108, loss 0.000369085, acc 1\n",
      "2018-05-01T21:39:03.407935: step 4109, loss 0.00215168, acc 1\n",
      "2018-05-01T21:39:03.421342: step 4110, loss 0.039734, acc 0.96875\n",
      "2018-05-01T21:39:03.434430: step 4111, loss 0.000618234, acc 1\n",
      "2018-05-01T21:39:03.448266: step 4112, loss 0.000589452, acc 1\n",
      "2018-05-01T21:39:03.462126: step 4113, loss 0.0829888, acc 0.9375\n",
      "2018-05-01T21:39:03.475181: step 4114, loss 0.000583625, acc 1\n",
      "2018-05-01T21:39:03.488597: step 4115, loss 0.00841426, acc 1\n",
      "2018-05-01T21:39:03.503689: step 4116, loss 0.000992513, acc 1\n",
      "2018-05-01T21:39:03.527656: step 4117, loss 0.0326734, acc 0.96875\n",
      "2018-05-01T21:39:03.541805: step 4118, loss 0.0120992, acc 1\n",
      "2018-05-01T21:39:03.555893: step 4119, loss 0.000182214, acc 1\n",
      "2018-05-01T21:39:03.569035: step 4120, loss 0.0225544, acc 1\n",
      "2018-05-01T21:39:03.582969: step 4121, loss 0.028185, acc 0.96875\n",
      "2018-05-01T21:39:03.596481: step 4122, loss 0.00026345, acc 1\n",
      "2018-05-01T21:39:03.610674: step 4123, loss 0.000152507, acc 1\n",
      "2018-05-01T21:39:03.625880: step 4124, loss 0.0842112, acc 0.9375\n",
      "2018-05-01T21:39:03.639582: step 4125, loss 0.0277181, acc 1\n",
      "2018-05-01T21:39:03.654094: step 4126, loss 0.0331224, acc 1\n",
      "2018-05-01T21:39:03.668918: step 4127, loss 0.00111531, acc 1\n",
      "2018-05-01T21:39:03.682385: step 4128, loss 7.60545e-05, acc 1\n",
      "2018-05-01T21:39:03.696119: step 4129, loss 0.000921113, acc 1\n",
      "2018-05-01T21:39:03.710376: step 4130, loss 5.90481e-05, acc 1\n",
      "2018-05-01T21:39:03.725450: step 4131, loss 6.43531e-05, acc 1\n",
      "2018-05-01T21:39:03.745585: step 4132, loss 0.074864, acc 0.96875\n",
      "2018-05-01T21:39:03.764563: step 4133, loss 0.000256014, acc 1\n",
      "2018-05-01T21:39:03.779259: step 4134, loss 0.000962783, acc 1\n",
      "2018-05-01T21:39:03.793651: step 4135, loss 0.00295849, acc 1\n",
      "2018-05-01T21:39:03.807771: step 4136, loss 0.0049669, acc 1\n",
      "2018-05-01T21:39:03.820891: step 4137, loss 0.00635772, acc 1\n",
      "2018-05-01T21:39:03.835612: step 4138, loss 0.00147034, acc 1\n",
      "2018-05-01T21:39:03.849407: step 4139, loss 0.00132679, acc 1\n",
      "2018-05-01T21:39:03.863550: step 4140, loss 0.00117099, acc 1\n",
      "2018-05-01T21:39:03.878410: step 4141, loss 0.00223303, acc 1\n",
      "2018-05-01T21:39:03.892539: step 4142, loss 0.000410088, acc 1\n",
      "2018-05-01T21:39:03.906787: step 4143, loss 0.0957137, acc 0.96875\n",
      "2018-05-01T21:39:03.920238: step 4144, loss 0.00021199, acc 1\n",
      "2018-05-01T21:39:03.933972: step 4145, loss 0.0726049, acc 0.96875\n",
      "2018-05-01T21:39:03.947831: step 4146, loss 0.0147328, acc 1\n",
      "2018-05-01T21:39:03.966557: step 4147, loss 0.0806644, acc 0.96875\n",
      "2018-05-01T21:39:03.984049: step 4148, loss 0.0712736, acc 0.96875\n",
      "2018-05-01T21:39:03.998637: step 4149, loss 0.00106355, acc 1\n",
      "2018-05-01T21:39:04.011555: step 4150, loss 0.0021971, acc 1\n",
      "2018-05-01T21:39:04.024748: step 4151, loss 0.00458682, acc 1\n",
      "2018-05-01T21:39:04.038427: step 4152, loss 0.00778764, acc 1\n",
      "2018-05-01T21:39:04.053160: step 4153, loss 0.0131207, acc 1\n",
      "2018-05-01T21:39:04.066478: step 4154, loss 0.0904294, acc 0.96875\n",
      "2018-05-01T21:39:04.080293: step 4155, loss 0.0165998, acc 1\n",
      "2018-05-01T21:39:04.094493: step 4156, loss 0.0072488, acc 1\n",
      "2018-05-01T21:39:04.108824: step 4157, loss 0.00472894, acc 1\n",
      "2018-05-01T21:39:04.121997: step 4158, loss 0.0942875, acc 0.96875\n",
      "2018-05-01T21:39:04.135189: step 4159, loss 7.05986e-05, acc 1\n",
      "2018-05-01T21:39:04.149134: step 4160, loss 0.00554515, acc 1\n",
      "2018-05-01T21:39:04.166513: step 4161, loss 0.0209256, acc 1\n",
      "2018-05-01T21:39:04.186929: step 4162, loss 0.0245169, acc 0.96875\n",
      "2018-05-01T21:39:04.201199: step 4163, loss 0.00310796, acc 1\n",
      "2018-05-01T21:39:04.215479: step 4164, loss 0.000308008, acc 1\n",
      "2018-05-01T21:39:04.230202: step 4165, loss 0.0815804, acc 0.96875\n",
      "2018-05-01T21:39:04.245693: step 4166, loss 0.0200765, acc 1\n",
      "2018-05-01T21:39:04.259848: step 4167, loss 0.0182761, acc 1\n",
      "2018-05-01T21:39:04.273336: step 4168, loss 0.000526781, acc 1\n",
      "2018-05-01T21:39:04.288319: step 4169, loss 0.000497513, acc 1\n",
      "2018-05-01T21:39:04.303121: step 4170, loss 7.49071e-05, acc 1\n",
      "2018-05-01T21:39:04.316581: step 4171, loss 0.051303, acc 0.96875\n",
      "2018-05-01T21:39:04.329861: step 4172, loss 9.87802e-05, acc 1\n",
      "2018-05-01T21:39:04.343921: step 4173, loss 0.000675469, acc 1\n",
      "2018-05-01T21:39:04.358035: step 4174, loss 0.00216329, acc 1\n",
      "2018-05-01T21:39:04.370715: step 4175, loss 0.0537807, acc 0.944444\n",
      "2018-05-01T21:39:04.389676: step 4176, loss 0.0603275, acc 0.9375\n",
      "2018-05-01T21:39:04.407467: step 4177, loss 0.0558035, acc 0.96875\n",
      "2018-05-01T21:39:04.421263: step 4178, loss 0.000167707, acc 1\n",
      "2018-05-01T21:39:04.435275: step 4179, loss 0.156322, acc 0.90625\n",
      "2018-05-01T21:39:04.450211: step 4180, loss 0.0059256, acc 1\n",
      "2018-05-01T21:39:04.463744: step 4181, loss 0.115385, acc 0.96875\n",
      "2018-05-01T21:39:04.476988: step 4182, loss 0.00173189, acc 1\n",
      "2018-05-01T21:39:04.489993: step 4183, loss 0.000407305, acc 1\n",
      "2018-05-01T21:39:04.503564: step 4184, loss 0.00102359, acc 1\n",
      "2018-05-01T21:39:04.517391: step 4185, loss 0.0801609, acc 0.9375\n",
      "2018-05-01T21:39:04.531073: step 4186, loss 9.91433e-05, acc 1\n",
      "2018-05-01T21:39:04.544270: step 4187, loss 0.00316162, acc 1\n",
      "2018-05-01T21:39:04.557681: step 4188, loss 0.00269877, acc 1\n",
      "2018-05-01T21:39:04.571823: step 4189, loss 0.00178184, acc 1\n",
      "2018-05-01T21:39:04.592227: step 4190, loss 0.00169881, acc 1\n",
      "2018-05-01T21:39:04.611044: step 4191, loss 0.00139082, acc 1\n",
      "2018-05-01T21:39:04.625079: step 4192, loss 0.010034, acc 1\n",
      "2018-05-01T21:39:04.638574: step 4193, loss 0.00104899, acc 1\n",
      "2018-05-01T21:39:04.651487: step 4194, loss 4.54296e-05, acc 1\n",
      "2018-05-01T21:39:04.665133: step 4195, loss 0.000426017, acc 1\n",
      "2018-05-01T21:39:04.679474: step 4196, loss 0.00022423, acc 1\n",
      "2018-05-01T21:39:04.692737: step 4197, loss 0.000263154, acc 1\n",
      "2018-05-01T21:39:04.705845: step 4198, loss 0.0321137, acc 0.96875\n",
      "2018-05-01T21:39:04.718992: step 4199, loss 0.00143985, acc 1\n",
      "2018-05-01T21:39:04.732040: step 4200, loss 2.98978e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:39:04.736690: step 4200, loss 0.723402, acc 0.827586\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-4200\n",
      "\n",
      "2018-05-01T21:39:04.802274: step 4201, loss 0.00234966, acc 1\n",
      "2018-05-01T21:39:04.820000: step 4202, loss 0.00206366, acc 1\n",
      "2018-05-01T21:39:04.836057: step 4203, loss 0.00182038, acc 1\n",
      "2018-05-01T21:39:04.850082: step 4204, loss 0.00122575, acc 1\n",
      "2018-05-01T21:39:04.863741: step 4205, loss 0.0341967, acc 0.96875\n",
      "2018-05-01T21:39:04.877311: step 4206, loss 0.000658266, acc 1\n",
      "2018-05-01T21:39:04.890736: step 4207, loss 0.00040911, acc 1\n",
      "2018-05-01T21:39:04.903322: step 4208, loss 0.00422443, acc 1\n",
      "2018-05-01T21:39:04.916906: step 4209, loss 0.000195435, acc 1\n",
      "2018-05-01T21:39:04.930595: step 4210, loss 0.112659, acc 0.96875\n",
      "2018-05-01T21:39:04.944045: step 4211, loss 0.00492663, acc 1\n",
      "2018-05-01T21:39:04.958182: step 4212, loss 0.00299674, acc 1\n",
      "2018-05-01T21:39:04.971970: step 4213, loss 0.0357499, acc 0.96875\n",
      "2018-05-01T21:39:04.985253: step 4214, loss 0.00350681, acc 1\n",
      "2018-05-01T21:39:04.999130: step 4215, loss 0.000288267, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:39:05.016203: step 4216, loss 0.002029, acc 1\n",
      "2018-05-01T21:39:05.033400: step 4217, loss 0.000924147, acc 1\n",
      "2018-05-01T21:39:05.050090: step 4218, loss 0.0645447, acc 0.96875\n",
      "2018-05-01T21:39:05.065952: step 4219, loss 0.0219571, acc 1\n",
      "2018-05-01T21:39:05.079639: step 4220, loss 0.000720137, acc 1\n",
      "2018-05-01T21:39:05.094232: step 4221, loss 0.000335312, acc 1\n",
      "2018-05-01T21:39:05.109393: step 4222, loss 0.00584506, acc 1\n",
      "2018-05-01T21:39:05.125384: step 4223, loss 0.036622, acc 0.96875\n",
      "2018-05-01T21:39:05.139115: step 4224, loss 0.202695, acc 0.96875\n",
      "2018-05-01T21:39:05.151458: step 4225, loss 0.00514902, acc 1\n",
      "2018-05-01T21:39:05.164890: step 4226, loss 0.00240665, acc 1\n",
      "2018-05-01T21:39:05.179209: step 4227, loss 0.0181049, acc 1\n",
      "2018-05-01T21:39:05.192456: step 4228, loss 0.000795105, acc 1\n",
      "2018-05-01T21:39:05.208496: step 4229, loss 0.000217778, acc 1\n",
      "2018-05-01T21:39:05.225584: step 4230, loss 0.00121846, acc 1\n",
      "2018-05-01T21:39:05.243987: step 4231, loss 0.00116837, acc 1\n",
      "2018-05-01T21:39:05.258937: step 4232, loss 0.0373416, acc 0.96875\n",
      "2018-05-01T21:39:05.273129: step 4233, loss 0.0119006, acc 1\n",
      "2018-05-01T21:39:05.286716: step 4234, loss 0.000118808, acc 1\n",
      "2018-05-01T21:39:05.299738: step 4235, loss 0.0423814, acc 0.96875\n",
      "2018-05-01T21:39:05.313416: step 4236, loss 0.00302415, acc 1\n",
      "2018-05-01T21:39:05.327237: step 4237, loss 0.017109, acc 1\n",
      "2018-05-01T21:39:05.340402: step 4238, loss 0.00018487, acc 1\n",
      "2018-05-01T21:39:05.354100: step 4239, loss 0.0181382, acc 1\n",
      "2018-05-01T21:39:05.368646: step 4240, loss 0.000248051, acc 1\n",
      "2018-05-01T21:39:05.382478: step 4241, loss 0.0662055, acc 0.96875\n",
      "2018-05-01T21:39:05.396734: step 4242, loss 0.000169488, acc 1\n",
      "2018-05-01T21:39:05.410467: step 4243, loss 0.118826, acc 0.96875\n",
      "2018-05-01T21:39:05.424404: step 4244, loss 0.00129285, acc 1\n",
      "2018-05-01T21:39:05.441351: step 4245, loss 0.000257629, acc 1\n",
      "2018-05-01T21:39:05.458363: step 4246, loss 0.0078347, acc 1\n",
      "2018-05-01T21:39:05.472110: step 4247, loss 0.00427752, acc 1\n",
      "2018-05-01T21:39:05.486767: step 4248, loss 0.0564712, acc 0.96875\n",
      "2018-05-01T21:39:05.501730: step 4249, loss 0.00128183, acc 1\n",
      "2018-05-01T21:39:05.514572: step 4250, loss 0.000149767, acc 1\n",
      "2018-05-01T21:39:05.529035: step 4251, loss 0.00148468, acc 1\n",
      "2018-05-01T21:39:05.544601: step 4252, loss 0.0057354, acc 1\n",
      "2018-05-01T21:39:05.558199: step 4253, loss 0.00381042, acc 1\n",
      "2018-05-01T21:39:05.572481: step 4254, loss 0.0339362, acc 0.96875\n",
      "2018-05-01T21:39:05.586170: step 4255, loss 0.0303314, acc 0.96875\n",
      "2018-05-01T21:39:05.600303: step 4256, loss 0.000217084, acc 1\n",
      "2018-05-01T21:39:05.613521: step 4257, loss 0.00033853, acc 1\n",
      "2018-05-01T21:39:05.627362: step 4258, loss 0.000695075, acc 1\n",
      "2018-05-01T21:39:05.641395: step 4259, loss 0.000237109, acc 1\n",
      "2018-05-01T21:39:05.658745: step 4260, loss 0.00942888, acc 1\n",
      "2018-05-01T21:39:05.675788: step 4261, loss 0.0274127, acc 0.96875\n",
      "2018-05-01T21:39:05.690270: step 4262, loss 0.0178691, acc 1\n",
      "2018-05-01T21:39:05.704420: step 4263, loss 0.0019127, acc 1\n",
      "2018-05-01T21:39:05.719150: step 4264, loss 0.0106996, acc 1\n",
      "2018-05-01T21:39:05.733886: step 4265, loss 0.0852918, acc 0.96875\n",
      "2018-05-01T21:39:05.748268: step 4266, loss 0.000314534, acc 1\n",
      "2018-05-01T21:39:05.765358: step 4267, loss 0.00122563, acc 1\n",
      "2018-05-01T21:39:05.779113: step 4268, loss 0.00110111, acc 1\n",
      "2018-05-01T21:39:05.793816: step 4269, loss 0.17692, acc 0.9375\n",
      "2018-05-01T21:39:05.807201: step 4270, loss 0.000820458, acc 1\n",
      "2018-05-01T21:39:05.821917: step 4271, loss 0.00615458, acc 1\n",
      "2018-05-01T21:39:05.836032: step 4272, loss 0.0190088, acc 1\n",
      "2018-05-01T21:39:05.850045: step 4273, loss 0.0426505, acc 0.96875\n",
      "2018-05-01T21:39:05.863549: step 4274, loss 0.0139873, acc 1\n",
      "2018-05-01T21:39:05.878705: step 4275, loss 0.00219202, acc 1\n",
      "2018-05-01T21:39:05.895550: step 4276, loss 0.00014798, acc 1\n",
      "2018-05-01T21:39:05.908772: step 4277, loss 0.0556751, acc 0.96875\n",
      "2018-05-01T21:39:05.922400: step 4278, loss 3.92595e-05, acc 1\n",
      "2018-05-01T21:39:05.935545: step 4279, loss 0.0043401, acc 1\n",
      "2018-05-01T21:39:05.950543: step 4280, loss 0.000445081, acc 1\n",
      "2018-05-01T21:39:05.965359: step 4281, loss 0.000645098, acc 1\n",
      "2018-05-01T21:39:05.979155: step 4282, loss 0.000270376, acc 1\n",
      "2018-05-01T21:39:05.992711: step 4283, loss 0.000525346, acc 1\n",
      "2018-05-01T21:39:06.006438: step 4284, loss 0.000389752, acc 1\n",
      "2018-05-01T21:39:06.020361: step 4285, loss 0.00812988, acc 1\n",
      "2018-05-01T21:39:06.033475: step 4286, loss 0.0508393, acc 0.96875\n",
      "2018-05-01T21:39:06.047656: step 4287, loss 0.001221, acc 1\n",
      "2018-05-01T21:39:06.062323: step 4288, loss 0.00241303, acc 1\n",
      "2018-05-01T21:39:06.079473: step 4289, loss 0.00395605, acc 1\n",
      "2018-05-01T21:39:06.098547: step 4290, loss 0.157262, acc 0.9375\n",
      "2018-05-01T21:39:06.113301: step 4291, loss 0.0053492, acc 1\n",
      "2018-05-01T21:39:06.128289: step 4292, loss 0.000142823, acc 1\n",
      "2018-05-01T21:39:06.142417: step 4293, loss 0.0939889, acc 0.96875\n",
      "2018-05-01T21:39:06.156488: step 4294, loss 0.000661404, acc 1\n",
      "2018-05-01T21:39:06.170593: step 4295, loss 0.056021, acc 0.96875\n",
      "2018-05-01T21:39:06.184352: step 4296, loss 0.000491204, acc 1\n",
      "2018-05-01T21:39:06.198626: step 4297, loss 0.000808629, acc 1\n",
      "2018-05-01T21:39:06.212752: step 4298, loss 0.000402428, acc 1\n",
      "2018-05-01T21:39:06.225876: step 4299, loss 0.00154036, acc 1\n",
      "2018-05-01T21:39:06.238748: step 4300, loss 9.63251e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:39:06.242926: step 4300, loss 0.702456, acc 0.83908\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-4300\n",
      "\n",
      "2018-05-01T21:39:06.315520: step 4301, loss 0.0104013, acc 1\n",
      "2018-05-01T21:39:06.329720: step 4302, loss 0.000438047, acc 1\n",
      "2018-05-01T21:39:06.343814: step 4303, loss 0.0082979, acc 1\n",
      "2018-05-01T21:39:06.358397: step 4304, loss 0.000503303, acc 1\n",
      "2018-05-01T21:39:06.373103: step 4305, loss 0.13809, acc 0.96875\n",
      "2018-05-01T21:39:06.386369: step 4306, loss 0.000223087, acc 1\n",
      "2018-05-01T21:39:06.399838: step 4307, loss 0.0262106, acc 1\n",
      "2018-05-01T21:39:06.412880: step 4308, loss 0.0100349, acc 1\n",
      "2018-05-01T21:39:06.426463: step 4309, loss 0.000195305, acc 1\n",
      "2018-05-01T21:39:06.440235: step 4310, loss 0.0335348, acc 1\n",
      "2018-05-01T21:39:06.454615: step 4311, loss 0.000744884, acc 1\n",
      "2018-05-01T21:39:06.468180: step 4312, loss 0.000402392, acc 1\n",
      "2018-05-01T21:39:06.482445: step 4313, loss 0.000273058, acc 1\n",
      "2018-05-01T21:39:06.496985: step 4314, loss 0.0141232, acc 1\n",
      "2018-05-01T21:39:06.522670: step 4315, loss 0.0768094, acc 0.96875\n",
      "2018-05-01T21:39:06.536729: step 4316, loss 0.000433836, acc 1\n",
      "2018-05-01T21:39:06.550485: step 4317, loss 0.00020188, acc 1\n",
      "2018-05-01T21:39:06.564640: step 4318, loss 0.00143728, acc 1\n",
      "2018-05-01T21:39:06.578704: step 4319, loss 0.00890737, acc 1\n",
      "2018-05-01T21:39:06.593363: step 4320, loss 0.00285021, acc 1\n",
      "2018-05-01T21:39:06.607478: step 4321, loss 0.000370204, acc 1\n",
      "2018-05-01T21:39:06.621584: step 4322, loss 0.0519633, acc 0.96875\n",
      "2018-05-01T21:39:06.634603: step 4323, loss 0.00259171, acc 1\n",
      "2018-05-01T21:39:06.648625: step 4324, loss 0.0108499, acc 1\n",
      "2018-05-01T21:39:06.660908: step 4325, loss 0.0130697, acc 1\n",
      "2018-05-01T21:39:06.675034: step 4326, loss 0.0102009, acc 1\n",
      "2018-05-01T21:39:06.688998: step 4327, loss 0.010191, acc 1\n",
      "2018-05-01T21:39:06.703751: step 4328, loss 0.00122981, acc 1\n",
      "2018-05-01T21:39:06.719267: step 4329, loss 0.00304433, acc 1\n",
      "2018-05-01T21:39:06.737289: step 4330, loss 0.0380161, acc 0.96875\n",
      "2018-05-01T21:39:06.751970: step 4331, loss 0.0224312, acc 1\n",
      "2018-05-01T21:39:06.767219: step 4332, loss 0.000165402, acc 1\n",
      "2018-05-01T21:39:06.781521: step 4333, loss 0.0180059, acc 1\n",
      "2018-05-01T21:39:06.794990: step 4334, loss 0.00166857, acc 1\n",
      "2018-05-01T21:39:06.809424: step 4335, loss 0.000993779, acc 1\n",
      "2018-05-01T21:39:06.824371: step 4336, loss 0.00446076, acc 1\n",
      "2018-05-01T21:39:06.838385: step 4337, loss 0.000816559, acc 1\n",
      "2018-05-01T21:39:06.851744: step 4338, loss 5.6613e-05, acc 1\n",
      "2018-05-01T21:39:06.866044: step 4339, loss 0.0455871, acc 0.96875\n",
      "2018-05-01T21:39:06.881352: step 4340, loss 0.00055926, acc 1\n",
      "2018-05-01T21:39:06.897059: step 4341, loss 0.000150662, acc 1\n",
      "2018-05-01T21:39:06.919759: step 4342, loss 0.00928464, acc 1\n",
      "2018-05-01T21:39:06.937146: step 4343, loss 0.0276435, acc 0.96875\n",
      "2018-05-01T21:39:06.951001: step 4344, loss 0.0125645, acc 1\n",
      "2018-05-01T21:39:06.964887: step 4345, loss 0.0111186, acc 1\n",
      "2018-05-01T21:39:06.977815: step 4346, loss 0.000125941, acc 1\n",
      "2018-05-01T21:39:06.991631: step 4347, loss 0.103509, acc 0.96875\n",
      "2018-05-01T21:39:07.005350: step 4348, loss 0.000231125, acc 1\n",
      "2018-05-01T21:39:07.019486: step 4349, loss 0.000752377, acc 1\n",
      "2018-05-01T21:39:07.033860: step 4350, loss 0.215887, acc 0.944444\n",
      "2018-05-01T21:39:07.047797: step 4351, loss 0.000114694, acc 1\n",
      "2018-05-01T21:39:07.062411: step 4352, loss 0.00111303, acc 1\n",
      "2018-05-01T21:39:07.086237: step 4353, loss 0.00017412, acc 1\n",
      "2018-05-01T21:39:07.100571: step 4354, loss 0.10149, acc 0.96875\n",
      "2018-05-01T21:39:07.113731: step 4355, loss 0.00106635, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:39:07.134009: step 4356, loss 0.000254996, acc 1\n",
      "2018-05-01T21:39:07.150661: step 4357, loss 0.0290883, acc 1\n",
      "2018-05-01T21:39:07.167170: step 4358, loss 0.0201149, acc 1\n",
      "2018-05-01T21:39:07.181180: step 4359, loss 0.0862802, acc 0.96875\n",
      "2018-05-01T21:39:07.195334: step 4360, loss 0.0028394, acc 1\n",
      "2018-05-01T21:39:07.209097: step 4361, loss 0.0110038, acc 1\n",
      "2018-05-01T21:39:07.223043: step 4362, loss 0.00321476, acc 1\n",
      "2018-05-01T21:39:07.236211: step 4363, loss 0.0139999, acc 1\n",
      "2018-05-01T21:39:07.250444: step 4364, loss 0.00136825, acc 1\n",
      "2018-05-01T21:39:07.265454: step 4365, loss 0.00130746, acc 1\n",
      "2018-05-01T21:39:07.278993: step 4366, loss 0.00121438, acc 1\n",
      "2018-05-01T21:39:07.293148: step 4367, loss 0.000131492, acc 1\n",
      "2018-05-01T21:39:07.307136: step 4368, loss 0.0158331, acc 1\n",
      "2018-05-01T21:39:07.321338: step 4369, loss 0.021585, acc 1\n",
      "2018-05-01T21:39:07.334871: step 4370, loss 5.92641e-05, acc 1\n",
      "2018-05-01T21:39:07.350841: step 4371, loss 0.000509244, acc 1\n",
      "2018-05-01T21:39:07.366374: step 4372, loss 0.0740975, acc 0.96875\n",
      "2018-05-01T21:39:07.380635: step 4373, loss 0.000824514, acc 1\n",
      "2018-05-01T21:39:07.394698: step 4374, loss 0.00194571, acc 1\n",
      "2018-05-01T21:39:07.407566: step 4375, loss 0.000241421, acc 1\n",
      "2018-05-01T21:39:07.421595: step 4376, loss 0.0574548, acc 0.96875\n",
      "2018-05-01T21:39:07.435420: step 4377, loss 0.00923134, acc 1\n",
      "2018-05-01T21:39:07.449748: step 4378, loss 0.0145967, acc 1\n",
      "2018-05-01T21:39:07.463606: step 4379, loss 0.0279736, acc 0.96875\n",
      "2018-05-01T21:39:07.477640: step 4380, loss 6.55417e-05, acc 1\n",
      "2018-05-01T21:39:07.492967: step 4381, loss 0.0499324, acc 0.96875\n",
      "2018-05-01T21:39:07.507328: step 4382, loss 0.00188634, acc 1\n",
      "2018-05-01T21:39:07.522490: step 4383, loss 0.00455562, acc 1\n",
      "2018-05-01T21:39:07.536076: step 4384, loss 0.0203421, acc 1\n",
      "2018-05-01T21:39:07.550207: step 4385, loss 6.32999e-05, acc 1\n",
      "2018-05-01T21:39:07.569784: step 4386, loss 0.00570648, acc 1\n",
      "2018-05-01T21:39:07.587057: step 4387, loss 0.000190762, acc 1\n",
      "2018-05-01T21:39:07.601166: step 4388, loss 0.000774451, acc 1\n",
      "2018-05-01T21:39:07.615380: step 4389, loss 0.0041533, acc 1\n",
      "2018-05-01T21:39:07.629919: step 4390, loss 0.000998253, acc 1\n",
      "2018-05-01T21:39:07.642897: step 4391, loss 0.0711189, acc 0.96875\n",
      "2018-05-01T21:39:07.656928: step 4392, loss 0.000562363, acc 1\n",
      "2018-05-01T21:39:07.671219: step 4393, loss 0.00251112, acc 1\n",
      "2018-05-01T21:39:07.684943: step 4394, loss 0.00113974, acc 1\n",
      "2018-05-01T21:39:07.698721: step 4395, loss 0.000297923, acc 1\n",
      "2018-05-01T21:39:07.712571: step 4396, loss 0.000240107, acc 1\n",
      "2018-05-01T21:39:07.726547: step 4397, loss 0.0652167, acc 0.96875\n",
      "2018-05-01T21:39:07.739226: step 4398, loss 0.0197162, acc 1\n",
      "2018-05-01T21:39:07.751692: step 4399, loss 0.000172434, acc 1\n",
      "2018-05-01T21:39:07.764575: step 4400, loss 0.0158876, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:39:07.769784: step 4400, loss 0.723047, acc 0.827586\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-4400\n",
      "\n",
      "2018-05-01T21:39:07.850848: step 4401, loss 0.00729021, acc 1\n",
      "2018-05-01T21:39:07.866048: step 4402, loss 0.00234691, acc 1\n",
      "2018-05-01T21:39:07.881087: step 4403, loss 0.000931989, acc 1\n",
      "2018-05-01T21:39:07.894117: step 4404, loss 0.00100914, acc 1\n",
      "2018-05-01T21:39:07.907531: step 4405, loss 0.0097485, acc 1\n",
      "2018-05-01T21:39:07.920729: step 4406, loss 0.00989532, acc 1\n",
      "2018-05-01T21:39:07.934616: step 4407, loss 9.31372e-05, acc 1\n",
      "2018-05-01T21:39:07.948803: step 4408, loss 0.000401595, acc 1\n",
      "2018-05-01T21:39:07.965061: step 4409, loss 0.00381629, acc 1\n",
      "2018-05-01T21:39:07.978297: step 4410, loss 0.0216323, acc 1\n",
      "2018-05-01T21:39:07.991693: step 4411, loss 9.87082e-05, acc 1\n",
      "2018-05-01T21:39:08.005469: step 4412, loss 0.0142772, acc 1\n",
      "2018-05-01T21:39:08.018766: step 4413, loss 0.136416, acc 0.96875\n",
      "2018-05-01T21:39:08.032713: step 4414, loss 0.0653113, acc 0.96875\n",
      "2018-05-01T21:39:08.049603: step 4415, loss 0.00050763, acc 1\n",
      "2018-05-01T21:39:08.084542: step 4416, loss 0.00031387, acc 1\n",
      "2018-05-01T21:39:08.102636: step 4417, loss 0.0174778, acc 1\n",
      "2018-05-01T21:39:08.116649: step 4418, loss 0.00406464, acc 1\n",
      "2018-05-01T21:39:08.134623: step 4419, loss 0.0130643, acc 1\n",
      "2018-05-01T21:39:08.150957: step 4420, loss 0.00315253, acc 1\n",
      "2018-05-01T21:39:08.165959: step 4421, loss 0.000365434, acc 1\n",
      "2018-05-01T21:39:08.178585: step 4422, loss 0.0694943, acc 0.96875\n",
      "2018-05-01T21:39:08.192108: step 4423, loss 0.0114671, acc 1\n",
      "2018-05-01T21:39:08.206373: step 4424, loss 0.0806914, acc 0.96875\n",
      "2018-05-01T21:39:08.218783: step 4425, loss 0.0162766, acc 1\n",
      "2018-05-01T21:39:08.234103: step 4426, loss 0.000102495, acc 1\n",
      "2018-05-01T21:39:08.247827: step 4427, loss 0.000576075, acc 1\n",
      "2018-05-01T21:39:08.262949: step 4428, loss 0.000540264, acc 1\n",
      "2018-05-01T21:39:08.281336: step 4429, loss 0.00895702, acc 1\n",
      "2018-05-01T21:39:08.296013: step 4430, loss 0.000154482, acc 1\n",
      "2018-05-01T21:39:08.310328: step 4431, loss 0.00927065, acc 1\n",
      "2018-05-01T21:39:08.323679: step 4432, loss 0.0448905, acc 0.96875\n",
      "2018-05-01T21:39:08.339006: step 4433, loss 0.00291405, acc 1\n",
      "2018-05-01T21:39:08.352562: step 4434, loss 0.00107704, acc 1\n",
      "2018-05-01T21:39:08.365807: step 4435, loss 0.000386298, acc 1\n",
      "2018-05-01T21:39:08.378812: step 4436, loss 0.0418237, acc 1\n",
      "2018-05-01T21:39:08.392916: step 4437, loss 0.00415545, acc 1\n",
      "2018-05-01T21:39:08.406797: step 4438, loss 0.00052687, acc 1\n",
      "2018-05-01T21:39:08.421912: step 4439, loss 0.000934918, acc 1\n",
      "2018-05-01T21:39:08.435248: step 4440, loss 0.000335687, acc 1\n",
      "2018-05-01T21:39:08.448695: step 4441, loss 0.0203674, acc 1\n",
      "2018-05-01T21:39:08.462437: step 4442, loss 0.0824732, acc 0.9375\n",
      "2018-05-01T21:39:08.482902: step 4443, loss 0.00835585, acc 1\n",
      "2018-05-01T21:39:08.502616: step 4444, loss 0.00483292, acc 1\n",
      "2018-05-01T21:39:08.517167: step 4445, loss 0.00374296, acc 1\n",
      "2018-05-01T21:39:08.530786: step 4446, loss 0.00010987, acc 1\n",
      "2018-05-01T21:39:08.544946: step 4447, loss 0.00454534, acc 1\n",
      "2018-05-01T21:39:08.559534: step 4448, loss 0.00233913, acc 1\n",
      "2018-05-01T21:39:08.573830: step 4449, loss 0.054921, acc 0.96875\n",
      "2018-05-01T21:39:08.586088: step 4450, loss 0.000387439, acc 1\n",
      "2018-05-01T21:39:08.599651: step 4451, loss 0.0100304, acc 1\n",
      "2018-05-01T21:39:08.613740: step 4452, loss 7.57438e-05, acc 1\n",
      "2018-05-01T21:39:08.628726: step 4453, loss 0.000397206, acc 1\n",
      "2018-05-01T21:39:08.642041: step 4454, loss 0.0505432, acc 0.96875\n",
      "2018-05-01T21:39:08.656027: step 4455, loss 0.00296916, acc 1\n",
      "2018-05-01T21:39:08.670059: step 4456, loss 0.000396677, acc 1\n",
      "2018-05-01T21:39:08.684161: step 4457, loss 0.0105151, acc 1\n",
      "2018-05-01T21:39:08.706237: step 4458, loss 0.00363365, acc 1\n",
      "2018-05-01T21:39:08.722919: step 4459, loss 0.0212587, acc 1\n",
      "2018-05-01T21:39:08.737184: step 4460, loss 0.0369969, acc 0.96875\n",
      "2018-05-01T21:39:08.750667: step 4461, loss 0.00248903, acc 1\n",
      "2018-05-01T21:39:08.765558: step 4462, loss 0.00799799, acc 1\n",
      "2018-05-01T21:39:08.779274: step 4463, loss 8.23376e-05, acc 1\n",
      "2018-05-01T21:39:08.793361: step 4464, loss 0.000181329, acc 1\n",
      "2018-05-01T21:39:08.807361: step 4465, loss 0.00119321, acc 1\n",
      "2018-05-01T21:39:08.821069: step 4466, loss 0.0337095, acc 1\n",
      "2018-05-01T21:39:08.834575: step 4467, loss 0.00607592, acc 1\n",
      "2018-05-01T21:39:08.847221: step 4468, loss 3.07371e-05, acc 1\n",
      "2018-05-01T21:39:08.861033: step 4469, loss 0.0445857, acc 0.96875\n",
      "2018-05-01T21:39:08.874313: step 4470, loss 0.091498, acc 0.96875\n",
      "2018-05-01T21:39:08.887709: step 4471, loss 0.00634289, acc 1\n",
      "2018-05-01T21:39:08.901575: step 4472, loss 0.00108401, acc 1\n",
      "2018-05-01T21:39:08.920353: step 4473, loss 0.0134558, acc 1\n",
      "2018-05-01T21:39:08.936984: step 4474, loss 0.0816113, acc 0.96875\n",
      "2018-05-01T21:39:08.950963: step 4475, loss 9.11036e-05, acc 1\n",
      "2018-05-01T21:39:08.964654: step 4476, loss 0.0359523, acc 0.96875\n",
      "2018-05-01T21:39:08.977887: step 4477, loss 0.0670627, acc 0.96875\n",
      "2018-05-01T21:39:08.991497: step 4478, loss 0.000819136, acc 1\n",
      "2018-05-01T21:39:09.004421: step 4479, loss 0.00535244, acc 1\n",
      "2018-05-01T21:39:09.020566: step 4480, loss 0.02714, acc 0.96875\n",
      "2018-05-01T21:39:09.034824: step 4481, loss 0.00481072, acc 1\n",
      "2018-05-01T21:39:09.049657: step 4482, loss 8.65363e-05, acc 1\n",
      "2018-05-01T21:39:09.063202: step 4483, loss 0.00103751, acc 1\n",
      "2018-05-01T21:39:09.076449: step 4484, loss 0.0249626, acc 0.96875\n",
      "2018-05-01T21:39:09.089714: step 4485, loss 7.08353e-05, acc 1\n",
      "2018-05-01T21:39:09.104539: step 4486, loss 0.000172907, acc 1\n",
      "2018-05-01T21:39:09.118364: step 4487, loss 0.0385677, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:39:09.135369: step 4488, loss 0.00157054, acc 1\n",
      "2018-05-01T21:39:09.152410: step 4489, loss 0.000164917, acc 1\n",
      "2018-05-01T21:39:09.167179: step 4490, loss 0.0412875, acc 0.96875\n",
      "2018-05-01T21:39:09.181877: step 4491, loss 0.00703151, acc 1\n",
      "2018-05-01T21:39:09.195651: step 4492, loss 0.109532, acc 0.9375\n",
      "2018-05-01T21:39:09.209516: step 4493, loss 4.47503e-05, acc 1\n",
      "2018-05-01T21:39:09.224091: step 4494, loss 0.000443768, acc 1\n",
      "2018-05-01T21:39:09.238463: step 4495, loss 0.000451917, acc 1\n",
      "2018-05-01T21:39:09.251407: step 4496, loss 7.50515e-05, acc 1\n",
      "2018-05-01T21:39:09.264801: step 4497, loss 0.00309127, acc 1\n",
      "2018-05-01T21:39:09.278117: step 4498, loss 0.0083569, acc 1\n",
      "2018-05-01T21:39:09.291872: step 4499, loss 8.29487e-05, acc 1\n",
      "2018-05-01T21:39:09.304394: step 4500, loss 0.000388715, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:39:09.308790: step 4500, loss 0.730549, acc 0.804598\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-4500\n",
      "\n",
      "2018-05-01T21:39:09.378814: step 4501, loss 0.0114593, acc 1\n",
      "2018-05-01T21:39:09.394205: step 4502, loss 0.00141744, acc 1\n",
      "2018-05-01T21:39:09.408879: step 4503, loss 0.00101956, acc 1\n",
      "2018-05-01T21:39:09.423741: step 4504, loss 0.0197368, acc 1\n",
      "2018-05-01T21:39:09.436689: step 4505, loss 7.94756e-05, acc 1\n",
      "2018-05-01T21:39:09.450520: step 4506, loss 0.00243364, acc 1\n",
      "2018-05-01T21:39:09.465655: step 4507, loss 0.051603, acc 0.96875\n",
      "2018-05-01T21:39:09.479660: step 4508, loss 0.000366809, acc 1\n",
      "2018-05-01T21:39:09.495785: step 4509, loss 0.00233227, acc 1\n",
      "2018-05-01T21:39:09.512121: step 4510, loss 0.00112932, acc 1\n",
      "2018-05-01T21:39:09.525644: step 4511, loss 4.94205e-05, acc 1\n",
      "2018-05-01T21:39:09.539649: step 4512, loss 0.0778434, acc 0.9375\n",
      "2018-05-01T21:39:09.553020: step 4513, loss 0.0134118, acc 1\n",
      "2018-05-01T21:39:09.566760: step 4514, loss 0.000212547, acc 1\n",
      "2018-05-01T21:39:09.587531: step 4515, loss 0.0489962, acc 0.96875\n",
      "2018-05-01T21:39:09.602287: step 4516, loss 6.27552e-05, acc 1\n",
      "2018-05-01T21:39:09.616514: step 4517, loss 0.00118387, acc 1\n",
      "2018-05-01T21:39:09.630736: step 4518, loss 0.000161208, acc 1\n",
      "2018-05-01T21:39:09.644481: step 4519, loss 0.000886615, acc 1\n",
      "2018-05-01T21:39:09.658705: step 4520, loss 0.0147466, acc 1\n",
      "2018-05-01T21:39:09.672726: step 4521, loss 0.11714, acc 0.96875\n",
      "2018-05-01T21:39:09.686159: step 4522, loss 0.000411723, acc 1\n",
      "2018-05-01T21:39:09.699437: step 4523, loss 0.0959083, acc 0.9375\n",
      "2018-05-01T21:39:09.713260: step 4524, loss 0.000447904, acc 1\n",
      "2018-05-01T21:39:09.726011: step 4525, loss 0.00178821, acc 1\n",
      "2018-05-01T21:39:09.740205: step 4526, loss 0.000374838, acc 1\n",
      "2018-05-01T21:39:09.754388: step 4527, loss 0.000420164, acc 1\n",
      "2018-05-01T21:39:09.768616: step 4528, loss 0.00651485, acc 1\n",
      "2018-05-01T21:39:09.789835: step 4529, loss 0.000122167, acc 1\n",
      "2018-05-01T21:39:09.804094: step 4530, loss 0.000223776, acc 1\n",
      "2018-05-01T21:39:09.819074: step 4531, loss 0.000135484, acc 1\n",
      "2018-05-01T21:39:09.832677: step 4532, loss 0.00111885, acc 1\n",
      "2018-05-01T21:39:09.845957: step 4533, loss 0.000387183, acc 1\n",
      "2018-05-01T21:39:09.860453: step 4534, loss 0.139904, acc 0.9375\n",
      "2018-05-01T21:39:09.873837: step 4535, loss 0.00722484, acc 1\n",
      "2018-05-01T21:39:09.888405: step 4536, loss 0.0483591, acc 0.96875\n",
      "2018-05-01T21:39:09.902450: step 4537, loss 0.00209393, acc 1\n",
      "2018-05-01T21:39:09.915416: step 4538, loss 0.00116465, acc 1\n",
      "2018-05-01T21:39:09.928634: step 4539, loss 5.06106e-05, acc 1\n",
      "2018-05-01T21:39:09.942359: step 4540, loss 0.00490415, acc 1\n",
      "2018-05-01T21:39:09.956036: step 4541, loss 0.00152977, acc 1\n",
      "2018-05-01T21:39:09.970756: step 4542, loss 0.00932653, acc 1\n",
      "2018-05-01T21:39:09.995251: step 4543, loss 4.26553e-05, acc 1\n",
      "2018-05-01T21:39:10.011334: step 4544, loss 0.000592302, acc 1\n",
      "2018-05-01T21:39:10.024702: step 4545, loss 0.00258792, acc 1\n",
      "2018-05-01T21:39:10.038260: step 4546, loss 0.000178864, acc 1\n",
      "2018-05-01T21:39:10.052493: step 4547, loss 0.061028, acc 0.96875\n",
      "2018-05-01T21:39:10.067402: step 4548, loss 0.00365893, acc 1\n",
      "2018-05-01T21:39:10.088681: step 4549, loss 0.000175239, acc 1\n",
      "2018-05-01T21:39:10.106091: step 4550, loss 0.026242, acc 1\n",
      "2018-05-01T21:39:10.120979: step 4551, loss 0.0314611, acc 0.96875\n",
      "2018-05-01T21:39:10.134132: step 4552, loss 0.00389453, acc 1\n",
      "2018-05-01T21:39:10.147637: step 4553, loss 0.0440007, acc 0.96875\n",
      "2018-05-01T21:39:10.161660: step 4554, loss 0.0185478, acc 1\n",
      "2018-05-01T21:39:10.176553: step 4555, loss 0.0184291, acc 1\n",
      "2018-05-01T21:39:10.190377: step 4556, loss 0.0396043, acc 0.96875\n",
      "2018-05-01T21:39:10.206116: step 4557, loss 0.000873773, acc 1\n",
      "2018-05-01T21:39:10.222708: step 4558, loss 0.0025338, acc 1\n",
      "2018-05-01T21:39:10.239694: step 4559, loss 0.00582529, acc 1\n",
      "2018-05-01T21:39:10.253623: step 4560, loss 0.000116039, acc 1\n",
      "2018-05-01T21:39:10.267669: step 4561, loss 0.0240382, acc 1\n",
      "2018-05-01T21:39:10.281354: step 4562, loss 0.000424811, acc 1\n",
      "2018-05-01T21:39:10.295182: step 4563, loss 0.0436758, acc 0.96875\n",
      "2018-05-01T21:39:10.309646: step 4564, loss 0.00107763, acc 1\n",
      "2018-05-01T21:39:10.323741: step 4565, loss 0.0625549, acc 0.96875\n",
      "2018-05-01T21:39:10.338659: step 4566, loss 0.000199407, acc 1\n",
      "2018-05-01T21:39:10.353418: step 4567, loss 0.00108759, acc 1\n",
      "2018-05-01T21:39:10.367424: step 4568, loss 0.0150988, acc 1\n",
      "2018-05-01T21:39:10.381476: step 4569, loss 0.000446621, acc 1\n",
      "2018-05-01T21:39:10.396962: step 4570, loss 0.0687329, acc 0.96875\n",
      "2018-05-01T21:39:10.413132: step 4571, loss 0.00024367, acc 1\n",
      "2018-05-01T21:39:10.430454: step 4572, loss 0.000697686, acc 1\n",
      "2018-05-01T21:39:10.448091: step 4573, loss 0.000260274, acc 1\n",
      "2018-05-01T21:39:10.461839: step 4574, loss 0.154189, acc 0.96875\n",
      "2018-05-01T21:39:10.475336: step 4575, loss 0.000435114, acc 1\n",
      "2018-05-01T21:39:10.489378: step 4576, loss 0.00600035, acc 1\n",
      "2018-05-01T21:39:10.503264: step 4577, loss 0.000124216, acc 1\n",
      "2018-05-01T21:39:10.517922: step 4578, loss 0.00034866, acc 1\n",
      "2018-05-01T21:39:10.531775: step 4579, loss 0.000848161, acc 1\n",
      "2018-05-01T21:39:10.545633: step 4580, loss 0.00748766, acc 1\n",
      "2018-05-01T21:39:10.559637: step 4581, loss 0.000838686, acc 1\n",
      "2018-05-01T21:39:10.573317: step 4582, loss 0.012139, acc 1\n",
      "2018-05-01T21:39:10.587292: step 4583, loss 0.0853458, acc 0.96875\n",
      "2018-05-01T21:39:10.601277: step 4584, loss 0.000208387, acc 1\n",
      "2018-05-01T21:39:10.615666: step 4585, loss 0.00806138, acc 1\n",
      "2018-05-01T21:39:10.632788: step 4586, loss 0.114014, acc 0.96875\n",
      "2018-05-01T21:39:10.650366: step 4587, loss 0.020241, acc 1\n",
      "2018-05-01T21:39:10.664086: step 4588, loss 0.0014299, acc 1\n",
      "2018-05-01T21:39:10.678119: step 4589, loss 0.000423231, acc 1\n",
      "2018-05-01T21:39:10.691934: step 4590, loss 0.000717932, acc 1\n",
      "2018-05-01T21:39:10.706368: step 4591, loss 0.00132438, acc 1\n",
      "2018-05-01T21:39:10.719440: step 4592, loss 0.00161241, acc 1\n",
      "2018-05-01T21:39:10.734096: step 4593, loss 0.000404038, acc 1\n",
      "2018-05-01T21:39:10.749390: step 4594, loss 0.0866593, acc 0.96875\n",
      "2018-05-01T21:39:10.764776: step 4595, loss 0.00243842, acc 1\n",
      "2018-05-01T21:39:10.777810: step 4596, loss 0.00138218, acc 1\n",
      "2018-05-01T21:39:10.791271: step 4597, loss 0.00112533, acc 1\n",
      "2018-05-01T21:39:10.806020: step 4598, loss 0.128434, acc 0.9375\n",
      "2018-05-01T21:39:10.820526: step 4599, loss 0.0104363, acc 1\n",
      "2018-05-01T21:39:10.835208: step 4600, loss 0.000651768, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:39:10.839554: step 4600, loss 0.671093, acc 0.816092\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-4600\n",
      "\n",
      "2018-05-01T21:39:10.912393: step 4601, loss 0.0584921, acc 0.96875\n",
      "2018-05-01T21:39:10.925702: step 4602, loss 0.00013828, acc 1\n",
      "2018-05-01T21:39:10.938944: step 4603, loss 0.000463302, acc 1\n",
      "2018-05-01T21:39:10.952092: step 4604, loss 0.00422882, acc 1\n",
      "2018-05-01T21:39:10.965495: step 4605, loss 0.00137827, acc 1\n",
      "2018-05-01T21:39:10.979091: step 4606, loss 0.00133444, acc 1\n",
      "2018-05-01T21:39:10.994216: step 4607, loss 0.00025002, acc 1\n",
      "2018-05-01T21:39:11.007681: step 4608, loss 0.00209142, acc 1\n",
      "2018-05-01T21:39:11.022529: step 4609, loss 0.0409613, acc 0.96875\n",
      "2018-05-01T21:39:11.040864: step 4610, loss 0.0471126, acc 0.96875\n",
      "2018-05-01T21:39:11.059415: step 4611, loss 0.0111632, acc 1\n",
      "2018-05-01T21:39:11.072983: step 4612, loss 0.0774883, acc 0.96875\n",
      "2018-05-01T21:39:11.087228: step 4613, loss 0.000497219, acc 1\n",
      "2018-05-01T21:39:11.100518: step 4614, loss 0.0149216, acc 1\n",
      "2018-05-01T21:39:11.114398: step 4615, loss 0.00529728, acc 1\n",
      "2018-05-01T21:39:11.128108: step 4616, loss 0.004792, acc 1\n",
      "2018-05-01T21:39:11.142301: step 4617, loss 0.0227336, acc 0.96875\n",
      "2018-05-01T21:39:11.157881: step 4618, loss 0.000315265, acc 1\n",
      "2018-05-01T21:39:11.172178: step 4619, loss 0.000320567, acc 1\n",
      "2018-05-01T21:39:11.186928: step 4620, loss 0.0118972, acc 1\n",
      "2018-05-01T21:39:11.200766: step 4621, loss 0.00178966, acc 1\n",
      "2018-05-01T21:39:11.213833: step 4622, loss 0.0126884, acc 1\n",
      "2018-05-01T21:39:11.228695: step 4623, loss 0.0718934, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:39:11.247517: step 4624, loss 0.000484658, acc 1\n",
      "2018-05-01T21:39:11.263324: step 4625, loss 0.000488224, acc 1\n",
      "2018-05-01T21:39:11.282432: step 4626, loss 0.000345991, acc 1\n",
      "2018-05-01T21:39:11.296191: step 4627, loss 0.0753264, acc 0.96875\n",
      "2018-05-01T21:39:11.310110: step 4628, loss 0.005478, acc 1\n",
      "2018-05-01T21:39:11.324110: step 4629, loss 0.00667853, acc 1\n",
      "2018-05-01T21:39:11.337432: step 4630, loss 0.0918773, acc 0.96875\n",
      "2018-05-01T21:39:11.351386: step 4631, loss 0.000440997, acc 1\n",
      "2018-05-01T21:39:11.365258: step 4632, loss 0.00174644, acc 1\n",
      "2018-05-01T21:39:11.379624: step 4633, loss 0.0270288, acc 0.96875\n",
      "2018-05-01T21:39:11.393962: step 4634, loss 0.00149682, acc 1\n",
      "2018-05-01T21:39:11.408190: step 4635, loss 0.00162981, acc 1\n",
      "2018-05-01T21:39:11.425637: step 4636, loss 0.123764, acc 0.96875\n",
      "2018-05-01T21:39:11.440735: step 4637, loss 0.00133346, acc 1\n",
      "2018-05-01T21:39:11.457688: step 4638, loss 0.015904, acc 1\n",
      "2018-05-01T21:39:11.473877: step 4639, loss 0.000223649, acc 1\n",
      "2018-05-01T21:39:11.487381: step 4640, loss 5.48372e-05, acc 1\n",
      "2018-05-01T21:39:11.500865: step 4641, loss 0.0129031, acc 1\n",
      "2018-05-01T21:39:11.515705: step 4642, loss 0.000367003, acc 1\n",
      "2018-05-01T21:39:11.531063: step 4643, loss 0.00118586, acc 1\n",
      "2018-05-01T21:39:11.547023: step 4644, loss 0.00175282, acc 1\n",
      "2018-05-01T21:39:11.560137: step 4645, loss 0.000235254, acc 1\n",
      "2018-05-01T21:39:11.574004: step 4646, loss 0.0106823, acc 1\n",
      "2018-05-01T21:39:11.587725: step 4647, loss 0.000261993, acc 1\n",
      "2018-05-01T21:39:11.602148: step 4648, loss 0.00480089, acc 1\n",
      "2018-05-01T21:39:11.615485: step 4649, loss 0.0508612, acc 0.96875\n",
      "2018-05-01T21:39:11.627594: step 4650, loss 6.63566e-06, acc 1\n",
      "2018-05-01T21:39:11.640999: step 4651, loss 0.0031477, acc 1\n",
      "2018-05-01T21:39:11.655177: step 4652, loss 0.000305391, acc 1\n",
      "2018-05-01T21:39:11.671906: step 4653, loss 0.0332372, acc 0.96875\n",
      "2018-05-01T21:39:11.690363: step 4654, loss 0.00365905, acc 1\n",
      "2018-05-01T21:39:11.705640: step 4655, loss 0.000571976, acc 1\n",
      "2018-05-01T21:39:11.719587: step 4656, loss 0.132446, acc 0.9375\n",
      "2018-05-01T21:39:11.733175: step 4657, loss 0.000297517, acc 1\n",
      "2018-05-01T21:39:11.747672: step 4658, loss 0.0044139, acc 1\n",
      "2018-05-01T21:39:11.761943: step 4659, loss 0.000280922, acc 1\n",
      "2018-05-01T21:39:11.776926: step 4660, loss 0.000162832, acc 1\n",
      "2018-05-01T21:39:11.791537: step 4661, loss 0.0162514, acc 1\n",
      "2018-05-01T21:39:11.806315: step 4662, loss 0.0284953, acc 1\n",
      "2018-05-01T21:39:11.820128: step 4663, loss 0.000110067, acc 1\n",
      "2018-05-01T21:39:11.834015: step 4664, loss 0.0153282, acc 1\n",
      "2018-05-01T21:39:11.846687: step 4665, loss 0.0841688, acc 0.9375\n",
      "2018-05-01T21:39:11.860764: step 4666, loss 0.00053604, acc 1\n",
      "2018-05-01T21:39:11.874901: step 4667, loss 0.0297696, acc 0.96875\n",
      "2018-05-01T21:39:11.896140: step 4668, loss 0.00190795, acc 1\n",
      "2018-05-01T21:39:11.909518: step 4669, loss 0.00109258, acc 1\n",
      "2018-05-01T21:39:11.923891: step 4670, loss 0.000431868, acc 1\n",
      "2018-05-01T21:39:11.938876: step 4671, loss 0.0255363, acc 1\n",
      "2018-05-01T21:39:11.953686: step 4672, loss 0.00509531, acc 1\n",
      "2018-05-01T21:39:11.967301: step 4673, loss 0.00311537, acc 1\n",
      "2018-05-01T21:39:11.981034: step 4674, loss 0.00119616, acc 1\n",
      "2018-05-01T21:39:11.993557: step 4675, loss 0.0174035, acc 1\n",
      "2018-05-01T21:39:12.007589: step 4676, loss 0.0201436, acc 1\n",
      "2018-05-01T21:39:12.020711: step 4677, loss 0.0384783, acc 0.96875\n",
      "2018-05-01T21:39:12.033800: step 4678, loss 0.00014209, acc 1\n",
      "2018-05-01T21:39:12.047131: step 4679, loss 0.000127147, acc 1\n",
      "2018-05-01T21:39:12.061045: step 4680, loss 0.0036054, acc 1\n",
      "2018-05-01T21:39:12.080140: step 4681, loss 0.0289538, acc 1\n",
      "2018-05-01T21:39:12.097687: step 4682, loss 0.000216198, acc 1\n",
      "2018-05-01T21:39:12.113582: step 4683, loss 0.00219046, acc 1\n",
      "2018-05-01T21:39:12.127979: step 4684, loss 0.0102561, acc 1\n",
      "2018-05-01T21:39:12.141922: step 4685, loss 0.0577751, acc 0.96875\n",
      "2018-05-01T21:39:12.156914: step 4686, loss 0.000163374, acc 1\n",
      "2018-05-01T21:39:12.171526: step 4687, loss 0.00255317, acc 1\n",
      "2018-05-01T21:39:12.186393: step 4688, loss 0.000649559, acc 1\n",
      "2018-05-01T21:39:12.199224: step 4689, loss 0.00152618, acc 1\n",
      "2018-05-01T21:39:12.212423: step 4690, loss 0.000924337, acc 1\n",
      "2018-05-01T21:39:12.226686: step 4691, loss 0.00689027, acc 1\n",
      "2018-05-01T21:39:12.241122: step 4692, loss 0.0146681, acc 1\n",
      "2018-05-01T21:39:12.255176: step 4693, loss 0.0121617, acc 1\n",
      "2018-05-01T21:39:12.268766: step 4694, loss 0.116825, acc 0.96875\n",
      "2018-05-01T21:39:12.284696: step 4695, loss 0.000106222, acc 1\n",
      "2018-05-01T21:39:12.302074: step 4696, loss 0.000711571, acc 1\n",
      "2018-05-01T21:39:12.320016: step 4697, loss 0.00130581, acc 1\n",
      "2018-05-01T21:39:12.334538: step 4698, loss 0.0120172, acc 1\n",
      "2018-05-01T21:39:12.347598: step 4699, loss 0.00685826, acc 1\n",
      "2018-05-01T21:39:12.360997: step 4700, loss 0.00210121, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:39:12.365467: step 4700, loss 0.692989, acc 0.827586\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-4700\n",
      "\n",
      "2018-05-01T21:39:12.428307: step 4701, loss 0.0865882, acc 0.96875\n",
      "2018-05-01T21:39:12.442166: step 4702, loss 0.0579284, acc 0.96875\n",
      "2018-05-01T21:39:12.456619: step 4703, loss 0.000238581, acc 1\n",
      "2018-05-01T21:39:12.470800: step 4704, loss 0.020742, acc 1\n",
      "2018-05-01T21:39:12.485242: step 4705, loss 6.88562e-05, acc 1\n",
      "2018-05-01T21:39:12.505537: step 4706, loss 0.0307174, acc 0.96875\n",
      "2018-05-01T21:39:12.523155: step 4707, loss 0.226562, acc 0.96875\n",
      "2018-05-01T21:39:12.537048: step 4708, loss 0.00626556, acc 1\n",
      "2018-05-01T21:39:12.551246: step 4709, loss 0.000414231, acc 1\n",
      "2018-05-01T21:39:12.565087: step 4710, loss 0.0107553, acc 1\n",
      "2018-05-01T21:39:12.578151: step 4711, loss 0.000846914, acc 1\n",
      "2018-05-01T21:39:12.592351: step 4712, loss 0.0020832, acc 1\n",
      "2018-05-01T21:39:12.605720: step 4713, loss 0.000161988, acc 1\n",
      "2018-05-01T21:39:12.618948: step 4714, loss 0.000397524, acc 1\n",
      "2018-05-01T21:39:12.632380: step 4715, loss 0.00303775, acc 1\n",
      "2018-05-01T21:39:12.646274: step 4716, loss 0.000650008, acc 1\n",
      "2018-05-01T21:39:12.659204: step 4717, loss 0.0344074, acc 0.96875\n",
      "2018-05-01T21:39:12.673293: step 4718, loss 0.00306716, acc 1\n",
      "2018-05-01T21:39:12.687160: step 4719, loss 0.0598717, acc 0.96875\n",
      "2018-05-01T21:39:12.700992: step 4720, loss 0.0167692, acc 1\n",
      "2018-05-01T21:39:12.717320: step 4721, loss 0.000356989, acc 1\n",
      "2018-05-01T21:39:12.733677: step 4722, loss 0.0514011, acc 0.96875\n",
      "2018-05-01T21:39:12.748762: step 4723, loss 0.0435326, acc 0.96875\n",
      "2018-05-01T21:39:12.762805: step 4724, loss 0.0183166, acc 1\n",
      "2018-05-01T21:39:12.777466: step 4725, loss 0.000758056, acc 1\n",
      "2018-05-01T21:39:12.791777: step 4726, loss 0.068568, acc 0.96875\n",
      "2018-05-01T21:39:12.805567: step 4727, loss 0.00706776, acc 1\n",
      "2018-05-01T21:39:12.818706: step 4728, loss 0.00768501, acc 1\n",
      "2018-05-01T21:39:12.834576: step 4729, loss 0.000139215, acc 1\n",
      "2018-05-01T21:39:12.849461: step 4730, loss 0.0112235, acc 1\n",
      "2018-05-01T21:39:12.863077: step 4731, loss 0.000355465, acc 1\n",
      "2018-05-01T21:39:12.878916: step 4732, loss 0.0953694, acc 0.96875\n",
      "2018-05-01T21:39:12.893393: step 4733, loss 0.0232032, acc 1\n",
      "2018-05-01T21:39:12.907730: step 4734, loss 0.000532281, acc 1\n",
      "2018-05-01T21:39:12.921436: step 4735, loss 0.000472997, acc 1\n",
      "2018-05-01T21:39:12.942294: step 4736, loss 0.000130363, acc 1\n",
      "2018-05-01T21:39:12.959062: step 4737, loss 0.000148488, acc 1\n",
      "2018-05-01T21:39:12.973956: step 4738, loss 0.00474779, acc 1\n",
      "2018-05-01T21:39:12.988393: step 4739, loss 0.0152041, acc 1\n",
      "2018-05-01T21:39:13.003123: step 4740, loss 0.0622423, acc 0.96875\n",
      "2018-05-01T21:39:13.017296: step 4741, loss 4.4933e-05, acc 1\n",
      "2018-05-01T21:39:13.030917: step 4742, loss 0.0303985, acc 0.96875\n",
      "2018-05-01T21:39:13.044336: step 4743, loss 0.0868778, acc 0.96875\n",
      "2018-05-01T21:39:13.058149: step 4744, loss 0.0046154, acc 1\n",
      "2018-05-01T21:39:13.072802: step 4745, loss 0.0001617, acc 1\n",
      "2018-05-01T21:39:13.086721: step 4746, loss 0.000442082, acc 1\n",
      "2018-05-01T21:39:13.100684: step 4747, loss 0.0053299, acc 1\n",
      "2018-05-01T21:39:13.115519: step 4748, loss 0.00021505, acc 1\n",
      "2018-05-01T21:39:13.132726: step 4749, loss 0.00786231, acc 1\n",
      "2018-05-01T21:39:13.149185: step 4750, loss 0.00166251, acc 1\n",
      "2018-05-01T21:39:13.168153: step 4751, loss 0.000949281, acc 1\n",
      "2018-05-01T21:39:13.181554: step 4752, loss 9.18602e-05, acc 1\n",
      "2018-05-01T21:39:13.194637: step 4753, loss 0.0489807, acc 0.96875\n",
      "2018-05-01T21:39:13.209675: step 4754, loss 0.00176257, acc 1\n",
      "2018-05-01T21:39:13.223504: step 4755, loss 0.00684527, acc 1\n",
      "2018-05-01T21:39:13.239052: step 4756, loss 0.0204714, acc 1\n",
      "2018-05-01T21:39:13.253949: step 4757, loss 0.0881798, acc 0.96875\n",
      "2018-05-01T21:39:13.267298: step 4758, loss 0.0618122, acc 0.96875\n",
      "2018-05-01T21:39:13.284686: step 4759, loss 0.00274903, acc 1\n",
      "2018-05-01T21:39:13.298616: step 4760, loss 0.00162561, acc 1\n",
      "2018-05-01T21:39:13.312360: step 4761, loss 0.00971795, acc 1\n",
      "2018-05-01T21:39:13.325955: step 4762, loss 0.0207679, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:39:13.344948: step 4763, loss 0.0240451, acc 0.96875\n",
      "2018-05-01T21:39:13.362311: step 4764, loss 0.0150711, acc 1\n",
      "2018-05-01T21:39:13.379750: step 4765, loss 0.0146201, acc 1\n",
      "2018-05-01T21:39:13.393852: step 4766, loss 0.00554908, acc 1\n",
      "2018-05-01T21:39:13.407845: step 4767, loss 0.0436817, acc 0.96875\n",
      "2018-05-01T21:39:13.421186: step 4768, loss 0.000578404, acc 1\n",
      "2018-05-01T21:39:13.434736: step 4769, loss 3.65912e-05, acc 1\n",
      "2018-05-01T21:39:13.448787: step 4770, loss 0.000657839, acc 1\n",
      "2018-05-01T21:39:13.464190: step 4771, loss 6.76368e-05, acc 1\n",
      "2018-05-01T21:39:13.477484: step 4772, loss 0.00364073, acc 1\n",
      "2018-05-01T21:39:13.490971: step 4773, loss 0.00846415, acc 1\n",
      "2018-05-01T21:39:13.504831: step 4774, loss 0.00558604, acc 1\n",
      "2018-05-01T21:39:13.517869: step 4775, loss 8.2163e-05, acc 1\n",
      "2018-05-01T21:39:13.530778: step 4776, loss 8.56865e-05, acc 1\n",
      "2018-05-01T21:39:13.544575: step 4777, loss 0.000572235, acc 1\n",
      "2018-05-01T21:39:13.568198: step 4778, loss 0.000473295, acc 1\n",
      "2018-05-01T21:39:13.582929: step 4779, loss 0.000496483, acc 1\n",
      "2018-05-01T21:39:13.596338: step 4780, loss 0.0115525, acc 1\n",
      "2018-05-01T21:39:13.609990: step 4781, loss 0.000828694, acc 1\n",
      "2018-05-01T21:39:13.623624: step 4782, loss 0.000213801, acc 1\n",
      "2018-05-01T21:39:13.637700: step 4783, loss 0.0109104, acc 1\n",
      "2018-05-01T21:39:13.650904: step 4784, loss 0.000110496, acc 1\n",
      "2018-05-01T21:39:13.665066: step 4785, loss 0.000614194, acc 1\n",
      "2018-05-01T21:39:13.679776: step 4786, loss 0.0723214, acc 0.96875\n",
      "2018-05-01T21:39:13.693708: step 4787, loss 0.0609055, acc 0.96875\n",
      "2018-05-01T21:39:13.707449: step 4788, loss 0.0149366, acc 1\n",
      "2018-05-01T21:39:13.720950: step 4789, loss 0.00758293, acc 1\n",
      "2018-05-01T21:39:13.735032: step 4790, loss 0.000741087, acc 1\n",
      "2018-05-01T21:39:13.748417: step 4791, loss 0.000427427, acc 1\n",
      "2018-05-01T21:39:13.761547: step 4792, loss 0.00299591, acc 1\n",
      "2018-05-01T21:39:13.779138: step 4793, loss 0.0159223, acc 1\n",
      "2018-05-01T21:39:13.798993: step 4794, loss 0.0383641, acc 0.96875\n",
      "2018-05-01T21:39:13.813250: step 4795, loss 0.000213154, acc 1\n",
      "2018-05-01T21:39:13.826455: step 4796, loss 0.00688773, acc 1\n",
      "2018-05-01T21:39:13.840187: step 4797, loss 0.00367896, acc 1\n",
      "2018-05-01T21:39:13.854836: step 4798, loss 0.0617782, acc 0.96875\n",
      "2018-05-01T21:39:13.869229: step 4799, loss 0.0461154, acc 0.96875\n",
      "2018-05-01T21:39:13.881960: step 4800, loss 0.00879764, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:39:13.886489: step 4800, loss 0.769704, acc 0.83908\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-4800\n",
      "\n",
      "2018-05-01T21:39:13.951830: step 4801, loss 0.00212947, acc 1\n",
      "2018-05-01T21:39:13.966245: step 4802, loss 0.00876313, acc 1\n",
      "2018-05-01T21:39:13.980772: step 4803, loss 0.00663507, acc 1\n",
      "2018-05-01T21:39:14.005007: step 4804, loss 0.00119594, acc 1\n",
      "2018-05-01T21:39:14.020863: step 4805, loss 0.0212389, acc 1\n",
      "2018-05-01T21:39:14.035075: step 4806, loss 0.0695643, acc 0.96875\n",
      "2018-05-01T21:39:14.048752: step 4807, loss 0.000129844, acc 1\n",
      "2018-05-01T21:39:14.062869: step 4808, loss 0.000118913, acc 1\n",
      "2018-05-01T21:39:14.076282: step 4809, loss 0.000452438, acc 1\n",
      "2018-05-01T21:39:14.090486: step 4810, loss 0.0833752, acc 0.96875\n",
      "2018-05-01T21:39:14.105523: step 4811, loss 0.00589501, acc 1\n",
      "2018-05-01T21:39:14.119544: step 4812, loss 0.000924919, acc 1\n",
      "2018-05-01T21:39:14.133279: step 4813, loss 0.0531985, acc 0.96875\n",
      "2018-05-01T21:39:14.146788: step 4814, loss 0.00839067, acc 1\n",
      "2018-05-01T21:39:14.161273: step 4815, loss 0.00211826, acc 1\n",
      "2018-05-01T21:39:14.174608: step 4816, loss 0.000199245, acc 1\n",
      "2018-05-01T21:39:14.189447: step 4817, loss 0.00359816, acc 1\n",
      "2018-05-01T21:39:14.203605: step 4818, loss 0.00104549, acc 1\n",
      "2018-05-01T21:39:14.220766: step 4819, loss 0.0295621, acc 0.96875\n",
      "2018-05-01T21:39:14.237590: step 4820, loss 0.000439739, acc 1\n",
      "2018-05-01T21:39:14.253017: step 4821, loss 0.0904771, acc 0.96875\n",
      "2018-05-01T21:39:14.266872: step 4822, loss 0.000255646, acc 1\n",
      "2018-05-01T21:39:14.280546: step 4823, loss 0.0306929, acc 0.96875\n",
      "2018-05-01T21:39:14.295304: step 4824, loss 0.00172396, acc 1\n",
      "2018-05-01T21:39:14.309075: step 4825, loss 0.00314855, acc 1\n",
      "2018-05-01T21:39:14.322228: step 4826, loss 0.00839777, acc 1\n",
      "2018-05-01T21:39:14.336487: step 4827, loss 0.000781756, acc 1\n",
      "2018-05-01T21:39:14.350769: step 4828, loss 0.000476484, acc 1\n",
      "2018-05-01T21:39:14.364709: step 4829, loss 0.0327466, acc 0.96875\n",
      "2018-05-01T21:39:14.378610: step 4830, loss 0.00535327, acc 1\n",
      "2018-05-01T21:39:14.393350: step 4831, loss 0.048989, acc 0.96875\n",
      "2018-05-01T21:39:14.407696: step 4832, loss 0.000246551, acc 1\n",
      "2018-05-01T21:39:14.421289: step 4833, loss 0.00852598, acc 1\n",
      "2018-05-01T21:39:14.438696: step 4834, loss 0.00101864, acc 1\n",
      "2018-05-01T21:39:14.454484: step 4835, loss 0.00681958, acc 1\n",
      "2018-05-01T21:39:14.467399: step 4836, loss 0.0003073, acc 1\n",
      "2018-05-01T21:39:14.481077: step 4837, loss 0.00208956, acc 1\n",
      "2018-05-01T21:39:14.493911: step 4838, loss 0.000268761, acc 1\n",
      "2018-05-01T21:39:14.507173: step 4839, loss 0.000508168, acc 1\n",
      "2018-05-01T21:39:14.521441: step 4840, loss 0.00124012, acc 1\n",
      "2018-05-01T21:39:14.535504: step 4841, loss 0.00286563, acc 1\n",
      "2018-05-01T21:39:14.549934: step 4842, loss 0.0218552, acc 1\n",
      "2018-05-01T21:39:14.563873: step 4843, loss 0.0253286, acc 1\n",
      "2018-05-01T21:39:14.578325: step 4844, loss 0.207733, acc 0.96875\n",
      "2018-05-01T21:39:14.591920: step 4845, loss 0.000120041, acc 1\n",
      "2018-05-01T21:39:14.605921: step 4846, loss 0.00975637, acc 1\n",
      "2018-05-01T21:39:14.619801: step 4847, loss 0.0220571, acc 0.96875\n",
      "2018-05-01T21:39:14.634084: step 4848, loss 0.0377263, acc 0.96875\n",
      "2018-05-01T21:39:14.648717: step 4849, loss 0.000523116, acc 1\n",
      "2018-05-01T21:39:14.663835: step 4850, loss 0.0082676, acc 1\n",
      "2018-05-01T21:39:14.680208: step 4851, loss 0.0157469, acc 1\n",
      "2018-05-01T21:39:14.693816: step 4852, loss 0.0449241, acc 0.96875\n",
      "2018-05-01T21:39:14.707895: step 4853, loss 0.00777883, acc 1\n",
      "2018-05-01T21:39:14.721800: step 4854, loss 0.00104574, acc 1\n",
      "2018-05-01T21:39:14.735442: step 4855, loss 0.00285791, acc 1\n",
      "2018-05-01T21:39:14.750169: step 4856, loss 0.000704582, acc 1\n",
      "2018-05-01T21:39:14.763456: step 4857, loss 0.00208359, acc 1\n",
      "2018-05-01T21:39:14.777397: step 4858, loss 0.0343261, acc 0.96875\n",
      "2018-05-01T21:39:14.790968: step 4859, loss 0.00499562, acc 1\n",
      "2018-05-01T21:39:14.804477: step 4860, loss 0.000131358, acc 1\n",
      "2018-05-01T21:39:14.817925: step 4861, loss 0.043714, acc 0.96875\n",
      "2018-05-01T21:39:14.831088: step 4862, loss 0.000475404, acc 1\n",
      "2018-05-01T21:39:14.844006: step 4863, loss 0.000485903, acc 1\n",
      "2018-05-01T21:39:14.865397: step 4864, loss 0.0889322, acc 0.96875\n",
      "2018-05-01T21:39:14.881715: step 4865, loss 0.000299688, acc 1\n",
      "2018-05-01T21:39:14.896248: step 4866, loss 0.0219193, acc 1\n",
      "2018-05-01T21:39:14.910114: step 4867, loss 0.00521968, acc 1\n",
      "2018-05-01T21:39:14.926257: step 4868, loss 0.00685271, acc 1\n",
      "2018-05-01T21:39:14.941186: step 4869, loss 0.0015042, acc 1\n",
      "2018-05-01T21:39:14.955488: step 4870, loss 0.0398464, acc 0.96875\n",
      "2018-05-01T21:39:14.970225: step 4871, loss 0.00223376, acc 1\n",
      "2018-05-01T21:39:14.985514: step 4872, loss 0.0101235, acc 1\n",
      "2018-05-01T21:39:14.999943: step 4873, loss 0.000232224, acc 1\n",
      "2018-05-01T21:39:15.013557: step 4874, loss 0.0192831, acc 1\n",
      "2018-05-01T21:39:15.027663: step 4875, loss 0.0165676, acc 1\n",
      "2018-05-01T21:39:15.042995: step 4876, loss 0.00562749, acc 1\n",
      "2018-05-01T21:39:15.056196: step 4877, loss 0.000445776, acc 1\n",
      "2018-05-01T21:39:15.070957: step 4878, loss 0.00134796, acc 1\n",
      "2018-05-01T21:39:15.092605: step 4879, loss 0.0135079, acc 1\n",
      "2018-05-01T21:39:15.109479: step 4880, loss 0.00684246, acc 1\n",
      "2018-05-01T21:39:15.123738: step 4881, loss 0.0284358, acc 1\n",
      "2018-05-01T21:39:15.137689: step 4882, loss 0.00447299, acc 1\n",
      "2018-05-01T21:39:15.151750: step 4883, loss 0.0227318, acc 1\n",
      "2018-05-01T21:39:15.166796: step 4884, loss 0.00245664, acc 1\n",
      "2018-05-01T21:39:15.180665: step 4885, loss 0.000699862, acc 1\n",
      "2018-05-01T21:39:15.194169: step 4886, loss 0.00764524, acc 1\n",
      "2018-05-01T21:39:15.209380: step 4887, loss 0.0765551, acc 0.96875\n",
      "2018-05-01T21:39:15.223290: step 4888, loss 0.0444922, acc 0.96875\n",
      "2018-05-01T21:39:15.236709: step 4889, loss 0.00261725, acc 1\n",
      "2018-05-01T21:39:15.251369: step 4890, loss 0.0141341, acc 1\n",
      "2018-05-01T21:39:15.266044: step 4891, loss 0.0251804, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-01T21:39:15.282659: step 4892, loss 0.000231587, acc 1\n",
      "2018-05-01T21:39:15.305233: step 4893, loss 0.00580374, acc 1\n",
      "2018-05-01T21:39:15.320147: step 4894, loss 0.00185575, acc 1\n",
      "2018-05-01T21:39:15.333928: step 4895, loss 0.000106495, acc 1\n",
      "2018-05-01T21:39:15.347451: step 4896, loss 0.000195675, acc 1\n",
      "2018-05-01T21:39:15.360893: step 4897, loss 0.000656128, acc 1\n",
      "2018-05-01T21:39:15.374492: step 4898, loss 0.0544328, acc 0.96875\n",
      "2018-05-01T21:39:15.388235: step 4899, loss 0.00191773, acc 1\n",
      "2018-05-01T21:39:15.401245: step 4900, loss 6.221e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:39:15.405536: step 4900, loss 0.763266, acc 0.83908\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-4900\n",
      "\n",
      "2018-05-01T21:39:15.469513: step 4901, loss 0.00019933, acc 1\n",
      "2018-05-01T21:39:15.482965: step 4902, loss 0.00734048, acc 1\n",
      "2018-05-01T21:39:15.502183: step 4903, loss 0.131668, acc 0.96875\n",
      "2018-05-01T21:39:15.517950: step 4904, loss 0.000180298, acc 1\n",
      "2018-05-01T21:39:15.531854: step 4905, loss 0.0250156, acc 1\n",
      "2018-05-01T21:39:15.546485: step 4906, loss 0.000266036, acc 1\n",
      "2018-05-01T21:39:15.561976: step 4907, loss 0.0158752, acc 1\n",
      "2018-05-01T21:39:15.575698: step 4908, loss 0.0889523, acc 0.9375\n",
      "2018-05-01T21:39:15.592209: step 4909, loss 0.00423401, acc 1\n",
      "2018-05-01T21:39:15.605920: step 4910, loss 0.00133648, acc 1\n",
      "2018-05-01T21:39:15.619243: step 4911, loss 0.000263331, acc 1\n",
      "2018-05-01T21:39:15.633857: step 4912, loss 0.00189571, acc 1\n",
      "2018-05-01T21:39:15.647355: step 4913, loss 0.00676675, acc 1\n",
      "2018-05-01T21:39:15.660754: step 4914, loss 0.00128308, acc 1\n",
      "2018-05-01T21:39:15.673966: step 4915, loss 0.0785216, acc 0.9375\n",
      "2018-05-01T21:39:15.688203: step 4916, loss 0.000453217, acc 1\n",
      "2018-05-01T21:39:15.702094: step 4917, loss 0.000364772, acc 1\n",
      "2018-05-01T21:39:15.720236: step 4918, loss 0.0284538, acc 0.96875\n",
      "2018-05-01T21:39:15.737955: step 4919, loss 0.00650336, acc 1\n",
      "2018-05-01T21:39:15.753096: step 4920, loss 9.72759e-05, acc 1\n",
      "2018-05-01T21:39:15.767355: step 4921, loss 0.000148433, acc 1\n",
      "2018-05-01T21:39:15.781695: step 4922, loss 0.0274819, acc 0.96875\n",
      "2018-05-01T21:39:15.794958: step 4923, loss 0.000110318, acc 1\n",
      "2018-05-01T21:39:15.809251: step 4924, loss 0.000459593, acc 1\n",
      "2018-05-01T21:39:15.822380: step 4925, loss 0.0172119, acc 1\n",
      "2018-05-01T21:39:15.835932: step 4926, loss 0.0130978, acc 1\n",
      "2018-05-01T21:39:15.849904: step 4927, loss 0.00589363, acc 1\n",
      "2018-05-01T21:39:15.863652: step 4928, loss 0.0147862, acc 1\n",
      "2018-05-01T21:39:15.877954: step 4929, loss 0.000651482, acc 1\n",
      "2018-05-01T21:39:15.891577: step 4930, loss 0.0594004, acc 0.96875\n",
      "2018-05-01T21:39:15.904946: step 4931, loss 0.000427332, acc 1\n",
      "2018-05-01T21:39:15.919668: step 4932, loss 0.00248043, acc 1\n",
      "2018-05-01T21:39:15.934354: step 4933, loss 0.0493565, acc 0.96875\n",
      "2018-05-01T21:39:15.952003: step 4934, loss 0.0204366, acc 1\n",
      "2018-05-01T21:39:15.965863: step 4935, loss 0.00159556, acc 1\n",
      "2018-05-01T21:39:15.980717: step 4936, loss 0.000230794, acc 1\n",
      "2018-05-01T21:39:15.994096: step 4937, loss 0.000466329, acc 1\n",
      "2018-05-01T21:39:16.007577: step 4938, loss 0.000525657, acc 1\n",
      "2018-05-01T21:39:16.022150: step 4939, loss 0.00138031, acc 1\n",
      "2018-05-01T21:39:16.036218: step 4940, loss 0.00311954, acc 1\n",
      "2018-05-01T21:39:16.050001: step 4941, loss 2.77301e-05, acc 1\n",
      "2018-05-01T21:39:16.063744: step 4942, loss 0.0218718, acc 1\n",
      "2018-05-01T21:39:16.077425: step 4943, loss 0.0945397, acc 0.96875\n",
      "2018-05-01T21:39:16.091812: step 4944, loss 0.000499344, acc 1\n",
      "2018-05-01T21:39:16.105074: step 4945, loss 0.00108838, acc 1\n",
      "2018-05-01T21:39:16.119307: step 4946, loss 0.000956145, acc 1\n",
      "2018-05-01T21:39:16.132730: step 4947, loss 0.000201731, acc 1\n",
      "2018-05-01T21:39:16.150064: step 4948, loss 8.3023e-05, acc 1\n",
      "2018-05-01T21:39:16.168059: step 4949, loss 0.000330741, acc 1\n",
      "2018-05-01T21:39:16.180559: step 4950, loss 0.04878, acc 0.944444\n",
      "2018-05-01T21:39:16.195240: step 4951, loss 0.00157591, acc 1\n",
      "2018-05-01T21:39:16.209076: step 4952, loss 0.0977764, acc 0.96875\n",
      "2018-05-01T21:39:16.222181: step 4953, loss 0.0180435, acc 1\n",
      "2018-05-01T21:39:16.235416: step 4954, loss 0.0307051, acc 0.96875\n",
      "2018-05-01T21:39:16.249803: step 4955, loss 0.00330499, acc 1\n",
      "2018-05-01T21:39:16.263437: step 4956, loss 0.0328929, acc 0.96875\n",
      "2018-05-01T21:39:16.276841: step 4957, loss 0.0030415, acc 1\n",
      "2018-05-01T21:39:16.291032: step 4958, loss 0.00194895, acc 1\n",
      "2018-05-01T21:39:16.307644: step 4959, loss 0.000528604, acc 1\n",
      "2018-05-01T21:39:16.324232: step 4960, loss 8.04401e-05, acc 1\n",
      "2018-05-01T21:39:16.340299: step 4961, loss 0.00693461, acc 1\n",
      "2018-05-01T21:39:16.355519: step 4962, loss 0.00944048, acc 1\n",
      "2018-05-01T21:39:16.390286: step 4963, loss 0.000994001, acc 1\n",
      "2018-05-01T21:39:16.407868: step 4964, loss 0.000228109, acc 1\n",
      "2018-05-01T21:39:16.431165: step 4965, loss 0.0063949, acc 1\n",
      "2018-05-01T21:39:16.459997: step 4966, loss 0.0497324, acc 0.96875\n",
      "2018-05-01T21:39:16.481898: step 4967, loss 0.105156, acc 0.96875\n",
      "2018-05-01T21:39:16.503306: step 4968, loss 0.00364395, acc 1\n",
      "2018-05-01T21:39:16.525390: step 4969, loss 0.00367059, acc 1\n",
      "2018-05-01T21:39:16.543207: step 4970, loss 0.000687741, acc 1\n",
      "2018-05-01T21:39:16.566042: step 4971, loss 0.00453136, acc 1\n",
      "2018-05-01T21:39:16.590491: step 4972, loss 0.0273293, acc 0.96875\n",
      "2018-05-01T21:39:16.605471: step 4973, loss 0.00144106, acc 1\n",
      "2018-05-01T21:39:16.621764: step 4974, loss 0.002979, acc 1\n",
      "2018-05-01T21:39:16.634520: step 4975, loss 1.8384e-05, acc 1\n",
      "2018-05-01T21:39:16.649325: step 4976, loss 0.0221578, acc 0.96875\n",
      "2018-05-01T21:39:16.665148: step 4977, loss 0.00891655, acc 1\n",
      "2018-05-01T21:39:16.682103: step 4978, loss 0.000396413, acc 1\n",
      "2018-05-01T21:39:16.704877: step 4979, loss 0.0656556, acc 0.96875\n",
      "2018-05-01T21:39:16.724144: step 4980, loss 0.000101876, acc 1\n",
      "2018-05-01T21:39:16.748497: step 4981, loss 0.00233046, acc 1\n",
      "2018-05-01T21:39:16.768917: step 4982, loss 0.00187292, acc 1\n",
      "2018-05-01T21:39:16.799785: step 4983, loss 0.000273121, acc 1\n",
      "2018-05-01T21:39:16.826521: step 4984, loss 0.0096941, acc 1\n",
      "2018-05-01T21:39:16.841303: step 4985, loss 0.00208445, acc 1\n",
      "2018-05-01T21:39:16.856099: step 4986, loss 0.000159833, acc 1\n",
      "2018-05-01T21:39:16.871257: step 4987, loss 7.67266e-05, acc 1\n",
      "2018-05-01T21:39:16.885510: step 4988, loss 0.00252382, acc 1\n",
      "2018-05-01T21:39:16.899637: step 4989, loss 0.000344982, acc 1\n",
      "2018-05-01T21:39:16.914110: step 4990, loss 0.00331366, acc 1\n",
      "2018-05-01T21:39:16.927395: step 4991, loss 0.000596388, acc 1\n",
      "2018-05-01T21:39:16.942594: step 4992, loss 0.00824278, acc 1\n",
      "2018-05-01T21:39:16.957051: step 4993, loss 0.0453939, acc 0.96875\n",
      "2018-05-01T21:39:16.973214: step 4994, loss 0.00765814, acc 1\n",
      "2018-05-01T21:39:16.993192: step 4995, loss 0.219923, acc 0.96875\n",
      "2018-05-01T21:39:17.026347: step 4996, loss 0.0038789, acc 1\n",
      "2018-05-01T21:39:17.054595: step 4997, loss 0.00287086, acc 1\n",
      "2018-05-01T21:39:17.077917: step 4998, loss 0.0236711, acc 0.96875\n",
      "2018-05-01T21:39:17.097066: step 4999, loss 0.0117593, acc 1\n",
      "2018-05-01T21:39:17.120312: step 5000, loss 0.000255402, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-05-01T21:39:17.128958: step 5000, loss 0.937263, acc 0.804598\n",
      "\n",
      "Saved model checkpoint to /home/yyliu/code/NLP/src/jupyter_notebook/runs/1525181871/checkpoints/model-5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import datetime\n",
    "batch_size = 32\n",
    "num_epochs = 200\n",
    "dropout_keep_prob = 0.5\n",
    "\n",
    "num_checkpoints = 5\n",
    "checkpoint_every = 100\n",
    "\n",
    "evaluate_every = 100\n",
    "\n",
    "filter_sizes = (3,4,5)\n",
    "num_filters = 128\n",
    "l2_reg_lambda = 0.0\n",
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(sequence_length=SEQUENCE_LENGTH, \n",
    "                     num_classes=2, \n",
    "                     vocab_size=len(vocab_processor.vocabulary_), \n",
    "                     embedding_size=embedding_size, \n",
    "                     filter_sizes=filter_sizes, \n",
    "                     num_filters=num_filters, \n",
    "                      l2_reg_lambda=l2_reg_lambda)\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "        \n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "        \n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        def train_step(x_batch, y_batch):\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
