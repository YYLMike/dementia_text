{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gensim\n",
    "import numpy as np\n",
    "import string\n",
    "from opencc import OpenCC\n",
    "import ckip\n",
    "import jieba\n",
    "# Path of files\n",
    "SENTENCE_DICT = \"../../pickle/sentence_dict.pickle\"\n",
    "WORDVEC_MODEL = '../../wordvec_model/'\n",
    "# Variables\n",
    "DEMENTIA_NUM = 51\n",
    "CONTROL_NUM = 51\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "def read_sentence_file(file_name=None):\n",
    "    with open(SENTENCE_DICT, 'rb') as f:\n",
    "        sentence_dict = pickle.load(f)\n",
    "        print(\"Load sentence text data ...\")\n",
    "    return sentence_dict\n",
    "\n",
    "def load_wordvec_model(file_name):\n",
    "    w2v_model = gensim.models.Word2Vec.load(WORDVEC_MODEL+file_name)\n",
    "    words = []\n",
    "    for word in w2v_model.wv.vocab:\n",
    "        words.append(word)\n",
    "    print('Load word2vec model sucess ...')\n",
    "    print('Number of token: {}'.format(len(words)))\n",
    "    print('Dimensions of word vector: {}'.format(len(w2v_model[words[0]])))\n",
    "    return w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load sentence text data ...\n"
     ]
    }
   ],
   "source": [
    "sentence_dict = read_sentence_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load word2vec model sucess ...\n",
      "Number of token: 259425\n",
      "Dimensions of word vector: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yyliu/anaconda3/envs/NLP/lib/python3.6/site-packages/ipykernel_launcher.py:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "w2v_model = load_wordvec_model('100features_20context_20mincount_zht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(w2v_model.wv.vocab.keys())\n",
    "word_embedding = []\n",
    "for k in w2v_model.wv.vocab.keys():\n",
    "    word_embedding.append(np.asarray(w2v_model.wv[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3個人', '一個媽媽兩個小孩', '小孩站在椅子上要拿西點', '椅子都快倒下來了', '在拿這個西點餅乾要吃', '手下還拿著一塊']\n"
     ]
    }
   ],
   "source": [
    "def split_punctuation(sentence):\n",
    "    punctuation = set(string.punctuation+\"，\"+\"、\"+\"」\"+\"「\"+\"。\"+\" \"+\"！\")\n",
    "    sentence_split = []\n",
    "    tmp = ''\n",
    "    for i in sentence:\n",
    "        if i not in punctuation:\n",
    "            tmp += i\n",
    "        else:\n",
    "            sentence_split.append(tmp)\n",
    "            tmp = ''\n",
    "    return sentence_split\n",
    "sentence = '3個人，一個媽媽兩個小孩，小孩站在椅子上要拿西點，椅子都快倒下來了，在拿這個西點餅乾要吃，手下還拿著一塊，'\n",
    "print(split_punctuation(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of train set: 873\n",
      "sentence number of dementia subject: 442\n",
      "sentence number of control normal subject: 431\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/dementia.txt', encoding='utf8') as f:\n",
    "    dementia_txt = f.readlines()\n",
    "sentence = []\n",
    "for i in range(len(dementia_txt)):\n",
    "    if i%2==0:\n",
    "        sentence.extend(split_punctuation(dementia_txt[i+1]))\n",
    "dementia_num = len(sentence)\n",
    "with open('../../data/control_51.txt', encoding='utf8') as f:\n",
    "    control_txt = f.readlines()\n",
    "for i in range(len(control_txt)):\n",
    "    if i%2==0:\n",
    "        sentence.extend(split_punctuation(control_txt[i+1]))\n",
    "control_num = len(sentence) - dementia_num\n",
    "############\n",
    "# train set#\n",
    "############\n",
    "train_data = np.array(sentence)\n",
    "dementia_labels = [[0, 1] for _ in train_data[:dementia_num]]\n",
    "control_labels = [[1, 0] for _ in train_data[dementia_num:]]\n",
    "print('total number of train set: {}'.format(train_data.shape[0]))\n",
    "print('sentence number of dementia subject: {}'.format(len(dementia_labels)))\n",
    "print('sentence number of control normal subject: {}'.format(len(control_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/yyliu/code/NLP/data/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.u74f96b08eeb68fe4b0ac4c13a6f276ed.cache\n",
      "Loading model cost 1.434 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手下 還拿著 一塊\n"
     ]
    }
   ],
   "source": [
    "JIEBA_DICT = '../../data/dict.txt.big'\n",
    "jieba.set_dictionary(JIEBA_DICT)\n",
    "train_data_seg = []\n",
    "for i in train_data:\n",
    "    train_data_seg.append(' '.join(jieba.lcut(i)))\n",
    "print(train_data_seg[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token number of sentence: 17\n",
      "Min token number of sentence: 1\n",
      "Mean token number of sentence: 5.747995418098511\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LENGTH = 17\n",
    "\n",
    "train_data_seg_array = np.array(train_data_seg)\n",
    "l = []\n",
    "for i in range(len(train_data_seg_array)):\n",
    "    l.append(len(train_data_seg_array[i].split(' ')))\n",
    "#     if len(train_data_seg_array[i])==1:\n",
    "#         print(i, train_data_seg_array[i])\n",
    "print('Max token number of sentence: {}'.format(np.max(l)))\n",
    "print('Min token number of sentence: {}'.format(np.min(l)))\n",
    "print('Mean token number of sentence: {}'.format(np.mean(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_vec = []\n",
    "# for s in train_data_seg:\n",
    "#     token_list = []\n",
    "#     for token in s.split(' '):\n",
    "#         if token in w2v_model.wv.vocab:\n",
    "#             token_list.append(np.asarray(w2v_model.wv[token]))\n",
    "#     if len(token_list) < SEQUENCE_LENGTH:\n",
    "#         for i in range(SEQUENCE_LENGTH - len(token_list)):\n",
    "#             token_list.append(np.zeros(shape=VOCAB_DIM))\n",
    "#     train_vec.append([token_list[0:SEQUENCE_LENGTH]])\n",
    "\n",
    "# seg_sentence_vec = []\n",
    "# for key, s in seg_sentence.items():\n",
    "#     token_list = []\n",
    "#     for token in s:\n",
    "#         if token in w2v_model.wv.vocab:\n",
    "#             token_list.append(np.asarray(w2v_model.wv[token]))\n",
    "#     if len(token_list) < SEQUENCE_LENGTH:\n",
    "#         for i in range(SEQUENCE_LENGTH - len(token_list)):\n",
    "#             token_list.append(np.zeros(shape=VOCAB_DIM))\n",
    "# #             token_list.append(np.zeros(shape=(VOCAB_DIM, 1)).tolist())\n",
    "#     seg_sentence_vec.append([token_list[0:SEQUENCE_LENGTH]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_vec = np.asarray(train_vec).reshape(len(train_data_seg),-1,500)\n",
    "# print(train_vec.shape)\n",
    "# print(train_vec[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "max_sentence_length = max([len(x.split(' ')) for x in train_data_seg_array])\n",
    "print(max_sentence_length)\n",
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_sentence_length)\n",
    "vocab_processor.fit(w2v_model.wv.vocab.keys())\n",
    "x_one_hot = np.array(list(vocab_processor.transform(train_data_seg_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,  1191,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [  367, 19105,   874, 19002,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [19002, 10258,    58, 89874,  8208, 11504, 87750,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [89874,   255, 10264,  2404,  1931,   224,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [   58, 11504,  1233, 87750, 90507,  2410, 25878,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_one_hot[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size=len(vocab_processor.vocabulary_)\n",
    "# embedding_size = 128\n",
    "# with tf.device('/cpu:0'), tf.name_scope('embedding'):\n",
    "#     W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name = 'W')\n",
    "#     embedded_chars = tf.nn.embedding_lookup(W, input_x)\n",
    "#     embedded_chars_expand = tf.expand_dims(embedded_chars, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, SEQUENCE_LENGTH], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"), tf.variable_scope('scope_1', reuse=tf.AUTO_REUSE):\n",
    "            self.W = tf.get_variable(shape=[vocab_size,EMBEDDING_DIM],\n",
    "                                     initializer=tf.constant_initializer(np.array(word_embedding)), name='W', trainable=False)\n",
    "#             self.W = tf.Variable(\n",
    "#                 tf.constant(0.0, shape=[vocab_size, EMBEDDING_DIM]),\n",
    "#                 name=\"W\", trainable=False)\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length-filter_size+1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate([dementia_labels, control_labels], 0)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x_one_hot[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sample_index = -1 * int(.1*float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "del x_shuffled, y_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 259426\n",
      "Train/Dev split : 786/87\n"
     ]
    }
   ],
   "source": [
    "print('vocab_size: {}'.format(len(vocab_processor.vocabulary_)))\n",
    "print('Train/Dev split : {}/{}'.format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/yyliu/code/NLP/src/jupyter_notebook/runs_2/1525243561\n",
      "\n",
      "2018-05-02T14:46:27.084772: step 1, loss 0.86701, acc 0.53125\n",
      "2018-05-02T14:46:27.161362: step 2, loss 0.774399, acc 0.6875\n",
      "2018-05-02T14:46:27.171688: step 3, loss 0.870083, acc 0.5625\n",
      "2018-05-02T14:46:27.181809: step 4, loss 0.743286, acc 0.6875\n",
      "2018-05-02T14:46:27.192793: step 5, loss 0.850482, acc 0.46875\n",
      "2018-05-02T14:46:27.202490: step 6, loss 0.783676, acc 0.65625\n",
      "2018-05-02T14:46:27.212721: step 7, loss 0.78464, acc 0.625\n",
      "2018-05-02T14:46:27.222267: step 8, loss 0.826423, acc 0.53125\n",
      "2018-05-02T14:46:27.232566: step 9, loss 0.77679, acc 0.46875\n",
      "2018-05-02T14:46:27.242282: step 10, loss 0.792036, acc 0.59375\n",
      "2018-05-02T14:46:27.253606: step 11, loss 0.726116, acc 0.6875\n",
      "2018-05-02T14:46:27.264278: step 12, loss 0.882247, acc 0.5\n",
      "2018-05-02T14:46:27.275368: step 13, loss 0.903145, acc 0.46875\n",
      "2018-05-02T14:46:27.286036: step 14, loss 0.697982, acc 0.59375\n",
      "2018-05-02T14:46:27.295820: step 15, loss 0.765502, acc 0.625\n",
      "2018-05-02T14:46:27.306579: step 16, loss 0.844022, acc 0.4375\n",
      "2018-05-02T14:46:27.316350: step 17, loss 0.847016, acc 0.34375\n",
      "2018-05-02T14:46:27.326486: step 18, loss 0.638717, acc 0.71875\n",
      "2018-05-02T14:46:27.336343: step 19, loss 0.726092, acc 0.625\n",
      "2018-05-02T14:46:27.345653: step 20, loss 0.676387, acc 0.71875\n",
      "2018-05-02T14:46:27.355655: step 21, loss 0.737627, acc 0.5625\n",
      "2018-05-02T14:46:27.365971: step 22, loss 0.704589, acc 0.625\n",
      "2018-05-02T14:46:27.374966: step 23, loss 0.749535, acc 0.5625\n",
      "2018-05-02T14:46:27.384666: step 24, loss 0.809501, acc 0.59375\n",
      "2018-05-02T14:46:27.519357: step 25, loss 0.820776, acc 0.5\n",
      "2018-05-02T14:46:27.529654: step 26, loss 0.731776, acc 0.59375\n",
      "2018-05-02T14:46:27.538463: step 27, loss 0.656156, acc 0.71875\n",
      "2018-05-02T14:46:27.547539: step 28, loss 0.685577, acc 0.65625\n",
      "2018-05-02T14:46:27.557450: step 29, loss 0.668635, acc 0.6875\n",
      "2018-05-02T14:46:27.568334: step 30, loss 0.764202, acc 0.6875\n",
      "2018-05-02T14:46:27.578906: step 31, loss 0.637348, acc 0.78125\n",
      "2018-05-02T14:46:27.587873: step 32, loss 0.728491, acc 0.65625\n",
      "2018-05-02T14:46:27.597152: step 33, loss 0.650945, acc 0.65625\n",
      "2018-05-02T14:46:27.606440: step 34, loss 0.681203, acc 0.625\n",
      "2018-05-02T14:46:27.615790: step 35, loss 0.747314, acc 0.65625\n",
      "2018-05-02T14:46:27.625283: step 36, loss 0.730593, acc 0.75\n",
      "2018-05-02T14:46:27.634462: step 37, loss 0.572775, acc 0.8125\n",
      "2018-05-02T14:46:27.645387: step 38, loss 0.764093, acc 0.5625\n",
      "2018-05-02T14:46:27.656846: step 39, loss 0.659955, acc 0.65625\n",
      "2018-05-02T14:46:27.670778: step 40, loss 0.678628, acc 0.625\n",
      "2018-05-02T14:46:27.682221: step 41, loss 0.688155, acc 0.65625\n",
      "2018-05-02T14:46:27.691573: step 42, loss 0.767293, acc 0.5625\n",
      "2018-05-02T14:46:27.700303: step 43, loss 0.708822, acc 0.59375\n",
      "2018-05-02T14:46:27.710731: step 44, loss 0.621229, acc 0.78125\n",
      "2018-05-02T14:46:27.721429: step 45, loss 0.711204, acc 0.625\n",
      "2018-05-02T14:46:27.731635: step 46, loss 0.648634, acc 0.8125\n",
      "2018-05-02T14:46:27.740658: step 47, loss 0.672328, acc 0.6875\n",
      "2018-05-02T14:46:27.751885: step 48, loss 0.630983, acc 0.75\n",
      "2018-05-02T14:46:27.760788: step 49, loss 0.625341, acc 0.71875\n",
      "2018-05-02T14:46:27.771468: step 50, loss 0.698881, acc 0.555556\n",
      "2018-05-02T14:46:27.781541: step 51, loss 0.712573, acc 0.6875\n",
      "2018-05-02T14:46:27.797487: step 52, loss 0.679542, acc 0.71875\n",
      "2018-05-02T14:46:27.810217: step 53, loss 0.702259, acc 0.59375\n",
      "2018-05-02T14:46:27.820795: step 54, loss 0.55878, acc 0.90625\n",
      "2018-05-02T14:46:27.833532: step 55, loss 0.589592, acc 0.75\n",
      "2018-05-02T14:46:27.842249: step 56, loss 0.646615, acc 0.6875\n",
      "2018-05-02T14:46:27.852460: step 57, loss 0.627611, acc 0.75\n",
      "2018-05-02T14:46:27.862361: step 58, loss 0.634283, acc 0.71875\n",
      "2018-05-02T14:46:27.872076: step 59, loss 0.629888, acc 0.75\n",
      "2018-05-02T14:46:27.882265: step 60, loss 0.548065, acc 0.8125\n",
      "2018-05-02T14:46:27.892805: step 61, loss 0.591857, acc 0.84375\n",
      "2018-05-02T14:46:27.901677: step 62, loss 0.609476, acc 0.75\n",
      "2018-05-02T14:46:27.911808: step 63, loss 0.787523, acc 0.53125\n",
      "2018-05-02T14:46:27.921332: step 64, loss 0.692339, acc 0.5625\n",
      "2018-05-02T14:46:27.931293: step 65, loss 0.537741, acc 0.8125\n",
      "2018-05-02T14:46:27.949488: step 66, loss 0.615297, acc 0.8125\n",
      "2018-05-02T14:46:27.958800: step 67, loss 0.517341, acc 0.90625\n",
      "2018-05-02T14:46:27.968045: step 68, loss 0.540109, acc 0.78125\n",
      "2018-05-02T14:46:27.979263: step 69, loss 0.578458, acc 0.65625\n",
      "2018-05-02T14:46:27.990238: step 70, loss 0.682756, acc 0.71875\n",
      "2018-05-02T14:46:27.998911: step 71, loss 0.578177, acc 0.8125\n",
      "2018-05-02T14:46:28.008448: step 72, loss 0.575234, acc 0.6875\n",
      "2018-05-02T14:46:28.017027: step 73, loss 0.624791, acc 0.71875\n",
      "2018-05-02T14:46:28.025907: step 74, loss 0.606776, acc 0.78125\n",
      "2018-05-02T14:46:28.034610: step 75, loss 0.502493, acc 0.777778\n",
      "2018-05-02T14:46:28.044557: step 76, loss 0.586477, acc 0.84375\n",
      "2018-05-02T14:46:28.053010: step 77, loss 0.611315, acc 0.71875\n",
      "2018-05-02T14:46:28.061439: step 78, loss 0.521664, acc 0.8125\n",
      "2018-05-02T14:46:28.070025: step 79, loss 0.504042, acc 0.84375\n",
      "2018-05-02T14:46:28.079905: step 80, loss 0.619327, acc 0.6875\n",
      "2018-05-02T14:46:28.089675: step 81, loss 0.554263, acc 0.78125\n",
      "2018-05-02T14:46:28.098994: step 82, loss 0.638571, acc 0.625\n",
      "2018-05-02T14:46:28.109130: step 83, loss 0.646222, acc 0.71875\n",
      "2018-05-02T14:46:28.118832: step 84, loss 0.557625, acc 0.8125\n",
      "2018-05-02T14:46:28.128306: step 85, loss 0.615475, acc 0.75\n",
      "2018-05-02T14:46:28.136812: step 86, loss 0.550044, acc 0.78125\n",
      "2018-05-02T14:46:28.146199: step 87, loss 0.529089, acc 0.84375\n",
      "2018-05-02T14:46:28.162006: step 88, loss 0.497611, acc 0.84375\n",
      "2018-05-02T14:46:28.172018: step 89, loss 0.518172, acc 0.875\n",
      "2018-05-02T14:46:28.181329: step 90, loss 0.630382, acc 0.6875\n",
      "2018-05-02T14:46:28.190445: step 91, loss 0.570432, acc 0.75\n",
      "2018-05-02T14:46:28.199628: step 92, loss 0.602628, acc 0.625\n",
      "2018-05-02T14:46:28.208366: step 93, loss 0.497146, acc 0.875\n",
      "2018-05-02T14:46:28.218488: step 94, loss 0.500215, acc 0.8125\n",
      "2018-05-02T14:46:28.227402: step 95, loss 0.553588, acc 0.8125\n",
      "2018-05-02T14:46:28.237017: step 96, loss 0.676259, acc 0.71875\n",
      "2018-05-02T14:46:28.247178: step 97, loss 0.535243, acc 0.78125\n",
      "2018-05-02T14:46:28.257569: step 98, loss 0.625699, acc 0.65625\n",
      "2018-05-02T14:46:28.266619: step 99, loss 0.502777, acc 0.8125\n",
      "2018-05-02T14:46:28.274456: step 100, loss 0.609533, acc 0.777778\n",
      "\n",
      "Evaluation:\n",
      "2018-05-02T14:46:28.333544: step 100, loss 0.582453, acc 0.827586\n",
      "\n",
      "2018-05-02T14:46:28.346348: step 101, loss 0.465869, acc 0.90625\n",
      "2018-05-02T14:46:28.356881: step 102, loss 0.495969, acc 0.78125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-02T14:46:28.367998: step 103, loss 0.625424, acc 0.75\n",
      "2018-05-02T14:46:28.378108: step 104, loss 0.529945, acc 0.78125\n",
      "2018-05-02T14:46:28.387919: step 105, loss 0.528089, acc 0.78125\n",
      "2018-05-02T14:46:28.398544: step 106, loss 0.580177, acc 0.8125\n",
      "2018-05-02T14:46:28.407743: step 107, loss 0.460612, acc 0.875\n",
      "2018-05-02T14:46:28.417404: step 108, loss 0.486588, acc 0.875\n",
      "2018-05-02T14:46:28.427120: step 109, loss 0.483983, acc 0.90625\n",
      "2018-05-02T14:46:28.437429: step 110, loss 0.489171, acc 0.78125\n",
      "2018-05-02T14:46:28.447251: step 111, loss 0.453513, acc 0.875\n",
      "2018-05-02T14:46:28.455684: step 112, loss 0.476339, acc 0.84375\n",
      "2018-05-02T14:46:28.464204: step 113, loss 0.520922, acc 0.75\n",
      "2018-05-02T14:46:28.483174: step 114, loss 0.559811, acc 0.78125\n",
      "2018-05-02T14:46:28.494167: step 115, loss 0.603401, acc 0.75\n",
      "2018-05-02T14:46:28.504676: step 116, loss 0.541277, acc 0.78125\n",
      "2018-05-02T14:46:28.513651: step 117, loss 0.551685, acc 0.84375\n",
      "2018-05-02T14:46:28.522699: step 118, loss 0.525523, acc 0.84375\n",
      "2018-05-02T14:46:28.531709: step 119, loss 0.432094, acc 0.875\n",
      "2018-05-02T14:46:28.541413: step 120, loss 0.437381, acc 0.90625\n",
      "2018-05-02T14:46:28.551861: step 121, loss 0.541222, acc 0.8125\n",
      "2018-05-02T14:46:28.560557: step 122, loss 0.600639, acc 0.75\n",
      "2018-05-02T14:46:28.570207: step 123, loss 0.515667, acc 0.84375\n",
      "2018-05-02T14:46:28.580212: step 124, loss 0.494856, acc 0.84375\n",
      "2018-05-02T14:46:28.588868: step 125, loss 0.375527, acc 0.944444\n",
      "2018-05-02T14:46:28.598152: step 126, loss 0.479024, acc 0.875\n",
      "2018-05-02T14:46:28.607861: step 127, loss 0.596398, acc 0.75\n",
      "2018-05-02T14:46:28.616681: step 128, loss 0.51019, acc 0.8125\n",
      "2018-05-02T14:46:28.626626: step 129, loss 0.503796, acc 0.75\n",
      "2018-05-02T14:46:28.637516: step 130, loss 0.56356, acc 0.71875\n",
      "2018-05-02T14:46:28.651892: step 131, loss 0.496532, acc 0.84375\n",
      "2018-05-02T14:46:28.661261: step 132, loss 0.463143, acc 0.90625\n",
      "2018-05-02T14:46:28.670695: step 133, loss 0.438663, acc 0.875\n",
      "2018-05-02T14:46:28.680042: step 134, loss 0.471937, acc 0.84375\n",
      "2018-05-02T14:46:28.689926: step 135, loss 0.523138, acc 0.8125\n",
      "2018-05-02T14:46:28.699977: step 136, loss 0.509952, acc 0.78125\n",
      "2018-05-02T14:46:28.709934: step 137, loss 0.534054, acc 0.71875\n",
      "2018-05-02T14:46:28.719454: step 138, loss 0.449016, acc 0.875\n",
      "2018-05-02T14:46:28.728817: step 139, loss 0.517176, acc 0.75\n",
      "2018-05-02T14:46:28.737971: step 140, loss 0.517154, acc 0.8125\n",
      "2018-05-02T14:46:28.748040: step 141, loss 0.396953, acc 0.90625\n",
      "2018-05-02T14:46:28.756441: step 142, loss 0.476804, acc 0.875\n",
      "2018-05-02T14:46:28.765886: step 143, loss 0.394035, acc 0.9375\n",
      "2018-05-02T14:46:28.777196: step 144, loss 0.493285, acc 0.78125\n",
      "2018-05-02T14:46:28.788571: step 145, loss 0.422649, acc 0.875\n",
      "2018-05-02T14:46:28.797696: step 146, loss 0.517838, acc 0.78125\n",
      "2018-05-02T14:46:28.806716: step 147, loss 0.559424, acc 0.75\n",
      "2018-05-02T14:46:28.815739: step 148, loss 0.440438, acc 0.875\n",
      "2018-05-02T14:46:28.824885: step 149, loss 0.411589, acc 0.9375\n",
      "2018-05-02T14:46:28.833210: step 150, loss 0.613095, acc 0.722222\n",
      "2018-05-02T14:46:28.842188: step 151, loss 0.417609, acc 0.84375\n",
      "2018-05-02T14:46:28.852152: step 152, loss 0.491656, acc 0.75\n",
      "2018-05-02T14:46:28.861015: step 153, loss 0.447274, acc 0.90625\n",
      "2018-05-02T14:46:28.870834: step 154, loss 0.436658, acc 0.875\n",
      "2018-05-02T14:46:28.885631: step 155, loss 0.461466, acc 0.96875\n",
      "2018-05-02T14:46:28.898855: step 156, loss 0.44178, acc 0.9375\n",
      "2018-05-02T14:46:28.909207: step 157, loss 0.433481, acc 0.90625\n",
      "2018-05-02T14:46:28.918359: step 158, loss 0.486769, acc 0.78125\n",
      "2018-05-02T14:46:28.927536: step 159, loss 0.475441, acc 0.78125\n",
      "2018-05-02T14:46:28.936586: step 160, loss 0.427658, acc 0.84375\n",
      "2018-05-02T14:46:28.947693: step 161, loss 0.44529, acc 0.84375\n",
      "2018-05-02T14:46:28.956252: step 162, loss 0.388203, acc 0.84375\n",
      "2018-05-02T14:46:28.965156: step 163, loss 0.409546, acc 0.875\n",
      "2018-05-02T14:46:28.974784: step 164, loss 0.428, acc 0.90625\n",
      "2018-05-02T14:46:28.984665: step 165, loss 0.42523, acc 0.875\n",
      "2018-05-02T14:46:28.993401: step 166, loss 0.561567, acc 0.8125\n",
      "2018-05-02T14:46:29.001904: step 167, loss 0.380023, acc 0.90625\n",
      "2018-05-02T14:46:29.011241: step 168, loss 0.468615, acc 0.90625\n",
      "2018-05-02T14:46:29.020085: step 169, loss 0.39503, acc 0.875\n",
      "2018-05-02T14:46:29.032659: step 170, loss 0.472735, acc 0.84375\n",
      "2018-05-02T14:46:29.041937: step 171, loss 0.401936, acc 0.9375\n",
      "2018-05-02T14:46:29.051309: step 172, loss 0.337461, acc 1\n",
      "2018-05-02T14:46:29.061140: step 173, loss 0.364847, acc 0.90625\n",
      "2018-05-02T14:46:29.070026: step 174, loss 0.45799, acc 0.875\n",
      "2018-05-02T14:46:29.078811: step 175, loss 0.490064, acc 0.833333\n",
      "2018-05-02T14:46:29.088851: step 176, loss 0.483046, acc 0.78125\n",
      "2018-05-02T14:46:29.097605: step 177, loss 0.629513, acc 0.5625\n",
      "2018-05-02T14:46:29.107206: step 178, loss 0.431316, acc 0.84375\n",
      "2018-05-02T14:46:29.117073: step 179, loss 0.403978, acc 0.90625\n",
      "2018-05-02T14:46:29.128273: step 180, loss 0.441808, acc 0.9375\n",
      "2018-05-02T14:46:29.137322: step 181, loss 0.448191, acc 0.875\n",
      "2018-05-02T14:46:29.146381: step 182, loss 0.339642, acc 0.9375\n",
      "2018-05-02T14:46:29.155696: step 183, loss 0.424276, acc 0.90625\n",
      "2018-05-02T14:46:29.164035: step 184, loss 0.49718, acc 0.875\n",
      "2018-05-02T14:46:29.173422: step 185, loss 0.422294, acc 0.875\n",
      "2018-05-02T14:46:29.182750: step 186, loss 0.391504, acc 0.90625\n",
      "2018-05-02T14:46:29.192075: step 187, loss 0.34449, acc 0.9375\n",
      "2018-05-02T14:46:29.201334: step 188, loss 0.46676, acc 0.8125\n",
      "2018-05-02T14:46:29.211369: step 189, loss 0.365249, acc 0.9375\n",
      "2018-05-02T14:46:29.221159: step 190, loss 0.42858, acc 0.84375\n",
      "2018-05-02T14:46:29.230426: step 191, loss 0.475191, acc 0.8125\n",
      "2018-05-02T14:46:29.238853: step 192, loss 0.301663, acc 1\n",
      "2018-05-02T14:46:29.247194: step 193, loss 0.415446, acc 0.9375\n",
      "2018-05-02T14:46:29.255730: step 194, loss 0.295134, acc 0.90625\n",
      "2018-05-02T14:46:29.264394: step 195, loss 0.385018, acc 0.90625\n",
      "2018-05-02T14:46:29.274028: step 196, loss 0.423543, acc 0.90625\n",
      "2018-05-02T14:46:29.288639: step 197, loss 0.402837, acc 0.875\n",
      "2018-05-02T14:46:29.297994: step 198, loss 0.409548, acc 0.84375\n",
      "2018-05-02T14:46:29.308275: step 199, loss 0.463625, acc 0.84375\n",
      "2018-05-02T14:46:29.318542: step 200, loss 0.399713, acc 0.944444\n",
      "\n",
      "Evaluation:\n",
      "2018-05-02T14:46:29.325435: step 200, loss 0.508679, acc 0.827586\n",
      "\n",
      "2018-05-02T14:46:29.335421: step 201, loss 0.369796, acc 0.84375\n",
      "2018-05-02T14:46:29.348078: step 202, loss 0.474559, acc 0.84375\n",
      "2018-05-02T14:46:29.357344: step 203, loss 0.393247, acc 0.96875\n",
      "2018-05-02T14:46:29.367225: step 204, loss 0.386076, acc 0.90625\n",
      "2018-05-02T14:46:29.382259: step 205, loss 0.445441, acc 0.8125\n",
      "2018-05-02T14:46:29.391090: step 206, loss 0.383397, acc 0.9375\n",
      "2018-05-02T14:46:29.402061: step 207, loss 0.359689, acc 0.875\n",
      "2018-05-02T14:46:29.412987: step 208, loss 0.368526, acc 0.9375\n",
      "2018-05-02T14:46:29.421436: step 209, loss 0.398857, acc 0.875\n",
      "2018-05-02T14:46:29.430621: step 210, loss 0.472203, acc 0.8125\n",
      "2018-05-02T14:46:29.439868: step 211, loss 0.29077, acc 0.96875\n",
      "2018-05-02T14:46:29.449693: step 212, loss 0.462883, acc 0.875\n",
      "2018-05-02T14:46:29.460091: step 213, loss 0.389132, acc 0.875\n",
      "2018-05-02T14:46:29.469752: step 214, loss 0.297735, acc 1\n",
      "2018-05-02T14:46:29.478244: step 215, loss 0.361423, acc 0.875\n",
      "2018-05-02T14:46:29.494108: step 216, loss 0.433024, acc 0.84375\n",
      "2018-05-02T14:46:29.503415: step 217, loss 0.467603, acc 0.84375\n",
      "2018-05-02T14:46:29.512887: step 218, loss 0.349522, acc 0.875\n",
      "2018-05-02T14:46:29.522624: step 219, loss 0.396703, acc 0.875\n",
      "2018-05-02T14:46:29.531740: step 220, loss 0.30293, acc 0.9375\n",
      "2018-05-02T14:46:29.541598: step 221, loss 0.380978, acc 0.90625\n",
      "2018-05-02T14:46:29.551175: step 222, loss 0.476825, acc 0.84375\n",
      "2018-05-02T14:46:29.560299: step 223, loss 0.350384, acc 0.90625\n",
      "2018-05-02T14:46:29.569007: step 224, loss 0.483229, acc 0.78125\n",
      "2018-05-02T14:46:29.577928: step 225, loss 0.400223, acc 0.833333\n",
      "2018-05-02T14:46:29.590652: step 226, loss 0.393462, acc 0.9375\n",
      "2018-05-02T14:46:29.605499: step 227, loss 0.430978, acc 0.90625\n",
      "2018-05-02T14:46:29.628859: step 228, loss 0.282123, acc 1\n",
      "2018-05-02T14:46:29.639202: step 229, loss 0.394179, acc 0.84375\n",
      "2018-05-02T14:46:29.649132: step 230, loss 0.383967, acc 0.90625\n",
      "2018-05-02T14:46:29.658190: step 231, loss 0.300215, acc 0.9375\n",
      "2018-05-02T14:46:29.668853: step 232, loss 0.456453, acc 0.78125\n",
      "2018-05-02T14:46:29.677296: step 233, loss 0.421036, acc 0.84375\n",
      "2018-05-02T14:46:29.687108: step 234, loss 0.361294, acc 0.9375\n",
      "2018-05-02T14:46:29.696342: step 235, loss 0.342476, acc 0.875\n",
      "2018-05-02T14:46:29.705929: step 236, loss 0.411839, acc 0.875\n",
      "2018-05-02T14:46:29.720445: step 237, loss 0.380013, acc 0.9375\n",
      "2018-05-02T14:46:29.730604: step 238, loss 0.33467, acc 0.96875\n",
      "2018-05-02T14:46:29.741384: step 239, loss 0.417415, acc 0.9375\n",
      "2018-05-02T14:46:29.750336: step 240, loss 0.366929, acc 0.96875\n",
      "2018-05-02T14:46:29.759098: step 241, loss 0.334158, acc 0.90625\n",
      "2018-05-02T14:46:29.767892: step 242, loss 0.374684, acc 0.875\n",
      "2018-05-02T14:46:29.778011: step 243, loss 0.312586, acc 0.9375\n",
      "2018-05-02T14:46:29.787619: step 244, loss 0.388242, acc 0.875\n",
      "2018-05-02T14:46:29.797190: step 245, loss 0.297472, acc 1\n",
      "2018-05-02T14:46:29.807243: step 246, loss 0.374046, acc 0.90625\n",
      "2018-05-02T14:46:29.816673: step 247, loss 0.296791, acc 0.96875\n",
      "2018-05-02T14:46:29.825359: step 248, loss 0.301288, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-02T14:46:29.835670: step 249, loss 0.321729, acc 0.9375\n",
      "2018-05-02T14:46:29.844601: step 250, loss 0.44431, acc 0.833333\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import datetime\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "dropout_keep_prob = 0.5\n",
    "\n",
    "num_checkpoints = 5\n",
    "# checkpoint_every = 100\n",
    "\n",
    "evaluate_every = 100\n",
    "\n",
    "filter_sizes = (3,4,5)\n",
    "num_filters = 128\n",
    "l2_reg_lambda = 0.05\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(sequence_length=SEQUENCE_LENGTH, \n",
    "                     num_classes=2, \n",
    "                     vocab_size=len(vocab_processor.vocabulary_), \n",
    "                     embedding_size=EMBEDDING_DIM, \n",
    "                     filter_sizes=filter_sizes, \n",
    "                     num_filters=num_filters, \n",
    "                      l2_reg_lambda=l2_reg_lambda)\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "        \n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs_2\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "        \n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "#         checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "#         checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "#         if not os.path.exists(checkpoint_dir):\n",
    "#             os.makedirs(checkpoint_dir)\n",
    "#         saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        def train_step(x_batch, y_batch):\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "#             if current_step % checkpoint_every == 0:\n",
    "#                 path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "#                 print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
