{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = '../../docvec_model/doc_500features_10context_10mincount_zht'\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load paragraph2vec model used 4.660095562999999\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.process_time()\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec.load(MODEL)\n",
    "end = time.process_time() - start\n",
    "print('load paragraph2vec model used {}'.format(end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PICKLE = \"../../pickle/sentence_dict.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "sentence_dict = {}\n",
    "with open(DATA_PICKLE, 'rb') as f:\n",
    "    sentence_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "# from opencc import OpenCC \n",
    "# openCC = OpenCC('tw2s') \n",
    "# exclude = set(string.punctuation+'，'+'。'+'、'+'「'+'」'+'？'+'！')\n",
    "\n",
    "def sentence2vec(sentence):\n",
    "#     sentence = openCC.convert(sentence)\n",
    "    vector = np.zeros((d2v_model.docvecs[1].shape))\n",
    "    \n",
    "    token_sentence = jieba.lcut(sentence)\n",
    "    token_sentence = [t for t in token_sentence]\n",
    "    vector = d2v_model.infer_vector(token_sentence)\n",
    "    \n",
    "    return vector, token_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_dict = {}\n",
    "sentence_token_dict = {}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "002.\n",
      "003.\n",
      "004.\n",
      "005.\n",
      "006. \n",
      "007.\n",
      "008.\n",
      "009.\n",
      "010.\n",
      "011.\n",
      "012.\n",
      "013.\n",
      "015.\n",
      "016.\n",
      "018.\n",
      "019.\n",
      "020.\n",
      "021.\n",
      "023.\n",
      "024.\n",
      "025.\n",
      "026.\n",
      "027.\n",
      "029.\n",
      "030.\n",
      "032.\n",
      "034.\n",
      "035.\n",
      "037.\n",
      "038.\n",
      "039.\n",
      "041.\n",
      "044.\n",
      "045.\n",
      "046.\n",
      "048.\n",
      "049.\n",
      "050.\n",
      "052.\n",
      "054.\n",
      "055.\n",
      "056.\n",
      "057.\n",
      "058.\n",
      "059.\n",
      "061.\n",
      "061_.\n",
      "063.\n",
      "064.\n",
      "065.\n",
      "066.\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "for k, s in sentence_dict.items():\n",
    "    sv, sentence_token = sentence2vec(s)\n",
    "    print(k)\n",
    "    sv_dict[k] = sv\n",
    "    sentence_token_dict[k] = sentence_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 500)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_dict_array = np.asarray([i for i in sv_dict.values()])\n",
    "sv_dict_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump sentence vector to pickle\n",
    "import pickle\n",
    "sentence2vec_array = sv_dict_array\n",
    "file = open('para2v_array_zht_500dim.pickle', 'wb')\n",
    "pickle.dump(sentence2vec_array, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2).fit(sv_dict_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dementia = model.labels_[:52]\n",
    "control = model.labels_[52:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dementia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 1], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP: (array([ 1,  2,  3,  5,  9, 10, 11, 12, 14, 15, 16, 18, 19, 20, 21, 22, 23,\n",
      "       24, 25, 29, 30, 32, 33, 34, 36, 37, 38, 40, 42, 43, 48, 50, 51]),)33\n",
      "FN: (array([ 7, 13, 16, 24, 26, 27, 28, 34, 38, 42, 43]),)\n",
      "sklearn kmeans score: -482.7035610675812\n",
      "Silhouette score: 0.13246048986911774\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "dementia = np.asarray(dementia)\n",
    "control = np.asarray(control)\n",
    "\n",
    "fp_sklearn = np.where(dementia==0)\n",
    "fn_sklearn = np.where(control==1)\n",
    "print(\"FP: \" + str(fp_sklearn)+ str(len(fp_sklearn[0])))\n",
    "print(\"FN: \" + str(fn_sklearn))\n",
    "print(\"sklearn kmeans score: {}\".format(model.score(sv_dict_array)))\n",
    "silhouette_score = metrics.silhouette_score(sv_dict_array, model.labels_, \n",
    "                                           metric='euclidean')\n",
    "print(\"Silhouette score: {}\".format(silhouette_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "kclusterer = KMeansClusterer(2, distance=nltk.cluster.util.cosine_distance,\n",
    "                            repeats=1000)\n",
    "assigned_clusters = kclusterer.cluster(sv_dict_array, assign_clusters=True)\n",
    "print(assigned_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP: (array([ 1,  2,  4,  6,  8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
      "       24, 27, 31, 32, 35, 36, 37, 38, 39, 41, 43, 44, 46, 47, 48, 49, 50,\n",
      "       51]),)\n",
      "FN: (array([ 0,  1,  3,  6, 10, 11, 12, 14, 16, 18, 23, 24, 26, 27, 31, 33, 34,\n",
      "       38, 45, 46, 47]),)\n"
     ]
    }
   ],
   "source": [
    "dement = np.asarray(assigned_clusters[:52])\n",
    "contr = np.asarray(assigned_clusters[52:])\n",
    "# dement = np.asarray(dement)\n",
    "\n",
    "fp = np.where(dement==0)\n",
    "fn = np.where(contr==1)\n",
    "print(\"FP: \" + str(fp))\n",
    "print(\"FN: \" + str(fn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
